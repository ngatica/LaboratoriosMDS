{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8623baeb",
   "metadata": {
    "cell_id": "1afe48f4-24bb-456a-8ebb-b21f17292fae",
    "deepnote_cell_height": 291.98333740234375,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Proyecto\n",
    "\n",
    "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Pablo Badilla\n",
    "- Auxiliar: Ignacio Meza D.\n",
    "- Ayudante: Patricio Ortiz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667fd4c",
   "metadata": {
    "cell_id": "00001-08980085-11ff-46bb-ad0e-cfeebe049a14",
    "deepnote_cell_height": 225.11666870117188,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- Fecha de entrega: 15/07/2021 (atrasos hasta el domingo 17 de julio)\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a3910",
   "metadata": {
    "cell_id": "00002-00231602-ae19-4e86-8713-55497e9c1dc0",
    "deepnote_cell_height": 785.2166748046875,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "d50c3174-babb-4861-9c71-7e3af66458b8"
   },
   "source": [
    "---\n",
    "\n",
    "## El Desafío de Renacín 🧸\n",
    "\n",
    "<div align='center'>\n",
    "<img src='https://media.cnnchile.com/sites/2/2018/09/maipu-renacin-740x430.jpg' width=300>\n",
    "</div>\n",
    "\n",
    "Renacín, ex-influencer y figura (de peluche) publica; luego de su despido, decide que será una buena idea darle un giro a su vida y dedicarse al rubro del asesoramiento de inversionistas en la industria del cine. \n",
    "\n",
    "El futuro empresario plantea que el éxito potencial de una propuesta película debe ser analizado en base a evidencia histórica de cintas similares y no en la intuición ni en simples corazonadas. \n",
    "Por esto, plantea a las gerencias de las principales productoras de cine que sería ideal contar con una máquina que, dada las características de una propuesta de película (su género, la productora, su duración, su historia, etc...), prediga si esta será potencialmente una inversión rentable o no.\n",
    "\n",
    "Renacín está convencido que el éxito de una inversión en un filme debe estar relacionada por dos características muy relevantes de estas una vez que salen al mercado:\n",
    "\n",
    "**1. La potencial evaluación (Positiva, Negativa, etc...) que le dan sus consumidores.**\n",
    "\n",
    "**2. Las potenciales ganancias de la película.**\n",
    "\n",
    "Si bien la idea puede sonar excelente, Renacín carece en su totalidad de una formación en Ciencia de los Datos, por lo que decide ir en ayuda de expertos para implementar su idea. \n",
    "\n",
    "Sin embargo, decide no contratar a un equipo en particular, si no que tener la libertad de elegir entre muchos equipos que compiten entre si para saber cuál contratar. Para esto recurre a una triquiñuela recurrentemente utilizada en Data Science: Establecer una competencia abierta y pagar por el mejor modelo (i.e, que cumpla mejor sus requisitos).\n",
    "\n",
    "Para esto, el ex-influencer decide abrir una competencia en la plataforma [Codalab](https://codalab.lisn.upsaclay.fr/competitions/5521?secret_key=7ecfd279-9521-457d-8602-616532fcd813) (plataforma similar a Kaggle) la cuál, espera que se replete de buenos modelos. Los equipos que mejor evaluaciones obtengan (los primeros 3 de cada tabla) serán contratados y retribuidos con un cupón canjeable con la friolera cantidad de 1 punto bonus para el proyecto en el curso MDS7202.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd093b",
   "metadata": {
    "cell_id": "00003-c3cde4b4-710d-4987-b470-2494e93fb1ec",
    "deepnote_cell_height": 964.3333129882812,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Definición Formal del Problema\n",
    "\n",
    "El objetivo de este proyecto es aplicar todo lo aprendido hasta este momento con el fin de solucionar 2 problemas distintos: \n",
    "\n",
    "1. **Clasificación de potenciales evaluaciones con las que los consumidores evaluarán las películas**. Las posibles clases que deben asignar a cada juego son `('Negative', 'Mixed', 'Mostly Positive', 'Positive', 'Very Positive')`. La métrica de evaluación utilizada para medir la clasificación es `f1_macro`.\n",
    "2. **Regresión de los potenciales ingresos que tendrán las películas**. La métrica de evaluación utilizada para medir la clasificación es `r_2`.\n",
    "\n",
    "Para esto, ustedes contarán un dataset que cuenta con diversa información sobre películas (tales como productora, actores, duración, fecha de lanzamiento, keywords, etc...) más las etiquetas y valores a predecir.\n",
    "\n",
    "El objetivo final es que generen el mejor modelo posible para ambos problemas y que con estos, participen en la competencia habilitada en el siguiente ([link](https://codalab.lisn.upsaclay.fr/competitions/5521?secret_key=7ecfd279-9521-457d-8602-616532fcd813)).\n",
    "\n",
    "### Competencias de Data Science\n",
    "\n",
    "> Una competencia de Data Science funciona generalmente de la siguiente manera: \n",
    "\n",
    "1. Se plantea un problema que los equipos deben resolver.\n",
    "2. Se provee de datos de entrenamiento a los equipos para que generen modelos que resuelvan el problema.\n",
    "3. Se provee de datos de prueba que los equipos deberán predecir con los modelos creados. Una vez predichos, los equipos deben subir los archivos a la plataforma, la cuál los evaluará y publicará en un tablero disponible para todos los participantes.\n",
    "\n",
    "Existen muchos sitios en donde se publican competencias recurrentemente tales como [Kaggle](https://www.kaggle.com/) y [Codalab](https://codalab.lisn.upsaclay.fr/).\n",
    "\n",
    "### Competencia del Proyecto\n",
    "\n",
    "Para este proyecto, para participar en la competencia se les proveerá de tres datasets: `train_numerical_features.parquet`, `train_text_features.parquet` y `test.pickle`.\n",
    "\n",
    "- `train_numerical_features.parquet` y `train_text_features.parquet` deben usarlos como conjunto de entrenamiento del modelo; por lo que incluye las etiquetas y valores por predecir. noten que esto no implica que no deban hacer *holdout* para evaluar internamente su modelo (en este caso, el set de test es llamado de *validación*). Por otro lado, se recomienda que junten ambos archivos para generar el tablón final de entrenamiento.\n",
    "\n",
    "- `test.pickle` se usará para evaluar el rendimiento de sus modelos en la competencia. Es decir, este dataset solo contiene características de las películas y ustedes deberán predecir tanto las potenciales evaluaciones como las ganancias de estas y subir sus resultados.\n",
    "\n",
    "Para subir archivos a la competencia deberán registrarse en Codalab. Para evitar overfitting y/o que intenten adivinar los datos de testing, **puden participar máximo 5 veces en la competencia**. Usenlos sabiamente.\n",
    "\n",
    "**MUY IMPORTANTE**: Para la clasificación no usen las ganancias (target de la regresión) como atributo. Por otro lado, para la regresión no utilicen las evaluaciones como atributo para predecir. **Infringir estas reglas implicará en no contar el puntaje de la competencia como también descuentos en los items de clasificación como de regresión.** Recuerden que esta es información del \"futuro\": cuando estén las propuestas de películas no dispondremos de estas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6443276",
   "metadata": {
    "cell_id": "00004-a4f74232-f16f-4b68-9488-a39082e07e79",
    "deepnote_cell_height": 8913.283203125,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb"
   },
   "source": [
    "---\n",
    "\n",
    "## Secciones Requeridas en el Informe\n",
    "\n",
    "La siguiente lista detalla las secciones que debe contener su notebook para resolver el proyecto. Es importante que al momento de desarrollar cada una de las secciones, estas sean escritas en un formato tipo **informe**, donde describan detalladamente cada uno de los puntos realizados.\n",
    "\n",
    "### 1. Introducción [0.5 Puntos]\n",
    "\n",
    "*Esta sección es literalmente una muy breve introducción con todo lo necesario para entender que hicieron en su proyecto.*\n",
    "\n",
    "- Describir brevemente ambos problemas planteados (clasificación y regresión).\n",
    "- Describir brevemente los datos de entrada que les provee el problema. \n",
    "- Describir qué métricas se ocupan para la evaluación del problema y por qué se utilizan.\n",
    "- [Escribir al final] Describir brevemente el modelo que usaron para resolver el problema (incluyendo las transformaciones intermedias de datos).\n",
    "- [Escribir al final] Indicar si lograron resolver el problema a través de su modelo. Indiquen además si creen que los resultados de su mejor modelo son aceptables y como les fue con respecto al resto de los equipos.\n",
    "\n",
    "### 2. Preparación del Dataset y Análisis Exploratorio de Datos [2 Puntos]\n",
    "\n",
    "\n",
    "*La idea de esta sección es que preparen y exploren el dataset y obtengan una idea de como son los datos de su problema para que en la siguientes secciones, puedan modelarlos.*\n",
    "\n",
    "#### Carga y Preparación de los Datos\n",
    "\n",
    "Primero, se les solicita que ejecuten los siguientes pasos de carga de datos:\n",
    "\n",
    "- Cargar los datos con Pandas y fusionar por `id`.\n",
    "- Eliminar columnas `'poster_path'`, `'backdrop_path'`, `'recommendations'`.\n",
    "- Filtrar ejemplos con `revenue` igual a 0.\n",
    "- Filtrar ejemplos con `release_date` y `runtime` nulos.\n",
    "- Convertir fechas de release_date a `pd.DateTime`.\n",
    "- Conservar solo los ejemplos con `status` `\"Released\"`.\n",
    "- Rellenar valores nulos categóricos y de texto con `''`.\n",
    "- Discretizar `vote_average` a los siguientes bins y guardar los resultados en la columna `label`: \n",
    "  - (0, 5]: `'Negative'`\n",
    "  - (5, 6]: `'Mixed'`\n",
    "  - (6, 7]: `'Mostly Positive'`\n",
    "  - (7, 8]: `'Positive'`\n",
    "  - (8, 10]: `'Very Positive'`\n",
    "- Eliminar la columna `vote_average` e `id`\n",
    "- Renombrar la columna `revenue` por `target`.\n",
    "\n",
    "Todos los pasos anteriormente nombrados deben ser realizados con Pandas y sus métodos, y sin la ayuda de ningún ciclo `for`. Si no conoce alguna operación, se recomienda fuertemente buscar en la web antes de intentar hacerlo por fuerza bruta.\n",
    "\n",
    "#### EDA\n",
    "\n",
    "Luego, se les solicita que realizen un EDA en donde hagan tareas generales como:\n",
    "\n",
    "- Analizar los tipos de datos y distribuciones de las variables a través de histogramas.\n",
    "- Generar visualizaciones de las interacciones (como por ejemplo, una scatter matrix) en las distintas variables.\n",
    "- Ver las correlaciones entre las distintas variables y los valores faltantes de cada una de estas. \n",
    "- Proyectar los datos con UMAP para ver si existen relaciones entre las distintas variables de forma bi o tri dimensional.\n",
    "- Reportar los patrones y relaciones interesantes.\n",
    "\n",
    "Como también tareas particulares como:\n",
    "\n",
    "- Genenrar un scatterplot de `budget` vs `target` (ex-revenue).\n",
    "- Buscar las peliculas de Marvel y hacer un scatter plot que en el eje x contenga `budget` y en el eje y `target` (ex-revenue). Utilice las columnas `` como `` y como `hover_name` .\n",
    "- Graficar a través de un gráfico de barras las 50 productoras más frecuentes. Incluya la cantidad de `nans`.\n",
    "- Graficar a través de un gráfico de barras los 50 artistas más frecuentes. \n",
    "\n",
    "Las tareas mencionadas anteriormente son lo mínimo. Sin embargo, ojalá no se restringan a lo mencionado anteriormente y exploren aún más en profundidad. Pueden usar tanto **`pandas`** como **plotly** y ``pandas profiling`` para esto. En el caso de generar un profile, adjuntar los resultados en el notebook.\n",
    "\n",
    "### 3. Preparación de Datos [0.5 Punto]\n",
    "\n",
    "#### ColumnTransformer y Holdout\n",
    "\n",
    "*Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo.*\n",
    "\n",
    "Generar un ColumnTransformer que:\n",
    "\n",
    "- Preprocese datos categóricos y ordinales.\n",
    "- Escale/estandarice datos numéricos.\n",
    "- Codifique texto.\n",
    "\n",
    "Luego, pruebe las transformaciones utilizando `fit_transform` y `get_feature_names out`.\n",
    "\n",
    "Posteriormente, ejecute un Holdout que le permita más adelante evaluar los modelos. **Recuerde eliminar los target y las labels del dataset antes de dividirlo**.\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "Adicionalmente puede generar una nueva transformación que genere nuevas features y que se aplique antes del ColumnTransformer dentro del pipeline de los modelos. Investigar [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) para ver como implementar una transformación a partir de una función que tome un dataframe y entregue uno distinto en la salida.\n",
    "\n",
    "- Encodear ciclicamente los meses/días de las fechas de lanzamiento.\n",
    "- Contar cuantas veces aparecen en las peliculas ciertos personajes célebres.\n",
    "- Indicar si la pelicula es de una productora famosa o no.\n",
    "- Agrupar distintas keywords en categorías más generales.\n",
    "- Generar ratios con las variables numericas del dataset (como duración de la película/presupuesto).\n",
    "- Contar los diferentes generos similares que posee una pelicula.\n",
    "- Extraer vectores desde los overviews de las peliculas.\n",
    "- Contar el número de actores/productoras/géneros.\n",
    "- Etc... Usen su creatividad!\n",
    "\n",
    "Nuevamente, recuerde no utilizar ni los targets ni las labels para generar nuevas features.\n",
    "\n",
    "Nota: Este último paso no es requisito pero puede catapultarlos a la cima del tablero de las competencias.\n",
    "\n",
    "### 4. Clasificación [1.5 puntos]\n",
    "\n",
    "#### Creación del modelo **Dummy** y del *Baseline* [0.5 Puntos]\n",
    "\n",
    "*En esta sección crearán el modelo más básico posible que resuelva el problema. La idea de este modelo usarlo como comparación para que en el siguiente paso lo puedan mejorar.*\n",
    "\n",
    "- Generar un modelo Dummy con estrategia estratificada que les permita comparar más adelante si su baseline de clasificación es mejor que el azar.\n",
    "- Generar un pipeline para la clasificación con un clasificador relativamente sencillo a la salida (a su elección, recomendado: arbol de decisión).\n",
    "- Evaluar ambos modelos según las métricas de evaluación y reportar.\n",
    "\n",
    "#### Optimización del Modelo [1 Puntos]\n",
    "\n",
    "*Aquí deberán mejorar del modelo de clasificación al variar los algoritmos/hiperparámetros que están ocupando.*\n",
    "\n",
    "- Generar una nueva `Pipeline` enfocada en buscar el mejor modelo usando GridSearch.\n",
    "- Usar **`GridSearchCV`** o **`HalvingGridSearchCV`** para tunear hipermarámetros. La primera demorará más que la segunda pero les traerá potencialmente mejores resultados. Pueden probar también [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html) de la librería [`Optuna`](https://optuna.org/) , la cuál es bastante popular para buscar modelos de redes neuronales.\n",
    "- Agregar técnicas de seleccion de atributos, como también usar mejores clasificadores y explorar sus hiperparámetros. \n",
    "- Probar distintos parámetros para las transformaciones de datos, seleccion de atributos, clasificadores, etc...\n",
    "- Probar modelos basados en gradient boosting/bagging. **Recomendación fuerte:** Probar [`LightGMB`](https://lightgbm.readthedocs.io/en/latest/) o [`xgboost`](https://xgboost.readthedocs.io/en/stable/).\n",
    "- Probar activando/descativando los procesadores de texto, de categorías, etc...\n",
    "- Recuerden setear la búsqueda para optimizar la métrica que se evalua en la competencia.\n",
    "\n",
    "Algunas notas interesantes sobre este proceso: \n",
    "\n",
    "- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n",
    "- Hagan grillas computables: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar. \n",
    "- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n",
    "\n",
    "**Al final de este proceso, seleccione el mejor modelo de clasificación encontrado, prediga las labels del test set de la competencia y envíelos a Codalab.**\n",
    "\n",
    "### 4. Regresión [1.5 Puntos]\n",
    "\n",
    "#### Creación del modelo **Dummy** y del *Baseline* [0.5 Punto]\n",
    "\n",
    "*En esta sección crearán el modelo más básico posible que resuelva el segundo problema, el de las ganancias. La idea de este modelo usarlo como comparación para que en el siguiente paso lo puedan mejorar.*\n",
    "\n",
    "- Generar un modelo Dummy que les permita comparar más adelante si su baseline de regresión es mejor que el azar.\n",
    "- Generar un pipeline para la regresión con un regresor relativamente sencillo de utilizar (a su elección, recomendado: arbol de decisión).\n",
    "- Evaluar ambos modelos según las métricas de evaluación y reportar.\n",
    "\n",
    "#### Optimización del Modelo [1 Puntos]\n",
    "\n",
    "*Aquí deberán mejorar del modelo de regresión al variar los algoritmos/hiperparámetros que están ocupando.*\n",
    "\n",
    "- Generar una nueva `Pipeline` enfocada en buscar el mejor modelo usando GridSearch.\n",
    "- Usar **`GridSearchCV`** o **`HalvingGridSearchCV`** para tunear hipermarámetros. La primera demorará más que la segunda pero les traerá potencialmente mejores resultados. Pueden probar también [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html) de la librería [`Optuna`](https://optuna.org/) , la cuál es bastante popular para buscar modelos de redes neuronales.\n",
    "- Agregar técnicas de seleccion de atributos, como también usar mejores clasificadores y explorar sus hiperparámetros. \n",
    "- Probar distintos parámetros para las transformaciones de datos, seleccion de atributos, clasificadores, etc...\n",
    "- Probar modelos basados en gradient boosting/bagging. **Recomendación fuerte:** Probar las librerías [`LightGMB`](https://lightgbm.readthedocs.io/en/latest/) o [`xgboost`](https://xgboost.readthedocs.io/en/stable/).\n",
    "- Probar activando/descativando los procesadores de texto, de categorías, etc...\n",
    "- Recuerden setear la búsqueda para optimizar la métrica que se evalua en la competencia.\n",
    "\n",
    "\n",
    "\n",
    "Algunas notas interesantes sobre este proceso (las mismas que antes...): \n",
    "\n",
    "- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n",
    "- Hagan grillas computables: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar. \n",
    "- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n",
    "\n",
    "**Al final de este proceso, seleccione el mejor modelo de regresión encontrado, prediga los target del test set de la competencia y envíelos a Codalab.**\n",
    "\n",
    "\n",
    "### 6. Concluir [0.5 Punto] \n",
    "\n",
    "\n",
    "*Aquí deben escribir una breve conclusión del trabajo que hicieron en donde incluyan (pero no se limiten) a responder las siguientes preguntas:*\n",
    "\n",
    "- ¿Pudieron resolver exitosamente el problema?\n",
    "- ¿Son aceptables los resultados obtenidos?\n",
    "- ¿En que medida el EDA ayudó a comprender los datos en miras de generar un modelo predictivo?\n",
    "\n",
    "Respecto a la clasificación:\n",
    "\n",
    "- ¿Como fue el rendimiento del baseline para la clasificación?\n",
    "- ¿Pudieron optimizar el baseline para la clasificación?\n",
    "- ¿Que tanto mejoro el baseline de la clasificación con respecto a sus optimizaciones?\n",
    "\n",
    "Respecto a la regresión:\n",
    "\n",
    "- ¿Como fue el rendimiento del baseline para la clasificación?\n",
    "- ¿Pudieron optimizar el baseline para la clasificación?\n",
    "- ¿Que tanto mejoro el baseline de la clasificación con respecto a sus optimizaciones?\n",
    "\n",
    "Finalmente: \n",
    "\n",
    "- ¿Estuvieron conformes con sus resultados?\n",
    "- ¿Creen que hayan mejores formas de modelar el problema?\n",
    "- ¿En general, qué aprendieron del proyecto? ¿Qué no aprendieron y les gustaría haber aprendido?\n",
    "\n",
    "**OJO** si usted decide responder parte de estas preguntas, debe redactarlas en un formato de informe y no responderlas directamente.\n",
    "\n",
    "### 7. Punto por superar el baseline de la competencia. [1 Punto]\n",
    "\n",
    "*Este punto es solo informativo, no deben escribir nada aquí*\n",
    "\n",
    "\n",
    "### Otras Instrucciones\n",
    "\n",
    "Pueden separar las tareas en 3 notebooks distintos: EDA, clasificación y regresión. Esto les permitirá tener el código de estas tres secciones separadas y mejor ordenado a la vez que reduce el uso de memoria RAM.\n",
    "Noten que una estructura así requerirá que la etapa de carga y preparación de los datos esté presente en los 3 notebooks (para que trabaje con el mismo dataset en cada instancia). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624c2bb",
   "metadata": {
    "cell_id": "00005-aeba1c85-fe35-4f74-9923-d2edc00d7e60",
    "deepnote_cell_height": 249.23333740234375,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## Esquema de la Tarea\n",
    "\n",
    "\n",
    "Pueden usar el siguiente esquema para organizar la tarea (y borrar todo lo anterior).\n",
    "Obviamente **no deben limitarse a lo que está escrito en esta**: puede incrementar en caso de más técnicas y obviar algunas partes en caso que alguna y otro punto no aplique a su problema.\n",
    "\n",
    "Pueden borrar las instrucciones anteriores y quedarse solo con lo que viene a continuación.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df401720",
   "metadata": {
    "cell_id": "00006-84a35c5d-0758-4cbb-b2ca-b182898b80d0",
    "deepnote_cell_height": 295.8833312988281,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Proyecto\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- \\<Primer integrante\\> Cristóbal Igor\n",
    "- \\<Segundo integrante\\> Nicolás Gatica\n",
    "\n",
    "- \\<Nombre de usuarios en Codalab\\> cigor\n",
    "\n",
    "- \\<Nombre del Equipo en Codalab\\> -\n",
    "\n",
    "### Link de repositorio de GitHub: `\\<https://github.com/ngatica/LaboratoriosMDS\\>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93b698",
   "metadata": {
    "cell_id": "00007-447f1977-318e-432f-ba83-12d7e667ae68",
    "deepnote_cell_height": 253.13333129882812,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1. Introducción\n",
    "\n",
    "El objetivo de este proyecto consiste en medir el éxito que tendrá una película. Esto se ve reflejado en dos métricas; los votos promedios, que seran categorizados en 'Negative', 'Mixed', 'Mostly Positive', 'Positive', 'Very Positive'; y la cantidad de dinero recaudado luego de ser estrenadas.\n",
    "\n",
    "Los datos que se proveen es un dataset con 6451 ejemplos que describen una observación de películas y series ya estrenadas. Esto a través de su título, presupuesto, duración, lema, creditos, generos, lenguaje, resumen, productoras, fecha de lanzamiento y palabras claves.\n",
    "\n",
    "En total son 13 atributos, 11 correspondientes a las variables mencionadas y 2 correspondientes a la variables objetivos. Esta últimas son de tipo; categórico, asociado a la características de los votos de de las películas (negativos, mixto, mayormente positivo, positivo, y muy positivos); y númerico asociado a las recaudaciones logradas por la película.\n",
    "\n",
    "La primera tarea, correspondiente a la clasificación de los votos promedios, se evalua en base a la métrica F1-macro ya que esta combina la precisión y la sensibilidad que tiene un test, calculando la media armónica entre ambas. Al tener un problema multiclase, la extensión macro obtiene el promedio de los F1 resultantes para cada una de las etiquetas.\n",
    "\n",
    "La segunda tarea, correspondiente a la predicción de la recaudación, se evalua en base a la métrica R2 ya que esta es una medida estadística que muestra qué tan cerca se encuentran los puntos de datos sobre la linea de regresión.\n",
    "\n",
    "Nuestra propuesta para resolver ambos problemas consistió, de forma resumida, en primero crear caracteristicas a partir de las ya existentes, y también vectorizar las variables en formato de texto.\n",
    "Luego, para clasificar y predecir se utilizaron principalmente modelos ensamblados, esto dado que demostraron tener un mejor desempeño que los modelos más básicos.\n",
    "Por último, partir de estos modelos se procedió a mejorar los parámetros asociados tanto a los modelos, como a los métodos de vectorización de los atributos de texto.\n",
    "\n",
    "En base a este procedimiento nuestros modelos cumplieron con las expectativas, lograron tener una buena capacidad de generalización y robustez, superando en ambos casos al baseline propuesto en la competencia, y destacando en la predicción de la recaudación de las películas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73159ed",
   "metadata": {
    "cell_id": "00008-6607fdcd-a35f-4e75-9b46-da3b181c1551",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "## 2. Prepración y Análisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28d09b2-b19b-47d5-9126-3247ab205433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Librerias\n",
    "!pip install -q pyarrow\n",
    "!pip install -q pandas\n",
    "!pip install -q sklearn\n",
    "!pip install -q xgboost\n",
    "!pip install -q lightgbm\n",
    "!pip install -q nltk\n",
    "!pip install -q seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-procesamiento\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# Texto\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "!pip install -q lxml\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import os, warnings\n",
    "    warnings.simplefilter(\"ignore\") # Change the filter in this process\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4134b2",
   "metadata": {
    "cell_id": "00009-e413acae-3cf7-44db-847c-b846e3673adc",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166425,
    "source_hash": "55969c6e"
   },
   "outputs": [],
   "source": [
    "##  Código Preparación de Datos.\n",
    "\n",
    "## Leyendo parquets\n",
    "data_numerica=pd.read_parquet('train_numerical_features.parquet')\n",
    "data_text=pd.read_parquet('train_text_features.parquet')\n",
    "\n",
    "## Merge data\n",
    "data=data_numerica.merge(data_text, how='inner', on=['id', 'title', 'credits', 'tagline'])\n",
    "\n",
    "## Eliminando columnas\n",
    "data.drop(columns=['poster_path', 'backdrop_path', 'recommendations'], inplace=True)\n",
    "\n",
    "## Filtrado revenue\n",
    "data=data[~(data.revenue==0)]\n",
    "\n",
    "## Filtrado release_date o runtime nulos\n",
    "data=data[(~(data.release_date.isna()) | (data.runtime.isna()))]\n",
    "\n",
    "## Fechas a datetime\n",
    "data['release_date']=pd.to_datetime(data.release_date)\n",
    "\n",
    "## Filtro status\n",
    "data=data[data.status=='Released']\n",
    "data.drop(columns=['status'], inplace=True)\n",
    "\n",
    "## Valores nulos de categóricas a string vacio\n",
    "data[data.select_dtypes(include='object').columns]=data[data.select_dtypes(include='object').columns].fillna('')\n",
    "\n",
    "## Discretizando los votos\n",
    "labels=['Negative', 'Mixed', 'Mostly Positive', 'Positive', 'Very Positive']\n",
    "data['label']=pd.cut(data.vote_average, bins=[0, 5, 6, 7, 8, 10], labels=labels)\n",
    "\n",
    "## Eliminando columnas vote average e id\n",
    "data.drop(columns=['vote_average', 'id'], inplace=True)\n",
    "\n",
    "## Renonmbrado columnas:\n",
    "data.rename(columns={'revenue': 'target'}, inplace=True)\n",
    "\n",
    "## Reseteando index\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e678393-c1d8-40b7-b180-bfda8d4c82bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>target</th>\n",
       "      <th>runtime</th>\n",
       "      <th>tagline</th>\n",
       "      <th>credits</th>\n",
       "      <th>genres</th>\n",
       "      <th>original_language</th>\n",
       "      <th>overview</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>release_date</th>\n",
       "      <th>keywords</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic Beasts: The Secrets of Dumbledore</td>\n",
       "      <td>200000000.0</td>\n",
       "      <td>400000000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>Return to the magic.</td>\n",
       "      <td>Jude Law-Eddie Redmayne-Mads Mikkelsen-Ezra Mi...</td>\n",
       "      <td>Fantasy-Adventure-Action</td>\n",
       "      <td>en</td>\n",
       "      <td>Professor Albus Dumbledore knows the powerful ...</td>\n",
       "      <td>Warner Bros. Pictures-Heyday Films</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>magic-curse-fantasy world-wizard-magical creat...</td>\n",
       "      <td>Mostly Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sonic the Hedgehog 2</td>\n",
       "      <td>110000000.0</td>\n",
       "      <td>393000000.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>Welcome to the next level.</td>\n",
       "      <td>James Marsden-Ben Schwartz-Tika Sumpter-Natash...</td>\n",
       "      <td>Action-Adventure-Family-Comedy</td>\n",
       "      <td>en</td>\n",
       "      <td>After settling in Green Hills Sonic is eager t...</td>\n",
       "      <td>Original Film-Blur Studio-Marza Animation Plan...</td>\n",
       "      <td>2022-03-30</td>\n",
       "      <td>sequel-based on video game-hedgehog-live actio...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Lost City</td>\n",
       "      <td>74000000.0</td>\n",
       "      <td>164289828.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>The adventure is real. The heroes are not.</td>\n",
       "      <td>Sandra Bullock-Channing Tatum-Daniel Radcliffe...</td>\n",
       "      <td>Action-Adventure-Comedy</td>\n",
       "      <td>en</td>\n",
       "      <td>A reclusive romance novelist was sure nothing ...</td>\n",
       "      <td>Paramount-Fortis Films-3dot Productions-Exhibi...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>duringcreditsstinger</td>\n",
       "      <td>Mostly Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title       budget       target  \\\n",
       "0  Fantastic Beasts: The Secrets of Dumbledore  200000000.0  400000000.0   \n",
       "1                         Sonic the Hedgehog 2  110000000.0  393000000.0   \n",
       "2                                The Lost City   74000000.0  164289828.0   \n",
       "\n",
       "   runtime                                     tagline  \\\n",
       "0    142.0                        Return to the magic.   \n",
       "1    122.0                  Welcome to the next level.   \n",
       "2    112.0  The adventure is real. The heroes are not.   \n",
       "\n",
       "                                             credits  \\\n",
       "0  Jude Law-Eddie Redmayne-Mads Mikkelsen-Ezra Mi...   \n",
       "1  James Marsden-Ben Schwartz-Tika Sumpter-Natash...   \n",
       "2  Sandra Bullock-Channing Tatum-Daniel Radcliffe...   \n",
       "\n",
       "                           genres original_language  \\\n",
       "0        Fantasy-Adventure-Action                en   \n",
       "1  Action-Adventure-Family-Comedy                en   \n",
       "2         Action-Adventure-Comedy                en   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Professor Albus Dumbledore knows the powerful ...   \n",
       "1  After settling in Green Hills Sonic is eager t...   \n",
       "2  A reclusive romance novelist was sure nothing ...   \n",
       "\n",
       "                                production_companies release_date  \\\n",
       "0                 Warner Bros. Pictures-Heyday Films   2022-04-06   \n",
       "1  Original Film-Blur Studio-Marza Animation Plan...   2022-03-30   \n",
       "2  Paramount-Fortis Films-3dot Productions-Exhibi...   2022-03-24   \n",
       "\n",
       "                                            keywords            label  \n",
       "0  magic-curse-fantasy world-wizard-magical creat...  Mostly Positive  \n",
       "1  sequel-based on video game-hedgehog-live actio...         Positive  \n",
       "2                               duringcreditsstinger  Mostly Positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71825663",
   "metadata": {
    "cell_id": "051fb133d1264c4dab1de21415aa18b6",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Código EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093606a9",
   "metadata": {
    "cell_id": "6dfb65f58341449fb33eb025db788790",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "```\n",
    "Análisis del EDA.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d9f15",
   "metadata": {
    "cell_id": "00011-95957584-71f1-4669-a6e5-9b8ac7b8d4e0",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocesamiento, Holdout y Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbec91d-f209-4c90-9f75-142bf5317412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    # Código Feature Engineering (Opcional)\n",
    "    # Trabajando con la fecha de lanzamiento\n",
    "    ## Separando la fecha\n",
    "    df['release_year']=df.release_date.dt.year\n",
    "    df['release_day']=df.release_date.dt.day\n",
    "    df['release_month']=df.release_date.dt.month\n",
    "\n",
    "    ## Dia de la semana a la que pertenece\n",
    "    df['release_date_day'] = df['release_date'].transform(lambda x: x.weekday())\n",
    "\n",
    "    ## Si es del fin de semana\n",
    "    df['weekend'] = df['release_date_day'].transform(lambda x: 1 if x<4 else 0)\n",
    "\n",
    "    ## Dia del año\n",
    "    df['dayofyear'] = pd.to_datetime(df['release_date']).dt.isocalendar().day\n",
    "\n",
    "    ## Semana del año\n",
    "    df['weekofyear'] = pd.to_datetime(df['release_date']).dt.isocalendar().week\n",
    "\n",
    "\n",
    "    #Conteo de atributos\n",
    "    ## Generos\n",
    "    df['genres_count'] = df['genres'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Actores\n",
    "    df['actores_count'] = df['credits'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Productoras\n",
    "    df['production_companies_count'] = df['production_companies'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Keywords\n",
    "    df['keywords_count'] = df['keywords'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Conteos del titulo\n",
    "    df['letters_title_count'] = df['title'].str.len() \n",
    "    df['words_title_count'] = df['title'].str.split().str.len() \n",
    "\n",
    "    ## Conteos del tagline\n",
    "    df['letters_tagline_count'] = df['tagline'].str.len() \n",
    "    df['words_tagline_count'] = df['tagline'].str.split().str.len() \n",
    "\n",
    "\n",
    "\n",
    "    ## Codificación generos:\n",
    "    gen = data['genres'].str.split(r\"-\", expand=True)\n",
    "    generos = pd.DataFrame()\n",
    "    for i in range(len(gen.columns)):\n",
    "        generos = pd.concat([generos, gen[i]], axis = 0)\n",
    "    generos.dropna(axis=0, inplace=True)\n",
    "    generos=generos[0].unique()\n",
    "    for i in generos:\n",
    "        column = str('genre_' + i)\n",
    "        df[column] = df['genres'].str.split(r\"-\").transform(lambda x: 1 if i in x else 0)\n",
    "\n",
    "\n",
    "    # Generación de ratios\n",
    "    ## Ratio actores budget\n",
    "    df['budget_actores'] = (df.budget)/(df.actores_count).replace([np.inf,-np.inf,np.nan],0)\n",
    "\n",
    "    ## Presupuesto dividido en año\n",
    "    df['budget_anio']=(df.budget)/(df.release_year)\n",
    "\n",
    "    ## Presupuesto dividido en runtime\n",
    "    df['budget_runtime']=(df.budget)/(df.runtime).replace([np.inf,-np.inf,np.nan], 0)\n",
    "\n",
    "    ## Presupuesto en logaritmo\n",
    "    df['log_budget']=np.log1p(df.budget)\n",
    "\n",
    "    def intersection(lst1, lst2):\n",
    "        return len(list(set(lst1) & set(lst2)))\n",
    "\n",
    "    # Productoras mas famosas\n",
    "    disney_compaines = ['ABC Pictures', 'DisneyToon Studios', 'Disneynature','Touchstone Pictures', 'Marvel Studios']\n",
    "    df['disney'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, disney_compaines))\n",
    "\n",
    "    twenty_century_fox_compaines = ['Fox 2000 Pictures','Fox Atomic','Fox Searchlight Pictures']\n",
    "    df['fox'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, twenty_century_fox_compaines))\n",
    "\n",
    "    warner_bros_compaines = ['Warner Bros. Family Entertainment', 'Warner Bros. Pictures', 'New Line Cinema', 'Alloy Entertainment', 'Castle Rock Entertainment', 'Bad Robot']\n",
    "    df['warner'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, warner_bros_compaines))\n",
    "\n",
    "    nbcuniversal_compaines = ['Universal Pictures', 'Universal Pictures International (UPI)', 'DreamWorks Animation', 'DreamWorks Pictures', 'Focus Features']\n",
    "    df['nbc'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, nbcuniversal_compaines))\n",
    "\n",
    "    sony_pictures_compaines = ['Columbia Pictures', 'Sony Pictures', 'Lakeshore Entertainment', 'Marvel Studios', 'Will Packer Productions']\n",
    "    df['sony'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, sony_pictures_compaines))\n",
    "\n",
    "    paramount_pictures_compaines = ['Paramount']\n",
    "    df['paramount'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, paramount_pictures_compaines))\n",
    "\n",
    "    # Actores mas famosos\n",
    "    top_twenty_actors = ['Jennifer Lopez', 'Leonardo DiCaprio', 'Robert Downey Jr.', 'Brad Pitt',\n",
    "                         'Will Smith', 'Jessica Alba', 'Keanu Reeves', 'Jackie Chan', 'Clint Eastwood', \n",
    "                         'Arnold Schwarzenegger', 'Sylvester Stallone', 'Bill Cosby', 'Jack Nicholson',  \n",
    "                         'Adam Sandler', 'Mel Gibson', 'George Clooney', 'Tom Cruise', 'Tyler Perry',  \n",
    "                         'Jerry Seinfeld','Oprah Winfrey']\n",
    "    df['count_top_actors'] = df.credits.str.split(r'-').apply(lambda x: intersection(x, top_twenty_actors))\n",
    "\n",
    "    # Directores mas valorados\n",
    "    top_thirty_directors=['Richard Linklater', 'Mike Leigh', 'Jafar Panahi', 'Jean-Pierre Dardenne', 'The Coen Brothers', 'Hirokazu Koreeda', \n",
    "                          'Steven Spielberg', 'Martin Scorsese', 'Pedro Almodavar', 'Werner Herzog', 'Paul Thomas Anderson', 'Michael Haneke',\n",
    "                          'Clint Eastwood', 'Steven Soderbergh', 'Olivier Assayas', 'Peter Jackson', 'David O. Russell', 'Spike Lee',\n",
    "                          'Quentin Tarantino', 'Michael Winterbottom', 'Alex Gibney', 'Stephen Frears', 'Yimou Zhang', 'Jia Zhangke',\n",
    "                          'Noah Baumbach', 'Ridley Scott', 'Christopher Nolan', 'Jonathan Demme', 'Wes Anderson', 'Guillermo del Toro']\n",
    "    df['count_top_directors'] = df.credits.str.split(r'-').apply(lambda x: intersection(x, top_thirty_directors))   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec355cf7-a9fc-42d8-9094-64914e036072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>original_language</th>\n",
       "      <th>overview</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>release_date</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>credits</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15927</td>\n",
       "      <td>A Passage to India</td>\n",
       "      <td>Drama-Adventure-History</td>\n",
       "      <td>en</td>\n",
       "      <td>Set during the period of growing influence of ...</td>\n",
       "      <td>EMI Films-Thorn EMI Screen Entertainment-HBO</td>\n",
       "      <td>1984-12-14</td>\n",
       "      <td>8000000.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>David Lean, the Director of \"Doctor Zhivago\", ...</td>\n",
       "      <td>Judy Davis-Victor Banerjee-Peggy Ashcroft-Jame...</td>\n",
       "      <td>based on novel or book-cave-hindu-doctor-india...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9064</td>\n",
       "      <td>Hellbound: Hellraiser II</td>\n",
       "      <td>Horror</td>\n",
       "      <td>en</td>\n",
       "      <td>Doctor Channard is sent a new patient a girl w...</td>\n",
       "      <td>Film Futures-New World Pictures-Cinemarque Ent...</td>\n",
       "      <td>1988-12-23</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>Time to play.</td>\n",
       "      <td>Ashley Laurence-Clare Higgins-Kenneth Cranham-...</td>\n",
       "      <td>seduction-pain-stepmother-hell-pinhead-sequel-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                     title                   genres original_language  \\\n",
       "0  15927        A Passage to India  Drama-Adventure-History                en   \n",
       "1   9064  Hellbound: Hellraiser II                   Horror                en   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Set during the period of growing influence of ...   \n",
       "1  Doctor Channard is sent a new patient a girl w...   \n",
       "\n",
       "                                production_companies release_date     budget  \\\n",
       "0       EMI Films-Thorn EMI Screen Entertainment-HBO   1984-12-14  8000000.0   \n",
       "1  Film Futures-New World Pictures-Cinemarque Ent...   1988-12-23  3000000.0   \n",
       "\n",
       "   runtime    status                                            tagline  \\\n",
       "0    163.0  Released  David Lean, the Director of \"Doctor Zhivago\", ...   \n",
       "1     97.0  Released                                      Time to play.   \n",
       "\n",
       "                                             credits  \\\n",
       "0  Judy Davis-Victor Banerjee-Peggy Ashcroft-Jame...   \n",
       "1  Ashley Laurence-Clare Higgins-Kenneth Cranham-...   \n",
       "\n",
       "                                            keywords  \n",
       "0  based on novel or book-cave-hindu-doctor-india...  \n",
       "1  seduction-pain-stepmother-hell-pinhead-sequel-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_pickle('test.pickle')  \n",
    "data_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b396c747-4018-458b-93b1-997b3d387134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_parquet('data_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc38ff7-7c92-4c15-8be8-5137edac1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feature_engineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0592c4f4-70da-4df4-a933-e401475f4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = feature_engineering(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5923087-35be-475d-b081-3e8d1201f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Jul 17 23:08:56 2022\n",
      "Fold 1 started at Sun Jul 17 23:08:56 2022\n",
      "Fold 2 started at Sun Jul 17 23:08:56 2022\n",
      "Fold 3 started at Sun Jul 17 23:08:57 2022\n",
      "Fold 4 started at Sun Jul 17 23:08:57 2022\n",
      "Fold 5 started at Sun Jul 17 23:08:57 2022\n",
      "Fold 6 started at Sun Jul 17 23:08:58 2022\n",
      "Fold 7 started at Sun Jul 17 23:08:58 2022\n",
      "Fold 8 started at Sun Jul 17 23:08:58 2022\n",
      "Fold 9 started at Sun Jul 17 23:08:58 2022\n",
      "CV mean score: 2.2249, std: 0.1098.\n",
      "Fold 0 started at Sun Jul 17 23:08:59 2022\n",
      "Fold 1 started at Sun Jul 17 23:08:59 2022\n",
      "Fold 2 started at Sun Jul 17 23:09:00 2022\n",
      "Fold 3 started at Sun Jul 17 23:09:00 2022\n",
      "Fold 4 started at Sun Jul 17 23:09:00 2022\n",
      "Fold 5 started at Sun Jul 17 23:09:01 2022\n",
      "Fold 6 started at Sun Jul 17 23:09:01 2022\n",
      "Fold 7 started at Sun Jul 17 23:09:02 2022\n",
      "Fold 8 started at Sun Jul 17 23:09:02 2022\n",
      "Fold 9 started at Sun Jul 17 23:09:02 2022\n",
      "CV mean score: 2.2146, std: 0.1085.\n",
      "Fold 0 started at Sun Jul 17 23:09:04 2022\n",
      "Fold 1 started at Sun Jul 17 23:09:05 2022\n",
      "Fold 2 started at Sun Jul 17 23:09:06 2022\n",
      "Fold 3 started at Sun Jul 17 23:09:07 2022\n",
      "Fold 4 started at Sun Jul 17 23:09:09 2022\n",
      "Fold 5 started at Sun Jul 17 23:09:10 2022\n",
      "Fold 6 started at Sun Jul 17 23:09:12 2022\n",
      "Fold 7 started at Sun Jul 17 23:09:13 2022\n",
      "Fold 8 started at Sun Jul 17 23:09:15 2022\n",
      "Fold 9 started at Sun Jul 17 23:09:16 2022\n",
      "CV mean score: 2.1622, std: 0.1161.\n",
      "Fold 0 started at Sun Jul 17 23:09:18 2022\n",
      "Fold 1 started at Sun Jul 17 23:09:18 2022\n",
      "Fold 2 started at Sun Jul 17 23:09:19 2022\n",
      "Fold 3 started at Sun Jul 17 23:09:20 2022\n",
      "Fold 4 started at Sun Jul 17 23:09:20 2022\n",
      "Fold 5 started at Sun Jul 17 23:09:21 2022\n",
      "Fold 6 started at Sun Jul 17 23:09:22 2022\n",
      "Fold 7 started at Sun Jul 17 23:09:22 2022\n",
      "Fold 8 started at Sun Jul 17 23:09:23 2022\n",
      "Fold 9 started at Sun Jul 17 23:09:24 2022\n",
      "CV mean score: 2.0312, std: 0.1115.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q catboost\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import linear_model\n",
    "\n",
    "text_columns=['title', 'tagline', 'overview', 'keywords']\n",
    "train_texts = data[text_columns]\n",
    "test_texts = data_test[text_columns]\n",
    "\n",
    "X = data.drop(['target'], axis=1)\n",
    "y = np.log1p(data['target'])\n",
    "X_test = data_test.drop(['id'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    prediction = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if model_type == 'sklearn':\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "                    verbose=1000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_squared_error(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction\n",
    "\n",
    "\n",
    "\n",
    "for col in train_texts.columns:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=10\n",
    "    )\n",
    "    vectorizer.fit(list(train_texts[col].fillna('')) + list(test_texts[col].fillna('')))\n",
    "\n",
    "    \n",
    "    train_col_text = vectorizer.transform(train_texts[col].fillna(''))\n",
    "    test_col_text = vectorizer.transform(test_texts[col].fillna(''))\n",
    "    model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n",
    "    oof_text, prediction_text = train_model(train_col_text, test_col_text, y, params=None, model_type='sklearn', model=model)\n",
    "    \n",
    "    data[col + '_oof'] = oof_text\n",
    "    data_test[col + '_oof'] = prediction_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fac0b7d-f266-4f99-873c-377039688e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "for column in ['title', 'overview']:\n",
    "    data[column] = data[column].astype(str).apply(clean_text)\n",
    "    data_test[column] = data_test[column].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e065f0-53ef-4d2f-8755-7a9fecc901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_columns = list(data.drop(columns=['target', 'label']).select_dtypes(include=numerics).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a07dc8e8",
   "metadata": {
    "cell_id": "00012-f977f172-2409-44c1-9ff4-118d043985cc",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166470,
    "source_hash": "2dc8e0f4"
   },
   "outputs": [],
   "source": [
    "## Código Holdout\n",
    "data_labels=data[['target', 'label']]\n",
    "data_info=data.drop(columns=['target', 'label'])\n",
    "\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(data_info, data_labels.label,\n",
    "                                                                            random_state=42, test_size=0.2, stratify=data_labels.label)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(data_info, data_labels.target,\n",
    "                                                                            random_state=42, test_size=0.2)\n",
    "\n",
    "X_all_class=data_info\n",
    "y_all_class=data_labels.label\n",
    "\n",
    "X_all_reg=data_info\n",
    "y_all_reg=data_labels.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0787b2c-cc42-49c3-92d4-1a6d286f1b55",
   "metadata": {
    "cell_id": "00014-3a4f50bf-0f3a-496f-9360-ce23ceaba0cb",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166470,
    "source_hash": "2dc8e0f4"
   },
   "outputs": [],
   "source": [
    "#### Código aquí ####\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=10\n",
    "    )\n",
    "\n",
    "preprocessing_transformer_class = ColumnTransformer(\n",
    "    [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46be297d-5893-4ff9-8729-38421d0805fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidftitle__american</th>\n",
       "      <th>tfidftitle__away</th>\n",
       "      <th>tfidftitle__baby</th>\n",
       "      <th>tfidftitle__back</th>\n",
       "      <th>tfidftitle__bad</th>\n",
       "      <th>tfidftitle__ball</th>\n",
       "      <th>tfidftitle__batman</th>\n",
       "      <th>tfidftitle__battle</th>\n",
       "      <th>tfidftitle__best</th>\n",
       "      <th>tfidftitle__big</th>\n",
       "      <th>...</th>\n",
       "      <th>OneHotEncoder__original_language_no</th>\n",
       "      <th>OneHotEncoder__original_language_pl</th>\n",
       "      <th>OneHotEncoder__original_language_pt</th>\n",
       "      <th>OneHotEncoder__original_language_ro</th>\n",
       "      <th>OneHotEncoder__original_language_ru</th>\n",
       "      <th>OneHotEncoder__original_language_sv</th>\n",
       "      <th>OneHotEncoder__original_language_te</th>\n",
       "      <th>OneHotEncoder__original_language_th</th>\n",
       "      <th>OneHotEncoder__original_language_tr</th>\n",
       "      <th>OneHotEncoder__original_language_zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5160 rows × 3080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidftitle__american  tfidftitle__away  tfidftitle__baby  \\\n",
       "0                      0.0               0.0               0.0   \n",
       "1                      0.0               0.0               0.0   \n",
       "2                      0.0               0.0               0.0   \n",
       "3                      0.0               0.0               0.0   \n",
       "4                      0.0               0.0               0.0   \n",
       "...                    ...               ...               ...   \n",
       "5155                   0.0               0.0               0.0   \n",
       "5156                   0.0               0.0               0.0   \n",
       "5157                   0.0               0.0               0.0   \n",
       "5158                   0.0               0.0               0.0   \n",
       "5159                   0.0               0.0               0.0   \n",
       "\n",
       "      tfidftitle__back  tfidftitle__bad  tfidftitle__ball  tfidftitle__batman  \\\n",
       "0                  0.0              0.0               0.0                 0.0   \n",
       "1                  0.0              0.0               0.0                 0.0   \n",
       "2                  0.0              0.0               0.0                 0.0   \n",
       "3                  0.0              0.0               0.0                 0.0   \n",
       "4                  0.0              0.0               0.0                 0.0   \n",
       "...                ...              ...               ...                 ...   \n",
       "5155               0.0              0.0               0.0                 0.0   \n",
       "5156               0.0              0.0               0.0                 0.0   \n",
       "5157               0.0              0.0               0.0                 0.0   \n",
       "5158               0.0              0.0               0.0                 0.0   \n",
       "5159               0.0              0.0               0.0                 0.0   \n",
       "\n",
       "      tfidftitle__battle  tfidftitle__best  tfidftitle__big  ...  \\\n",
       "0                    0.0               0.0              0.0  ...   \n",
       "1                    0.0               0.0              0.0  ...   \n",
       "2                    0.0               0.0              0.0  ...   \n",
       "3                    0.0               0.0              0.0  ...   \n",
       "4                    0.0               0.0              0.0  ...   \n",
       "...                  ...               ...              ...  ...   \n",
       "5155                 0.0               0.0              0.0  ...   \n",
       "5156                 0.0               0.0              0.0  ...   \n",
       "5157                 0.0               0.0              0.0  ...   \n",
       "5158                 0.0               0.0              0.0  ...   \n",
       "5159                 0.0               0.0              0.0  ...   \n",
       "\n",
       "      OneHotEncoder__original_language_no  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_pl  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_pt  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_ro  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_ru  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_sv  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_te  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_th  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_tr  OneHotEncoder__original_language_zh  \n",
       "0                                     0.0                                  0.0  \n",
       "1                                     0.0                                  0.0  \n",
       "2                                     0.0                                  0.0  \n",
       "3                                     0.0                                  0.0  \n",
       "4                                     0.0                                  0.0  \n",
       "...                                   ...                                  ...  \n",
       "5155                                  0.0                                  0.0  \n",
       "5156                                  0.0                                  0.0  \n",
       "5157                                  0.0                                  0.0  \n",
       "5158                                  0.0                                  0.0  \n",
       "5159                                  0.0                                  0.0  \n",
       "\n",
       "[5160 rows x 3080 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=preprocessing_transformer_class.fit_transform(X_train_class).toarray(),\n",
    "            columns=preprocessing_transformer_class.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b2e93-6f1f-44f0-87b7-d2dbc88738b9",
   "metadata": {
    "cell_id": "ec9c0f8a2b044f858858ef3c3248c239",
    "deepnote_cell_height": 102,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "source": [
    "```\n",
    "Comentarios\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcbc0f",
   "metadata": {
    "cell_id": "00018-d6de5b4a-3ce2-4aaa-9421-121c4fcaf3b5",
    "deepnote_cell_height": 117.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Clasificación\n",
    "\n",
    "### 4.1 Dummy y Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6aaeb1d",
   "metadata": {
    "cell_id": "00020-830185e8-7ee6-44a5-89f0-ccb5e5d4ef76",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166499,
    "source_hash": "3e943dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.1772358601612866\n"
     ]
    }
   ],
   "source": [
    "## Código Dummy\n",
    "dummy_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", DummyClassifier(strategy=\"stratified\", random_state=42))])\n",
    "\n",
    "dummy_pipe.fit(X_train_class, y_train_class)\n",
    "dummy_y_pred = dummy_pipe.predict(X_test_class)\n",
    "\n",
    "print('F1:', f1_score(y_test_class, dummy_y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abb3f143-99e7-4d21-927f-72c0ef9335e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.34361880421069213 Modelo: GradientBoostingClassifier()\n",
      "F1: 0.3279457492980857 Modelo: LGBMClassifier()\n",
      "F1: 0.24739573851798807 Modelo: RandomForestClassifier()\n",
      "F1: 0.23621503883190806 Modelo: SVC()\n",
      "F1: 0.3111668270540872 Modelo: AdaBoostClassifier()\n",
      "F1: 0.31756763093628315 Modelo: VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
      "                             ('knn', LGBMClassifier()),\n",
      "                             ('svc', AdaBoostClassifier())])\n"
     ]
    }
   ],
   "source": [
    "## Código Comparación de métricas\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf2 = LGBMClassifier()\n",
    "clf3 = AdaBoostClassifier()\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)])\n",
    "\n",
    "modelos=[GradientBoostingClassifier(), LGBMClassifier(), RandomForestClassifier(), SVC(), AdaBoostClassifier(), eclf]\n",
    "\n",
    "for model in modelos:\n",
    "    class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", model)])\n",
    "    class_pipe.fit(X_train_class, y_train_class)\n",
    "    y_pred = class_pipe.predict(X_test_class)\n",
    "    \n",
    "    print('F1:', f1_score(y_test_class, y_pred, average='macro'), f'Modelo: {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4614f0af-ca73-4729-af5f-7c7e0bf5edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.30375832061388364 Modelo: GradientBoostingClassifier()\n",
      "F1: 0.30136121006042366 Modelo: LGBMClassifier()\n",
      "F1: 0.23320853310663633 Modelo: RandomForestClassifier()\n",
      "F1: 0.2229800936089344 Modelo: SVC()\n",
      "F1: 0.33130807410909224 Modelo: AdaBoostClassifier()\n",
      "F1: 0.2984209846387036 Modelo: VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
      "                             ('knn', LGBMClassifier()),\n",
      "                             ('svc', AdaBoostClassifier())])\n"
     ]
    }
   ],
   "source": [
    "## Código Comparación de métricas\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf2 = LGBMClassifier()\n",
    "clf3 = AdaBoostClassifier()\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)])\n",
    "\n",
    "modelos=[GradientBoostingClassifier(), LGBMClassifier(), RandomForestClassifier(), SVC(), AdaBoostClassifier(), eclf]\n",
    "\n",
    "for model in modelos:\n",
    "    class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", model)])\n",
    "    print('F1:', cross_val_score(class_pipe, X_all_class, y_all_class, cv=5, scoring='f1_macro').mean() , f'Modelo: {model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76428c9-d158-49c6-af36-13452ad0b7df",
   "metadata": {
    "cell_id": "8b46d8fd6f9544b199b67020abad2562",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "id": "53c33b23",
    "tags": []
   },
   "source": [
    "```\n",
    "Justificación\n",
    "```\n",
    "\n",
    "Como se ve, el modelo dummy entrega un F1-macro muy inferior con respecto a todos los modelos entrenados, esto dado que solo utiliza los labels y las probabilidades de estos para predecir.\n",
    "\n",
    "En cuanto a los modelos entrenados, los que presentaron los mejores resultados fueron los ensamblados, y aquellos que aprenden de sus errores en cada iteración (GBC y LGBM).\n",
    "\n",
    "A continuación se hará el GridSearch para los modelos de GBC y LGBMC, dado que fueron los que mejor F1-Macro obtuvieron (tanto de en los conjuntos definidos, como al hacer cross validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96167ac",
   "metadata": {
    "cell_id": "00021-c294ef41-853d-4297-b051-d5d4e6577715",
    "deepnote_cell_height": 61.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### 4.2 Búsqueda del mejor modelo de Clasificación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "120aebb2-3809-4d78-851c-d0b7425a7387",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('tfidftitle',\n",
       "                                    TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                    sublinear_tf=True),\n",
       "                                    'title'),\n",
       "                                   ('tfidfoverview',\n",
       "                                    TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                    sublinear_tf=True),\n",
       "                                    'overview'),\n",
       "                                   ('StandardScaler', MinMaxScaler(),\n",
       "                                    ['budget', 'runtime', 'release_year',\n",
       "                                     'release_day', 'release_month',\n",
       "                                     'release_date_day', 'weekend', 'genres_count...\n",
       "                                     'words_title_count', 'letters_tagline_count',\n",
       "                                     'words_tagline_count', 'genre_Fantasy',\n",
       "                                     'genre_Action', 'genre_Animation',\n",
       "                                     'genre_Crime', 'genre_Science Fiction',\n",
       "                                     'genre_Horror', 'genre_Adventure',\n",
       "                                     'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                     'genre_Thriller', 'genre_Romance',\n",
       "                                     'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                   ('OneHotEncoder',\n",
       "                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                    ['original_language'])])),\n",
       "  ('bf', SelectPercentile(percentile=100)),\n",
       "  ('clf',\n",
       "   VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
       "                                ('knn', LGBMClassifier()),\n",
       "                                ('svc', AdaBoostClassifier())]))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('tfidftitle',\n",
       "                                  TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                  sublinear_tf=True),\n",
       "                                  'title'),\n",
       "                                 ('tfidfoverview',\n",
       "                                  TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                  sublinear_tf=True),\n",
       "                                  'overview'),\n",
       "                                 ('StandardScaler', MinMaxScaler(),\n",
       "                                  ['budget', 'runtime', 'release_year',\n",
       "                                   'release_day', 'release_month',\n",
       "                                   'release_date_day', 'weekend', 'genres_count...\n",
       "                                   'words_title_count', 'letters_tagline_count',\n",
       "                                   'words_tagline_count', 'genre_Fantasy',\n",
       "                                   'genre_Action', 'genre_Animation',\n",
       "                                   'genre_Crime', 'genre_Science Fiction',\n",
       "                                   'genre_Horror', 'genre_Adventure',\n",
       "                                   'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                   'genre_Thriller', 'genre_Romance',\n",
       "                                   'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                 ('OneHotEncoder',\n",
       "                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                  ['original_language'])]),\n",
       " 'bf': SelectPercentile(percentile=100),\n",
       " 'clf': VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
       "                              ('knn', LGBMClassifier()),\n",
       "                              ('svc', AdaBoostClassifier())]),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('tfidftitle',\n",
       "   TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       "   'title'),\n",
       "  ('tfidfoverview',\n",
       "   TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       "   'overview'),\n",
       "  ('StandardScaler',\n",
       "   MinMaxScaler(),\n",
       "   ['budget',\n",
       "    'runtime',\n",
       "    'release_year',\n",
       "    'release_day',\n",
       "    'release_month',\n",
       "    'release_date_day',\n",
       "    'weekend',\n",
       "    'genres_count',\n",
       "    'actores_count',\n",
       "    'production_companies_count',\n",
       "    'keywords_count',\n",
       "    'letters_title_count',\n",
       "    'words_title_count',\n",
       "    'letters_tagline_count',\n",
       "    'words_tagline_count',\n",
       "    'genre_Fantasy',\n",
       "    'genre_Action',\n",
       "    'genre_Animation',\n",
       "    'genre_Crime',\n",
       "    'genre_Science Fiction',\n",
       "    'genre_Horror',\n",
       "    'genre_Adventure',\n",
       "    'genre_Family',\n",
       "    'genre_Comedy',\n",
       "    'genre_Drama',\n",
       "    'genre_Thriller',\n",
       "    'genre_Romance',\n",
       "    'genre_War',\n",
       "    'genre_Mystery',\n",
       "    'genre_Music',\n",
       "    'genre_Western',\n",
       "    'genre_Documentary',\n",
       "    'genre_History',\n",
       "    'genre_TV Movie',\n",
       "    'budget_actores',\n",
       "    'budget_anio',\n",
       "    'budget_runtime',\n",
       "    'log_budget',\n",
       "    'disney',\n",
       "    'fox',\n",
       "    'warner',\n",
       "    'nbc',\n",
       "    'sony',\n",
       "    'paramount',\n",
       "    'count_top_actors',\n",
       "    'count_top_directors',\n",
       "    'title_oof',\n",
       "    'tagline_oof',\n",
       "    'overview_oof',\n",
       "    'keywords_oof']),\n",
       "  ('OneHotEncoder',\n",
       "   OneHotEncoder(handle_unknown='ignore'),\n",
       "   ['original_language'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__verbose_feature_names_out': True,\n",
       " 'preprocessing__tfidftitle': TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       " 'preprocessing__tfidfoverview': TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       " 'preprocessing__StandardScaler': MinMaxScaler(),\n",
       " 'preprocessing__OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__tfidftitle__analyzer': 'word',\n",
       " 'preprocessing__tfidftitle__binary': False,\n",
       " 'preprocessing__tfidftitle__decode_error': 'strict',\n",
       " 'preprocessing__tfidftitle__dtype': numpy.float64,\n",
       " 'preprocessing__tfidftitle__encoding': 'utf-8',\n",
       " 'preprocessing__tfidftitle__input': 'content',\n",
       " 'preprocessing__tfidftitle__lowercase': True,\n",
       " 'preprocessing__tfidftitle__max_df': 1.0,\n",
       " 'preprocessing__tfidftitle__max_features': None,\n",
       " 'preprocessing__tfidftitle__min_df': 10,\n",
       " 'preprocessing__tfidftitle__ngram_range': (1, 2),\n",
       " 'preprocessing__tfidftitle__norm': 'l2',\n",
       " 'preprocessing__tfidftitle__preprocessor': None,\n",
       " 'preprocessing__tfidftitle__smooth_idf': True,\n",
       " 'preprocessing__tfidftitle__stop_words': None,\n",
       " 'preprocessing__tfidftitle__strip_accents': None,\n",
       " 'preprocessing__tfidftitle__sublinear_tf': True,\n",
       " 'preprocessing__tfidftitle__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'preprocessing__tfidftitle__tokenizer': None,\n",
       " 'preprocessing__tfidftitle__use_idf': True,\n",
       " 'preprocessing__tfidftitle__vocabulary': None,\n",
       " 'preprocessing__tfidfoverview__analyzer': 'word',\n",
       " 'preprocessing__tfidfoverview__binary': False,\n",
       " 'preprocessing__tfidfoverview__decode_error': 'strict',\n",
       " 'preprocessing__tfidfoverview__dtype': numpy.float64,\n",
       " 'preprocessing__tfidfoverview__encoding': 'utf-8',\n",
       " 'preprocessing__tfidfoverview__input': 'content',\n",
       " 'preprocessing__tfidfoverview__lowercase': True,\n",
       " 'preprocessing__tfidfoverview__max_df': 1.0,\n",
       " 'preprocessing__tfidfoverview__max_features': None,\n",
       " 'preprocessing__tfidfoverview__min_df': 10,\n",
       " 'preprocessing__tfidfoverview__ngram_range': (1, 2),\n",
       " 'preprocessing__tfidfoverview__norm': 'l2',\n",
       " 'preprocessing__tfidfoverview__preprocessor': None,\n",
       " 'preprocessing__tfidfoverview__smooth_idf': True,\n",
       " 'preprocessing__tfidfoverview__stop_words': None,\n",
       " 'preprocessing__tfidfoverview__strip_accents': None,\n",
       " 'preprocessing__tfidfoverview__sublinear_tf': True,\n",
       " 'preprocessing__tfidfoverview__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'preprocessing__tfidfoverview__tokenizer': None,\n",
       " 'preprocessing__tfidfoverview__use_idf': True,\n",
       " 'preprocessing__tfidfoverview__vocabulary': None,\n",
       " 'preprocessing__StandardScaler__clip': False,\n",
       " 'preprocessing__StandardScaler__copy': True,\n",
       " 'preprocessing__StandardScaler__feature_range': (0, 1),\n",
       " 'preprocessing__OneHotEncoder__categories': 'auto',\n",
       " 'preprocessing__OneHotEncoder__drop': None,\n",
       " 'preprocessing__OneHotEncoder__dtype': numpy.float64,\n",
       " 'preprocessing__OneHotEncoder__handle_unknown': 'ignore',\n",
       " 'preprocessing__OneHotEncoder__max_categories': None,\n",
       " 'preprocessing__OneHotEncoder__min_frequency': None,\n",
       " 'preprocessing__OneHotEncoder__sparse': True,\n",
       " 'bf__percentile': 100,\n",
       " 'bf__score_func': <function sklearn.feature_selection._univariate_selection.f_classif(X, y)>,\n",
       " 'clf__estimators': [('dt', GradientBoostingClassifier()),\n",
       "  ('knn', LGBMClassifier()),\n",
       "  ('svc', AdaBoostClassifier())],\n",
       " 'clf__flatten_transform': True,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__verbose': False,\n",
       " 'clf__voting': 'hard',\n",
       " 'clf__weights': None,\n",
       " 'clf__dt': GradientBoostingClassifier(),\n",
       " 'clf__knn': LGBMClassifier(),\n",
       " 'clf__svc': AdaBoostClassifier(),\n",
       " 'clf__dt__ccp_alpha': 0.0,\n",
       " 'clf__dt__criterion': 'friedman_mse',\n",
       " 'clf__dt__init': None,\n",
       " 'clf__dt__learning_rate': 0.1,\n",
       " 'clf__dt__loss': 'log_loss',\n",
       " 'clf__dt__max_depth': 3,\n",
       " 'clf__dt__max_features': None,\n",
       " 'clf__dt__max_leaf_nodes': None,\n",
       " 'clf__dt__min_impurity_decrease': 0.0,\n",
       " 'clf__dt__min_samples_leaf': 1,\n",
       " 'clf__dt__min_samples_split': 2,\n",
       " 'clf__dt__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__dt__n_estimators': 100,\n",
       " 'clf__dt__n_iter_no_change': None,\n",
       " 'clf__dt__random_state': None,\n",
       " 'clf__dt__subsample': 1.0,\n",
       " 'clf__dt__tol': 0.0001,\n",
       " 'clf__dt__validation_fraction': 0.1,\n",
       " 'clf__dt__verbose': 0,\n",
       " 'clf__dt__warm_start': False,\n",
       " 'clf__knn__boosting_type': 'gbdt',\n",
       " 'clf__knn__class_weight': None,\n",
       " 'clf__knn__colsample_bytree': 1.0,\n",
       " 'clf__knn__importance_type': 'split',\n",
       " 'clf__knn__learning_rate': 0.1,\n",
       " 'clf__knn__max_depth': -1,\n",
       " 'clf__knn__min_child_samples': 20,\n",
       " 'clf__knn__min_child_weight': 0.001,\n",
       " 'clf__knn__min_split_gain': 0.0,\n",
       " 'clf__knn__n_estimators': 100,\n",
       " 'clf__knn__n_jobs': -1,\n",
       " 'clf__knn__num_leaves': 31,\n",
       " 'clf__knn__objective': None,\n",
       " 'clf__knn__random_state': None,\n",
       " 'clf__knn__reg_alpha': 0.0,\n",
       " 'clf__knn__reg_lambda': 0.0,\n",
       " 'clf__knn__silent': 'warn',\n",
       " 'clf__knn__subsample': 1.0,\n",
       " 'clf__knn__subsample_for_bin': 200000,\n",
       " 'clf__knn__subsample_freq': 0,\n",
       " 'clf__svc__algorithm': 'SAMME.R',\n",
       " 'clf__svc__base_estimator': None,\n",
       " 'clf__svc__learning_rate': 1.0,\n",
       " 'clf__svc__n_estimators': 50,\n",
       " 'clf__svc__random_state': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b0cf06b-a907-49e8-97d2-c3335275811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3564 candidates, totalling 17820 fits\n",
      "0.347376358401553 {'bf__percentile': 50, 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 1: vectorizer y percentile\n",
    "    {\n",
    "        \"bf__percentile\": range(50, 101, 5),\n",
    "        'preprocessing__tfidftitle__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [1, 3, 10],\n",
    "        'preprocessing__tfidfoverview__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [1, 3, 10],\n",
    "                            }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5b72277-da9c-49ef-be44-ed29d55af87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "0.34814444529711136 {'bf__percentile': 50, 'clf': AdaBoostClassifier(), 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 2: percentile y modelos\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [GradientBoostingClassifier(n)],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3]\n",
    "    },\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [AdaBoostClassifier()],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3]\n",
    "    }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs2 = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs2.best_score_, gs2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6ea580d-c1b3-4411-ac50-ae8748188520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "0.32266080235362954 {'bf__percentile': 50, 'clf': AdaBoostClassifier(base_estimator=GradientBoostingClassifier(),\n",
      "                   learning_rate=1.5, n_estimators=25), 'clf__algorithm': 'SAMME.R', 'clf__base_estimator': GradientBoostingClassifier(), 'clf__learning_rate': 1.5, 'clf__n_estimators': 25, 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 3: hiperparametros modelo\n",
    "    {\n",
    "        \"bf__percentile\": [50],\n",
    "        \"clf\": [AdaBoostClassifier()],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3],\n",
    "        'clf__base_estimator': [RandomForestClassifier(), GradientBoostingClassifier()],\n",
    "        'clf__n_estimators': [10, 25, 50],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.5, 1.0, 1.5],\n",
    "        'clf__algorithm': ['SAMME', 'SAMME.R']\n",
    "    }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs3 = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs3.best_score_, gs3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e920b9-fa78-4bca-a128-b52b315d06ae",
   "metadata": {},
   "source": [
    "Bajó su desempeño, se volverá a hacer el GridSearch sin modificar el base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5bfd7782-a09a-4477-b146-32309cb2d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n",
      "0.34780252055318284 {'bf__percentile': 50, 'clf': AdaBoostClassifier(), 'clf__algorithm': 'SAMME.R', 'clf__learning_rate': 1.0, 'clf__n_estimators': 50, 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 3: hiperparametros modelo\n",
    "    {\n",
    "        \"bf__percentile\": [45, 50, 55],\n",
    "        \"clf\": [AdaBoostClassifier()],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3],\n",
    "        # 'clf__base_estimator': [RandomForestClassifier(), GradientBoostingClassifier()],\n",
    "        'clf__n_estimators': [10, 25, 40, 50, 60],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.5, 0.8, 1.0, 1.2, 1.5],\n",
    "        'clf__algorithm': ['SAMME', 'SAMME.R']\n",
    "    }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=50)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs4 = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs4.best_score_, gs4.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13993f22-63af-44fe-bb1c-32bd2ec3f915",
   "metadata": {},
   "source": [
    "Finalmente, guardando el mejor modelo luego de varios GridSearch para el AdaBoostClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c75d656-33b0-4804-beb6-1b9708e6fd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3675788440827209\n"
     ]
    }
   ],
   "source": [
    "vectorizer_1 = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                norm='l1'\n",
    "    )\n",
    "\n",
    "vectorizer_2 = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                ngram_range=(2, 2),\n",
    "                min_df=3,\n",
    "                norm='l1'\n",
    "    )\n",
    "\n",
    "\n",
    "preprocessing_transformer_class = ColumnTransformer(\n",
    "    [('tfidftitle', vectorizer_1, 'title'),\n",
    "     ('tfidfoverview', vectorizer_2, 'overview'),\n",
    "     ('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=50)),\n",
    "                        (\"clf\", AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1.0, n_estimators=50))])\n",
    "\n",
    "class_pipe.fit(X_all_class, y_all_class)\n",
    "y_pred = class_pipe.predict(X_all_class)\n",
    "\n",
    "print('F1:', f1_score(y_all_class, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bea09-5d03-4e96-8737-ee8a0d5bfddf",
   "metadata": {},
   "source": [
    "Probando ahoora el GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ac81218-907f-4389-8149-dbe3990b7449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "F1: 0.8247417666055202\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingC\n",
    "learning_rate = [0.02,0.03]\n",
    "n_estimators = [100, 500, 1000]\n",
    "subsample = [0.9, 0.5, 0.2]\n",
    "max_depth = [4,6,8]\n",
    "\n",
    "hyperparams = {'n_estimators': n_estimators, 'subsample': subsample}\n",
    "\n",
    "\n",
    "gd_gbc=GridSearchCV(estimator = GradientBoostingClassifier(n_estimators=500), param_grid = hyperparams, \n",
    "                verbose=3, cv=5, scoring = 'f1_macro', n_jobs=-1)\n",
    "\n",
    "class_pipe2 = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=40)),\n",
    "                        (\"clf\", gd_gbc)])\n",
    "\n",
    "class_pipe2.fit(X_train_class, y_train_class)\n",
    "y_pred = class_pipe2.predict(X_all_class)\n",
    "print('F1:', f1_score(y_all_class, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47e18b2e-da4c-473a-b142-4bdedd6a39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3509967149571193 {'n_estimators': 1000, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(gd_gbc.best_score_, gd_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcbe25-ed8b-4edb-92ef-de9212a685ca",
   "metadata": {},
   "source": [
    "Viendo las features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbf8860d-d638-46be-b678-5a6cb55082ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StandardScaler__runtime</td>\n",
       "      <td>0.074299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StandardScaler__release_year</td>\n",
       "      <td>0.059640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StandardScaler__keywords_count</td>\n",
       "      <td>0.033530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StandardScaler__genre_Drama</td>\n",
       "      <td>0.025186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>StandardScaler__overview_oof</td>\n",
       "      <td>0.018516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>StandardScaler__budget_actores</td>\n",
       "      <td>0.018264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>StandardScaler__budget_runtime</td>\n",
       "      <td>0.018123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>StandardScaler__keywords_oof</td>\n",
       "      <td>0.017435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>StandardScaler__genre_Animation</td>\n",
       "      <td>0.016915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OneHotEncoder__original_language_en</td>\n",
       "      <td>0.016793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>StandardScaler__title_oof</td>\n",
       "      <td>0.015657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>StandardScaler__actores_count</td>\n",
       "      <td>0.014517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>StandardScaler__tagline_oof</td>\n",
       "      <td>0.014443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>StandardScaler__letters_tagline_count</td>\n",
       "      <td>0.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>StandardScaler__genre_Documentary</td>\n",
       "      <td>0.009267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>StandardScaler__budget_anio</td>\n",
       "      <td>0.008501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StandardScaler__letters_title_count</td>\n",
       "      <td>0.007524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>StandardScaler__release_month</td>\n",
       "      <td>0.006806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>StandardScaler__release_day</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StandardScaler__genre_Horror</td>\n",
       "      <td>0.005578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Feature  importance\n",
       "0                 StandardScaler__runtime    0.074299\n",
       "1            StandardScaler__release_year    0.059640\n",
       "2          StandardScaler__keywords_count    0.033530\n",
       "3             StandardScaler__genre_Drama    0.025186\n",
       "4            StandardScaler__overview_oof    0.018516\n",
       "5          StandardScaler__budget_actores    0.018264\n",
       "6          StandardScaler__budget_runtime    0.018123\n",
       "7            StandardScaler__keywords_oof    0.017435\n",
       "8         StandardScaler__genre_Animation    0.016915\n",
       "9     OneHotEncoder__original_language_en    0.016793\n",
       "10              StandardScaler__title_oof    0.015657\n",
       "11          StandardScaler__actores_count    0.014517\n",
       "12            StandardScaler__tagline_oof    0.014443\n",
       "13  StandardScaler__letters_tagline_count    0.010498\n",
       "14      StandardScaler__genre_Documentary    0.009267\n",
       "15            StandardScaler__budget_anio    0.008501\n",
       "16    StandardScaler__letters_title_count    0.007524\n",
       "17          StandardScaler__release_month    0.006806\n",
       "18            StandardScaler__release_day    0.006599\n",
       "19           StandardScaler__genre_Horror    0.005578"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_importances=pd.DataFrame({'Feature': preprocessing_transformer_class.get_feature_names_out(), 'Select': class_pipe2.steps[1][1].get_support()})\n",
    "feat_importances=feat_importances[feat_importances.Select==True]\n",
    "feat_importances['importance']=class_pipe2.steps[2][1].best_estimator_.feature_importances_\n",
    "feat_importances.sort_values(by='importance', ascending=False)[['Feature', 'importance']][:20].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ad86e",
   "metadata": {
    "cell_id": "00023-73786884-e1b2-448a-9636-ab05cf3c1fc5",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "```\n",
    "Justificación Aquí\n",
    "```\n",
    "Como se puede ver, el GridSearch del GBC da mucho mejor resultado que todo el GridSearch realizado para el AdaBoost, esto principalmente por el gran número de estimadores a utilizar (1000), sin embargo para la predicción final se utilizó la cantidad de 500 dado que con 1000 es muy posible que estuviera sobreajustando y no generalizando de buena manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6514b",
   "metadata": {
    "cell_id": "2d58aececbe34d6184477c8e3cfaa2e3",
    "deepnote_cell_height": 117.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Regresión\n",
    "\n",
    "### 5.1 Dummy y Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a57b61b",
   "metadata": {
    "cell_id": "86fd465c690a46d1bafed3115c0bceff",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: -0.0004752178066032009\n"
     ]
    }
   ],
   "source": [
    "## Código Dummy\n",
    "dummy_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", DummyRegressor())])\n",
    "\n",
    "dummy_pipe.fit(X_train_reg, y_train_reg)\n",
    "dummy_y_pred = dummy_pipe.predict(X_test_reg)\n",
    "\n",
    "print('R²:', r2_score(y_test_reg, dummy_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55467133-ab67-4cd2-bc8c-2b509cba1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.5700759442917105 Modelo: RandomForestRegressor()\n",
      "R²: 0.5610659144946457 Modelo: SGDRegressor()\n",
      "R²: 0.5716157260849514 Modelo: GradientBoostingRegressor()\n",
      "R²: 0.5448146031357388 Modelo: BaggingRegressor()\n",
      "R²: 0.5740716858110675 Modelo: VotingRegressor(estimators=[('gb',\n",
      "                             GradientBoostingRegressor(learning_rate=0.02,\n",
      "                                                       n_estimators=1500)),\n",
      "                            ('rf',\n",
      "                             RandomForestRegressor(bootstrap=False,\n",
      "                                                   max_features='sqrt')),\n",
      "                            ('lr', BaggingRegressor())])\n"
     ]
    }
   ],
   "source": [
    "## Código Comparación de métricas\n",
    "reg1 = GradientBoostingRegressor(learning_rate=0.02, n_estimators=1500)\n",
    "reg2 = RandomForestRegressor(bootstrap=False, max_features='sqrt')\n",
    "reg3 = BaggingRegressor()\n",
    "vr = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "modelos=[RandomForestRegressor(), SGDRegressor(), GradientBoostingRegressor(), BaggingRegressor(),\n",
    "         vr]\n",
    "\n",
    "for model in modelos:\n",
    "    reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", model)])\n",
    "    reg_pipe.fit(X_train_reg, y_train_reg)\n",
    "    y_pred = reg_pipe.predict(X_test_reg)\n",
    "    \n",
    "    print('R²:', r2_score(y_test_reg, y_pred), f'Modelo: {model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122122e3-505b-4ad3-ab80-1ef91fc56fca",
   "metadata": {
    "cell_id": "23ba7e6c56c841d8b2c11563ca64bf20",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "id": "59354a18",
    "tags": []
   },
   "source": [
    "```\n",
    "Justificación\n",
    "```\n",
    "En este caso de regresión, el modelo dummy también entrega un R2 cercano a 0 (es decir no se ajusta para nada a los datos), y muy inferior con respecto a todos los modelos entrenados. Esto dado que solo utiliza el promedio de la variable a predecir, asignando este valor como predicción a todas las entradas.\n",
    "\n",
    "En cuanto a los modelos entrenados, los que presentaron los mejores resultados fueron los ensamblados, entre ellos se encuentran el RandomForest el GradientBoosting, además de la combinación de estos en el VotingRegressor.\n",
    "\n",
    "A continuación se hará el GridSearch para lps modelos de GBC y RFR dado que fueron los que mejor R2 tuvieron sobre el conjunto de testeo (se podría ahondar en el VotingRegressor pero por temas de tiempo y complejidad de realizar el GridSearch se omitirá este modelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db11df",
   "metadata": {
    "cell_id": "df491c3ed9704211be52235df236b889",
    "deepnote_cell_height": 61.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### 5.2 Búsqueda del mejor modelo de Regresión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e4b6a0-e9a8-4c51-b18e-4b57743fd172",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "c60f85b8-2fc9-4943-8e16-023763fd7c62",
    "outputId": "50dcfec5-655d-400a-fa2e-65224e2bfbe5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-583e8f1cf831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                         (\"clf\", RandomForestRegressor(random_state=3))])\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 1: vectorizer\n",
    "    {\n",
    "        'preprocessing__tfidftitle__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [1, 3, 10],\n",
    "        'preprocessing__tfidfoverview__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [1, 3, 10],\n",
    "                            }\n",
    "]\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", RandomForestRegressor(random_state=3))])\n",
    "\n",
    "gs = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038874b-df7c-4b2e-8077-4ed22c3bfaae",
   "metadata": {},
   "source": [
    "Dado que tardó demasiado el GridSearch con la realización del vectorizer se optó por deshabilitar este paso del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45456b88-c8d8-49ba-9e37-bc84c728348b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('StandardScaler', MinMaxScaler(),\n",
       "                                    ['budget', 'runtime', 'release_year',\n",
       "                                     'release_day', 'release_month',\n",
       "                                     'release_date_day', 'weekend', 'genres_count',\n",
       "                                     'actores_count', 'production_companies_count',\n",
       "                                     'keywords_count', 'letters_title_count',\n",
       "                                     'words_title_count', 'letters_tagline_count',\n",
       "                                     'words_tagline_count', 'genre_Fantasy',\n",
       "                                     'genre_Action', 'genre_Animation',\n",
       "                                     'genre_Crime', 'genre_Science Fiction',\n",
       "                                     'genre_Horror', 'genre_Adventure',\n",
       "                                     'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                     'genre_Thriller', 'genre_Romance',\n",
       "                                     'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                   ('OneHotEncoder',\n",
       "                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                    ['original_language'])])),\n",
       "  ('bf', SelectPercentile(percentile=100)),\n",
       "  ('clf', RandomForestRegressor())],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('StandardScaler', MinMaxScaler(),\n",
       "                                  ['budget', 'runtime', 'release_year',\n",
       "                                   'release_day', 'release_month',\n",
       "                                   'release_date_day', 'weekend', 'genres_count',\n",
       "                                   'actores_count', 'production_companies_count',\n",
       "                                   'keywords_count', 'letters_title_count',\n",
       "                                   'words_title_count', 'letters_tagline_count',\n",
       "                                   'words_tagline_count', 'genre_Fantasy',\n",
       "                                   'genre_Action', 'genre_Animation',\n",
       "                                   'genre_Crime', 'genre_Science Fiction',\n",
       "                                   'genre_Horror', 'genre_Adventure',\n",
       "                                   'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                   'genre_Thriller', 'genre_Romance',\n",
       "                                   'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                 ('OneHotEncoder',\n",
       "                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                  ['original_language'])]),\n",
       " 'bf': SelectPercentile(percentile=100),\n",
       " 'clf': RandomForestRegressor(),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('StandardScaler',\n",
       "   MinMaxScaler(),\n",
       "   ['budget',\n",
       "    'runtime',\n",
       "    'release_year',\n",
       "    'release_day',\n",
       "    'release_month',\n",
       "    'release_date_day',\n",
       "    'weekend',\n",
       "    'genres_count',\n",
       "    'actores_count',\n",
       "    'production_companies_count',\n",
       "    'keywords_count',\n",
       "    'letters_title_count',\n",
       "    'words_title_count',\n",
       "    'letters_tagline_count',\n",
       "    'words_tagline_count',\n",
       "    'genre_Fantasy',\n",
       "    'genre_Action',\n",
       "    'genre_Animation',\n",
       "    'genre_Crime',\n",
       "    'genre_Science Fiction',\n",
       "    'genre_Horror',\n",
       "    'genre_Adventure',\n",
       "    'genre_Family',\n",
       "    'genre_Comedy',\n",
       "    'genre_Drama',\n",
       "    'genre_Thriller',\n",
       "    'genre_Romance',\n",
       "    'genre_War',\n",
       "    'genre_Mystery',\n",
       "    'genre_Music',\n",
       "    'genre_Western',\n",
       "    'genre_Documentary',\n",
       "    'genre_History',\n",
       "    'genre_TV Movie',\n",
       "    'budget_actores',\n",
       "    'budget_anio',\n",
       "    'budget_runtime',\n",
       "    'log_budget',\n",
       "    'disney',\n",
       "    'fox',\n",
       "    'warner',\n",
       "    'nbc',\n",
       "    'sony',\n",
       "    'paramount',\n",
       "    'count_top_actors',\n",
       "    'count_top_directors',\n",
       "    'title_oof',\n",
       "    'tagline_oof',\n",
       "    'overview_oof',\n",
       "    'keywords_oof']),\n",
       "  ('OneHotEncoder',\n",
       "   OneHotEncoder(handle_unknown='ignore'),\n",
       "   ['original_language'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__verbose_feature_names_out': True,\n",
       " 'preprocessing__StandardScaler': MinMaxScaler(),\n",
       " 'preprocessing__OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__StandardScaler__clip': False,\n",
       " 'preprocessing__StandardScaler__copy': True,\n",
       " 'preprocessing__StandardScaler__feature_range': (0, 1),\n",
       " 'preprocessing__OneHotEncoder__categories': 'auto',\n",
       " 'preprocessing__OneHotEncoder__drop': None,\n",
       " 'preprocessing__OneHotEncoder__dtype': numpy.float64,\n",
       " 'preprocessing__OneHotEncoder__handle_unknown': 'ignore',\n",
       " 'preprocessing__OneHotEncoder__max_categories': None,\n",
       " 'preprocessing__OneHotEncoder__min_frequency': None,\n",
       " 'preprocessing__OneHotEncoder__sparse': True,\n",
       " 'bf__percentile': 100,\n",
       " 'bf__score_func': <function sklearn.feature_selection._univariate_selection.f_classif(X, y)>,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__ccp_alpha': 0.0,\n",
       " 'clf__criterion': 'squared_error',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 1.0,\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__max_samples': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 100,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    # [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", RandomForestRegressor())])\n",
    "\n",
    "reg_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c60f85b8-2fc9-4943-8e16-023763fd7c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid = [\n",
    "#     # grilla 1: vectorizer\n",
    "#     {\n",
    "#         'preprocessing__tfidftitle__norm': ('l1', 'l2'),\n",
    "#         'preprocessing__tfidftitle__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "#         'preprocessing__tfidftitle__min_df': [1, 3, 10],\n",
    "#         'preprocessing__tfidfoverview__norm': ('l1', 'l2'),\n",
    "#         'preprocessing__tfidfoverview__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "#         'preprocessing__tfidfoverview__min_df': [1, 3, 10],\n",
    "#                             }\n",
    "# ]\n",
    "# reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "#                         (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "#                         (\"clf\", RandomForestRegressor(random_state=3))])\n",
    "\n",
    "# gs = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392c89f2-655d-4dc3-9c4b-7bf7fff4d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "0.05557985195183481 {'bf__percentile': 80, 'clf': GradientBoostingRegressor()}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 2: percentile y modelos\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "    },\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [RandomForestRegressor()],\n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "gs2 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs2.best_score_, gs2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace3f67e-a59b-4e18-9903-b8b8e5e9ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "0.06119467558900451 {'bf__percentile': 85, 'clf': GradientBoostingRegressor(criterion='squared_error'), 'clf__criterion': 'squared_error', 'clf__learning_rate': 0.1, 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 3: hiperparametros modelo v1\n",
    "    {\n",
    "        \"bf__percentile\": [80, 85, 90, 100],\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "        'clf__n_estimators': [25, 50, 100, 200],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.5, 1.0, 1.5],\n",
    "        'clf__criterion': ['friedman_mse', 'squared_error']\n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "\n",
    "gs3 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs3.best_score_, gs3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae43837-1037-4def-9d4f-f54017d43669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "0.07605075955315092 {'bf__percentile': 85, 'clf': GradientBoostingRegressor(criterion='squared_error', max_depth=5,\n",
      "                          max_features='sqrt', min_samples_split=9), 'clf__criterion': 'squared_error', 'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 9, 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 4: hiperparametros modelo v2\n",
    "    {\n",
    "        \"bf__percentile\": [80, 85, 90],\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "        'clf__n_estimators': [100],\n",
    "        'clf__learning_rate': [0.1],\n",
    "        'clf__criterion': ['squared_error'],\n",
    "        'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'clf__min_samples_split': [2, 5, 9],\n",
    "        'clf__max_depth': [3, 5, 9],\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "\n",
    "gs4 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs4.best_score_, gs4.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8c519ef-2520-48b1-9316-b58607287ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "0.08553275202699868 {'bf__percentile': 85, 'clf': GradientBoostingRegressor(criterion='squared_error', max_depth=5,\n",
      "                          max_features='sqrt', min_samples_leaf=8,\n",
      "                          min_samples_split=15, subsample=0.97), 'clf__criterion': 'squared_error', 'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 8, 'clf__min_samples_split': 15, 'clf__n_estimators': 100, 'clf__subsample': 0.97}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 5: hiperparametros modelo v3\n",
    "    {\n",
    "        \"bf__percentile\": [85],\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "        'clf__n_estimators': [100],\n",
    "        'clf__learning_rate': [0.1],\n",
    "        'clf__criterion': ['squared_error'],\n",
    "        'clf__max_features': ['sqrt'],\n",
    "        'clf__min_samples_split': [15, 20, 35],\n",
    "        'clf__max_depth': [5],\n",
    "        'clf__subsample': [0.95, 0.97, 0.985, 1.0],\n",
    "        'clf__min_samples_leaf': [1, 3, 5, 8, 10, 12],\n",
    "        \n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "\n",
    "gs5 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs5.best_score_, gs5.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91775cbe",
   "metadata": {
    "cell_id": "b00b9baef48947d9abcfb6972bdaa3aa",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.8087562984111456\n"
     ]
    }
   ],
   "source": [
    "### Código Predicción de datos de la competencia aquí\n",
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    # [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=85)),\n",
    "                        (\"clf\", GradientBoostingRegressor(criterion='squared_error', max_depth=5,\n",
    "                          max_features='sqrt', min_samples_leaf=8,\n",
    "                          min_samples_split=15, subsample=0.97))])\n",
    "\n",
    "reg_pipe.fit(X_all_reg, y_all_reg)\n",
    "y_pred = reg_pipe.predict(X_all_reg)\n",
    "\n",
    "print('R²:', r2_score(y_all_reg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3add7e-5cdd-4e13-88f2-bef1c2c345e0",
   "metadata": {},
   "source": [
    "Probando el RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ef58c3f-7d85-4ba8-86c5-90646ddc405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "0.015364982796172866 {'bf__percentile': 80, 'clf': RandomForestRegressor(criterion='poisson'), 'clf__criterion': 'poisson'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 2: percentile y modelos\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [RandomForestRegressor()],\n",
    "        'clf__criterion': ['squared_error', 'absolute_error', 'poisson']\n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "gsrf = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gsrf.best_score_, gsrf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a68ae766-3f21-4009-8034-891187152dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.9436675084604951\n"
     ]
    }
   ],
   "source": [
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    # [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", RandomForestRegressor(criterion='poisson'))])\n",
    "\n",
    "reg_pipe.fit(X_all_reg, y_all_reg)\n",
    "y_pred = reg_pipe.predict(X_all_reg)\n",
    "\n",
    "print('R²:', r2_score(y_all_reg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15c2f9-6c45-4512-84a4-96bf55b74091",
   "metadata": {
    "cell_id": "23ba7e6c56c841d8b2c11563ca64bf20",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "id": "59354a18",
    "tags": []
   },
   "source": [
    "```\n",
    "Justificación\n",
    "```\n",
    "En este caso de regresión, el RandomForestRegressor, al ajustarle el parámetro de criterio de error a poisson logró mejorar considerablemente lo obtenido por el modelo de GradientBoosting, es pore sto que también fue utilizado como modelo final al momento de competir. (No se ahondo en el GridSearch de este modelo dada la demora que se tiene.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dcbcc",
   "metadata": {
    "cell_id": "00025-2acf9c12-da85-4c1f-add5-f2b0f600177f",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0ea2a",
   "metadata": {
    "cell_id": "00026-15c07b20-0e16-48fa-bf3c-33aeb2c4c1db",
    "deepnote_cell_height": 51.53334045410156,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Los resultados de la competencia son los que se muestran a continuación:\n",
    "    - Problema de clasificación: 0.84 F1 - Score\n",
    "    - Problema de regresión: 0.91 R2\n",
    "\n",
    "En base a estos resultados se cree que la resolución del problema se logró de buena manera dado que la métrica de F1 Score en la clasificación alcanzó valores de 0.84 en los datos a clasificar, lo cual indica que de alguna manera que la presición y el recall del modelo están obteniendo buenos resultados, sin embargo debemos tener cuidado con la clasificación de las minoritarias (en general bajaban bastante los score). Por su parte, el modelo de regresión se ajusta de muy buena manera a las predicciones de éxito monetario que tendría una película, esto se debe principalmente la ingeniería de variables realizada como lo eran el conteo de algunas variables de texto, la vectorización de texto de las variables como overview y keyword, los ratios de budget vs runtime, el conteo de actores y productores relevantes, y también al ajuste de hiperparámetros para el modelo de regresión.\n",
    "\n",
    "Para llegar a esto, el EDA resultó útil de tal manera que se lograría entender principalmente cómo malear o manejar las features con las que ya se contaba, sin embargo, creemos como equipo que la ingeniería de variables no se basó principalmente en el eda, sino en la creatividad sobre cómo expresar de mejor manera los datos con tal de obtener mejores caracterizaciones. Para esto también creemos que ayudó bastante entender el contexto de estar trabajando en un problema orientado al mundo de las películas, por lo cual debíamos también agregar features realacionadas a eso, como los mejores actores, mejores productoras, etc.\n",
    "\n",
    "En cuanto a los modelos, se tiene que tanto el modelo clasificador como el modelo de regresión en versión dummy entregaban métricas que no eran aceptables para una solución, en este sentido el baseline fue fácil de superar sobretodo en la regresión, ya que este es un problema más específico. En cuanto a otros modelos, se utilizaron algunos simples como KNN y DesicionTreeClassifier en la clasificación y LinearRegresor para la regresión que no entregarían resultados muy elevados, y que los clasificadores boosteados sobrepasarían muy fácilmente. Por esto se utilizó el GradientBoostClassifier entregando las mejores métricas con los hiperparámetros por default al igual que AdaBoosting. Luego estos serían ingresados a una grilla y el modelo de GradientBoost terminaría ofreciendo mejores métricas. Lo mismo se hizo para el regresor, en donde modelos como Lasso, GradientBoost y RandomForestRegressor entregarían los mejores resultados, los cuales se juntaron en un clasificador de votación sin mayor éxito y se ingresaron en una grilla entregando el mejor modelo como RandomForestRegressor.\n",
    "\n",
    "Cabe destacar que luego de la grilla se ajustaron algunos parámetros a mano dado, esto dado que la grilla a veces no ofrecía los mejores resultados que si lograbamos con un ajuste a mano de algunos hiperparámetros.\n",
    "\n",
    "Por último, como equipo nos encontramos sumamente conformes con el desempeño del proyecto, obteniendo buenos lugares en la tabla de resultados, sin embargo dado que en la misma tabla existen mejores resultados en la clasificación, creemos que claramente el clasificador cuenta con un gran margen de mejoras, esto se podría hacer obteniendo vectores de semántica de texto y algunas redes neuronales de texto para así entrenar un modelo más robusto.\n",
    "\n",
    "En cuanto a lo aprendido dentro de este proceso, creemos que fue la metodología a seguir para llegar a cabo un protyecto de competencia que mantiene todas las características de un problemas de ciencia de datos que se puede presentar en la vida real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790b480",
   "metadata": {
    "cell_id": "00027-1e362e1d-a776-4423-93d5-0f568476e4c1",
    "deepnote_cell_height": 84.10000610351562,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Anexo: Generación de Archivo Submit de la Competencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb8c57",
   "metadata": {
    "cell_id": "00028-0a64e7e8-1077-4868-8c96-db3d51323157",
    "deepnote_cell_height": 671.9000244140625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Para subir los resultados obtenidos a la pagina de CodaLab utilice la función `generateFiles` entregada mas abajo. Esto es debido a que usted deberá generar archivos que respeten extrictamente el formato de CodaLab, de lo contario los resultados no se veran reflejados en la pagina de la competencia.\n",
    "\n",
    "Para los resultados obtenidos en su modelo de clasificación y regresión, estos serán guardados en un archivo zip que contenga los archivos `predicctions_clf.txt` para la clasificación y `predicctions_rgr.clf` para la regresión. Los resultados, como se comento antes, deberan ser obtenidos en base al dataset `test.pickle` y en cada una de las lineas deberan presentar las predicciones realizadas.\n",
    "\n",
    "Ejemplos de archivos:\n",
    "\n",
    "- [ ] `predicctions_clf.txt`\n",
    "\n",
    "        Mostly Positive\n",
    "        Mostly Positive\n",
    "        Negative\n",
    "        Positive\n",
    "        Negative\n",
    "        Positive\n",
    "        ...\n",
    "\n",
    "- [ ] `predicctions_rgr.txt`\n",
    "\n",
    "        16103.58\n",
    "        16103.58\n",
    "        16041.89\n",
    "        9328.62\n",
    "        107976.03\n",
    "        194374.08\n",
    "        ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a3c4a6",
   "metadata": {
    "cell_id": "00029-55f95a4c-2d1f-4354-a690-049fea34bdac",
    "deepnote_cell_height": 620.13330078125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166501,
    "source_hash": "b1cdf32f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "def generateFiles(predict_data, clf_pipe, rgr_pipe):\n",
    "    \"\"\"Genera los archivos a subir en CodaLab\n",
    "\n",
    "    Input\n",
    "    predict_data: Dataframe con los datos de entrada a predecir\n",
    "    clf_pipe: pipeline del clf\n",
    "    rgr_pipe: pipeline del rgr\n",
    "\n",
    "    Ouput\n",
    "    archivo de txt\n",
    "    \"\"\"\n",
    "    y_pred_clf = clf_pipe.predict(predict_data)\n",
    "    y_pred_rgr = rgr_pipe.predict(predict_data)\n",
    "    \n",
    "    with open('./predictions_clf.txt', 'w') as f:\n",
    "        for item in y_pred_clf:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with open('./predictions_rgr.txt', 'w') as f:\n",
    "        for item in y_pred_rgr:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with ZipFile('predictions.zip', 'w') as zipObj2:\n",
    "        zipObj2.write('predictions_rgr.txt')\n",
    "        zipObj2.write('predictions_clf.txt')\n",
    "\n",
    "    os.remove(\"predictions_rgr.txt\")\n",
    "    os.remove(\"predictions_clf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e1b3bb-0beb-495b-9508-6df20a49bbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.9478521316670252\n",
      "F1: 0.8586326112171789\n"
     ]
    }
   ],
   "source": [
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", RandomForestRegressor(criterion='poisson'))])\n",
    "\n",
    "reg_pipe.fit(X_all_reg, y_all_reg)\n",
    "print('R²:', r2_score(y_all_reg, reg_pipe.predict(X_all_reg)))\n",
    "\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                    (\"bf\", SelectPercentile(f_classif, percentile=50)),\n",
    "                    (\"clf\", GradientBoostingClassifier(n_estimators=500))])\n",
    "class_pipe.fit(X_all_class, y_all_class)\n",
    "print('F1:', f1_score(y_all_class, class_pipe.predict(X_all_class), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65649abb-3aed-42f1-98ac-2c855fa371b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16432509-fae3-4f72-9b97-2beaa394005a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>original_language</th>\n",
       "      <th>overview</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>release_date</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>status</th>\n",
       "      <th>...</th>\n",
       "      <th>warner</th>\n",
       "      <th>nbc</th>\n",
       "      <th>sony</th>\n",
       "      <th>paramount</th>\n",
       "      <th>count_top_actors</th>\n",
       "      <th>count_top_directors</th>\n",
       "      <th>title_oof</th>\n",
       "      <th>tagline_oof</th>\n",
       "      <th>overview_oof</th>\n",
       "      <th>keywords_oof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15927</td>\n",
       "      <td>passage india</td>\n",
       "      <td>Drama-Adventure-History</td>\n",
       "      <td>en</td>\n",
       "      <td>set period growing influence indian independen...</td>\n",
       "      <td>EMI Films-Thorn EMI Screen Entertainment-HBO</td>\n",
       "      <td>1984-12-14</td>\n",
       "      <td>8000000.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.899538</td>\n",
       "      <td>16.792093</td>\n",
       "      <td>16.731907</td>\n",
       "      <td>17.311062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9064</td>\n",
       "      <td>hellbound hellraiser ii</td>\n",
       "      <td>Horror</td>\n",
       "      <td>en</td>\n",
       "      <td>doctor channard sent new patient girl warning ...</td>\n",
       "      <td>Film Futures-New World Pictures-Cinemarque Ent...</td>\n",
       "      <td>1988-12-23</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.347646</td>\n",
       "      <td>16.528879</td>\n",
       "      <td>17.133209</td>\n",
       "      <td>16.788112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41602</td>\n",
       "      <td>charlie countryman</td>\n",
       "      <td>Action-Comedy-Drama-Romance-Thriller</td>\n",
       "      <td>en</td>\n",
       "      <td>traveling abroad guy falls romanian beauty who...</td>\n",
       "      <td>Bona Fide Productions-Voltage Pictures</td>\n",
       "      <td>2013-02-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.785088</td>\n",
       "      <td>16.540153</td>\n",
       "      <td>15.872622</td>\n",
       "      <td>16.321421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>293646</td>\n",
       "      <td>33</td>\n",
       "      <td>Drama-History</td>\n",
       "      <td>en</td>\n",
       "      <td>based true story collapse mine san jose chilet...</td>\n",
       "      <td>Alcon Entertainment-Phoenix Pictures-Half Circ...</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>25000000.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.907404</td>\n",
       "      <td>16.509489</td>\n",
       "      <td>16.757019</td>\n",
       "      <td>15.795558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157353</td>\n",
       "      <td>transcendence</td>\n",
       "      <td>Thriller-Science Fiction-Drama-Mystery</td>\n",
       "      <td>en</td>\n",
       "      <td>two leading computer scientists work toward go...</td>\n",
       "      <td>DMG Entertainment-Warner Bros. Pictures-Alcon ...</td>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>100000000.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.785088</td>\n",
       "      <td>17.001481</td>\n",
       "      <td>17.358071</td>\n",
       "      <td>16.925404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>6439</td>\n",
       "      <td>racing stripes</td>\n",
       "      <td>Comedy-Family</td>\n",
       "      <td>en</td>\n",
       "      <td>shattered illusions hard repair especially goo...</td>\n",
       "      <td>Alcon Entertainment</td>\n",
       "      <td>2005-01-06</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.785088</td>\n",
       "      <td>16.320282</td>\n",
       "      <td>17.618127</td>\n",
       "      <td>17.547067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>9396</td>\n",
       "      <td>crocodile dundee ii</td>\n",
       "      <td>Adventure-Comedy</td>\n",
       "      <td>en</td>\n",
       "      <td>australian outback expert protects new york lo...</td>\n",
       "      <td>Paramount</td>\n",
       "      <td>1988-05-19</td>\n",
       "      <td>14000000.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.347646</td>\n",
       "      <td>17.681122</td>\n",
       "      <td>17.204335</td>\n",
       "      <td>17.362434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>9357</td>\n",
       "      <td>one hour photo</td>\n",
       "      <td>Drama-Thriller</td>\n",
       "      <td>en</td>\n",
       "      <td>sy photo guy parrish lovingly developed photos...</td>\n",
       "      <td>Fox Searchlight Pictures-Catch 23 Entertainmen...</td>\n",
       "      <td>2002-08-21</td>\n",
       "      <td>12000000.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.665157</td>\n",
       "      <td>17.074108</td>\n",
       "      <td>16.932835</td>\n",
       "      <td>16.438952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>9494</td>\n",
       "      <td>look whos talking</td>\n",
       "      <td>Comedy-Romance</td>\n",
       "      <td>en</td>\n",
       "      <td>mollie single working mother whos find perfect...</td>\n",
       "      <td>Management Company Entertainment Group (MCEG)-...</td>\n",
       "      <td>1989-10-12</td>\n",
       "      <td>7500000.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.083372</td>\n",
       "      <td>17.259105</td>\n",
       "      <td>17.445195</td>\n",
       "      <td>16.654946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>9772</td>\n",
       "      <td>air force one</td>\n",
       "      <td>Action-Thriller</td>\n",
       "      <td>en</td>\n",
       "      <td>russian terrorists conspire hijack aircraft pr...</td>\n",
       "      <td>Radiant Productions-Beacon Communications-Beac...</td>\n",
       "      <td>1997-07-25</td>\n",
       "      <td>85000000.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.755094</td>\n",
       "      <td>17.231367</td>\n",
       "      <td>17.362235</td>\n",
       "      <td>18.309514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>656 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                    title                                  genres  \\\n",
       "0     15927            passage india                 Drama-Adventure-History   \n",
       "1      9064  hellbound hellraiser ii                                  Horror   \n",
       "2     41602       charlie countryman    Action-Comedy-Drama-Romance-Thriller   \n",
       "3    293646                       33                           Drama-History   \n",
       "4    157353            transcendence  Thriller-Science Fiction-Drama-Mystery   \n",
       "..      ...                      ...                                     ...   \n",
       "651    6439           racing stripes                           Comedy-Family   \n",
       "652    9396      crocodile dundee ii                        Adventure-Comedy   \n",
       "653    9357           one hour photo                          Drama-Thriller   \n",
       "654    9494        look whos talking                          Comedy-Romance   \n",
       "655    9772            air force one                         Action-Thriller   \n",
       "\n",
       "    original_language                                           overview  \\\n",
       "0                  en  set period growing influence indian independen...   \n",
       "1                  en  doctor channard sent new patient girl warning ...   \n",
       "2                  en  traveling abroad guy falls romanian beauty who...   \n",
       "3                  en  based true story collapse mine san jose chilet...   \n",
       "4                  en  two leading computer scientists work toward go...   \n",
       "..                ...                                                ...   \n",
       "651                en  shattered illusions hard repair especially goo...   \n",
       "652                en  australian outback expert protects new york lo...   \n",
       "653                en  sy photo guy parrish lovingly developed photos...   \n",
       "654                en  mollie single working mother whos find perfect...   \n",
       "655                en  russian terrorists conspire hijack aircraft pr...   \n",
       "\n",
       "                                  production_companies release_date  \\\n",
       "0         EMI Films-Thorn EMI Screen Entertainment-HBO   1984-12-14   \n",
       "1    Film Futures-New World Pictures-Cinemarque Ent...   1988-12-23   \n",
       "2               Bona Fide Productions-Voltage Pictures   2013-02-09   \n",
       "3    Alcon Entertainment-Phoenix Pictures-Half Circ...   2015-08-06   \n",
       "4    DMG Entertainment-Warner Bros. Pictures-Alcon ...   2014-04-16   \n",
       "..                                                 ...          ...   \n",
       "651                                Alcon Entertainment   2005-01-06   \n",
       "652                                          Paramount   1988-05-19   \n",
       "653  Fox Searchlight Pictures-Catch 23 Entertainmen...   2002-08-21   \n",
       "654  Management Company Entertainment Group (MCEG)-...   1989-10-12   \n",
       "655  Radiant Productions-Beacon Communications-Beac...   1997-07-25   \n",
       "\n",
       "          budget  runtime    status  ... warner nbc sony  paramount  \\\n",
       "0      8000000.0    163.0  Released  ...      0   0    0          0   \n",
       "1      3000000.0     97.0  Released  ...      0   0    0          0   \n",
       "2            0.0    108.0  Released  ...      0   0    0          0   \n",
       "3     25000000.0    120.0  Released  ...      0   0    0          0   \n",
       "4    100000000.0    119.0  Released  ...      0   0    0          0   \n",
       "..           ...      ...       ...  ...    ...  ..  ...        ...   \n",
       "651   30000000.0    102.0  Released  ...      0   0    0          0   \n",
       "652   14000000.0    110.0  Released  ...      0   0    0          1   \n",
       "653   12000000.0     96.0  Released  ...      0   0    0          0   \n",
       "654    7500000.0     93.0  Released  ...      0   0    0          0   \n",
       "655   85000000.0    124.0  Released  ...      0   0    0          0   \n",
       "\n",
       "     count_top_actors  count_top_directors  title_oof  tagline_oof  \\\n",
       "0                   0                    0  16.899538    16.792093   \n",
       "1                   0                    0  17.347646    16.528879   \n",
       "2                   0                    0  16.785088    16.540153   \n",
       "3                   0                    0  16.907404    16.509489   \n",
       "4                   0                    0  16.785088    17.001481   \n",
       "..                ...                  ...        ...          ...   \n",
       "651                 0                    0  16.785088    16.320282   \n",
       "652                 0                    0  17.347646    17.681122   \n",
       "653                 0                    0  16.665157    17.074108   \n",
       "654                 0                    0  17.083372    17.259105   \n",
       "655                 0                    0  16.755094    17.231367   \n",
       "\n",
       "     overview_oof  keywords_oof  \n",
       "0       16.731907     17.311062  \n",
       "1       17.133209     16.788112  \n",
       "2       15.872622     16.321421  \n",
       "3       16.757019     15.795558  \n",
       "4       17.358071     16.925404  \n",
       "..            ...           ...  \n",
       "651     17.618127     17.547067  \n",
       "652     17.204335     17.362434  \n",
       "653     16.932835     16.438952  \n",
       "654     17.445195     16.654946  \n",
       "655     17.362235     18.309514  \n",
       "\n",
       "[656 rows x 63 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41cc6b2f",
   "metadata": {
    "cell_id": "b589f659ce2246919e3707e79420aff2",
    "deepnote_cell_height": 138,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ejecutar función para generar el archivo de predicciones.\n",
    "# perdict_data debe tener cargada los datos del text.pickle\n",
    "# mientras que clf_pipe y rgr_pipe, son los pipeline de \n",
    "# clasificación y regresión respectivamente.\n",
    "generateFiles(data_test, class_pipe, reg_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda9345",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "dbddc0f3-10b8-4160-bc27-ac993196164c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
