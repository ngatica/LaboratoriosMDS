{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8623baeb",
   "metadata": {
    "cell_id": "1afe48f4-24bb-456a-8ebb-b21f17292fae",
    "deepnote_cell_height": 291.98333740234375,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Proyecto\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Pablo Badilla\n",
    "- Auxiliar: Ignacio Meza D.\n",
    "- Ayudante: Patricio Ortiz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667fd4c",
   "metadata": {
    "cell_id": "00001-08980085-11ff-46bb-ad0e-cfeebe049a14",
    "deepnote_cell_height": 225.11666870117188,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- Fecha de entrega: 15/07/2021 (atrasos hasta el domingo 17 de julio)\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a3910",
   "metadata": {
    "cell_id": "00002-00231602-ae19-4e86-8713-55497e9c1dc0",
    "deepnote_cell_height": 785.2166748046875,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "d50c3174-babb-4861-9c71-7e3af66458b8"
   },
   "source": [
    "---\n",
    "\n",
    "## El Desaf√≠o de Renac√≠n üß∏\n",
    "\n",
    "<div align='center'>\n",
    "<img src='https://media.cnnchile.com/sites/2/2018/09/maipu-renacin-740x430.jpg' width=300>\n",
    "</div>\n",
    "\n",
    "Renac√≠n, ex-influencer y figura (de peluche) publica; luego de su despido, decide que ser√° una buena idea darle un giro a su vida y dedicarse al rubro del asesoramiento de inversionistas en la industria del cine. \n",
    "\n",
    "El futuro empresario plantea que el √©xito potencial de una propuesta pel√≠cula debe ser analizado en base a evidencia hist√≥rica de cintas similares y no en la intuici√≥n ni en simples corazonadas. \n",
    "Por esto, plantea a las gerencias de las principales productoras de cine que ser√≠a ideal contar con una m√°quina que, dada las caracter√≠sticas de una propuesta de pel√≠cula (su g√©nero, la productora, su duraci√≥n, su historia, etc...), prediga si esta ser√° potencialmente una inversi√≥n rentable o no.\n",
    "\n",
    "Renac√≠n est√° convencido que el √©xito de una inversi√≥n en un filme debe estar relacionada por dos caracter√≠sticas muy relevantes de estas una vez que salen al mercado:\n",
    "\n",
    "**1. La potencial evaluaci√≥n (Positiva, Negativa, etc...) que le dan sus consumidores.**\n",
    "\n",
    "**2. Las potenciales ganancias de la pel√≠cula.**\n",
    "\n",
    "Si bien la idea puede sonar excelente, Renac√≠n carece en su totalidad de una formaci√≥n en Ciencia de los Datos, por lo que decide ir en ayuda de expertos para implementar su idea. \n",
    "\n",
    "Sin embargo, decide no contratar a un equipo en particular, si no que tener la libertad de elegir entre muchos equipos que compiten entre si para saber cu√°l contratar. Para esto recurre a una triqui√±uela recurrentemente utilizada en Data Science: Establecer una competencia abierta y pagar por el mejor modelo (i.e, que cumpla mejor sus requisitos).\n",
    "\n",
    "Para esto, el ex-influencer decide abrir una competencia en la plataforma [Codalab](https://codalab.lisn.upsaclay.fr/competitions/5521?secret_key=7ecfd279-9521-457d-8602-616532fcd813) (plataforma similar a Kaggle) la cu√°l, espera que se replete de buenos modelos. Los equipos que mejor evaluaciones obtengan (los primeros 3 de cada tabla) ser√°n contratados y retribuidos con un cup√≥n canjeable con la friolera cantidad de 1 punto bonus para el proyecto en el curso MDS7202.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd093b",
   "metadata": {
    "cell_id": "00003-c3cde4b4-710d-4987-b470-2494e93fb1ec",
    "deepnote_cell_height": 964.3333129882812,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Definici√≥n Formal del Problema\n",
    "\n",
    "El objetivo de este proyecto es aplicar todo lo aprendido hasta este momento con el fin de solucionar 2 problemas distintos: \n",
    "\n",
    "1. **Clasificaci√≥n de potenciales evaluaciones con las que los consumidores evaluar√°n las pel√≠culas**. Las posibles clases que deben asignar a cada juego son `('Negative', 'Mixed', 'Mostly Positive', 'Positive', 'Very Positive')`. La m√©trica de evaluaci√≥n utilizada para medir la clasificaci√≥n es `f1_macro`.\n",
    "2. **Regresi√≥n de los potenciales ingresos que tendr√°n las pel√≠culas**. La m√©trica de evaluaci√≥n utilizada para medir la clasificaci√≥n es `r_2`.\n",
    "\n",
    "Para esto, ustedes contar√°n un dataset que cuenta con diversa informaci√≥n sobre pel√≠culas (tales como productora, actores, duraci√≥n, fecha de lanzamiento, keywords, etc...) m√°s las etiquetas y valores a predecir.\n",
    "\n",
    "El objetivo final es que generen el mejor modelo posible para ambos problemas y que con estos, participen en la competencia habilitada en el siguiente ([link](https://codalab.lisn.upsaclay.fr/competitions/5521?secret_key=7ecfd279-9521-457d-8602-616532fcd813)).\n",
    "\n",
    "### Competencias de Data Science\n",
    "\n",
    "> Una competencia de Data Science funciona generalmente de la siguiente manera: \n",
    "\n",
    "1. Se plantea un problema que los equipos deben resolver.\n",
    "2. Se provee de datos de entrenamiento a los equipos para que generen modelos que resuelvan el problema.\n",
    "3. Se provee de datos de prueba que los equipos deber√°n predecir con los modelos creados. Una vez predichos, los equipos deben subir los archivos a la plataforma, la cu√°l los evaluar√° y publicar√° en un tablero disponible para todos los participantes.\n",
    "\n",
    "Existen muchos sitios en donde se publican competencias recurrentemente tales como [Kaggle](https://www.kaggle.com/) y [Codalab](https://codalab.lisn.upsaclay.fr/).\n",
    "\n",
    "### Competencia del Proyecto\n",
    "\n",
    "Para este proyecto, para participar en la competencia se les proveer√° de tres datasets: `train_numerical_features.parquet`, `train_text_features.parquet` y `test.pickle`.\n",
    "\n",
    "- `train_numerical_features.parquet` y `train_text_features.parquet` deben usarlos como conjunto de entrenamiento del modelo; por lo que incluye las etiquetas y valores por predecir. noten que esto no implica que no deban hacer *holdout* para evaluar internamente su modelo (en este caso, el set de test es llamado de *validaci√≥n*). Por otro lado, se recomienda que junten ambos archivos para generar el tabl√≥n final de entrenamiento.\n",
    "\n",
    "- `test.pickle` se usar√° para evaluar el rendimiento de sus modelos en la competencia. Es decir, este dataset solo contiene caracter√≠sticas de las pel√≠culas y ustedes deber√°n predecir tanto las potenciales evaluaciones como las ganancias de estas y subir sus resultados.\n",
    "\n",
    "Para subir archivos a la competencia deber√°n registrarse en Codalab. Para evitar overfitting y/o que intenten adivinar los datos de testing, **puden participar m√°ximo 5 veces en la competencia**. Usenlos sabiamente.\n",
    "\n",
    "**MUY IMPORTANTE**: Para la clasificaci√≥n no usen las ganancias (target de la regresi√≥n) como atributo. Por otro lado, para la regresi√≥n no utilicen las evaluaciones como atributo para predecir. **Infringir estas reglas implicar√° en no contar el puntaje de la competencia como tambi√©n descuentos en los items de clasificaci√≥n como de regresi√≥n.** Recuerden que esta es informaci√≥n del \"futuro\": cuando est√©n las propuestas de pel√≠culas no dispondremos de estas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6443276",
   "metadata": {
    "cell_id": "00004-a4f74232-f16f-4b68-9488-a39082e07e79",
    "deepnote_cell_height": 8913.283203125,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb"
   },
   "source": [
    "---\n",
    "\n",
    "## Secciones Requeridas en el Informe\n",
    "\n",
    "La siguiente lista detalla las secciones que debe contener su notebook para resolver el proyecto. Es importante que al momento de desarrollar cada una de las secciones, estas sean escritas en un formato tipo **informe**, donde describan detalladamente cada uno de los puntos realizados.\n",
    "\n",
    "### 1. Introducci√≥n [0.5 Puntos]\n",
    "\n",
    "*Esta secci√≥n es literalmente una muy breve introducci√≥n con todo lo necesario para entender que hicieron en su proyecto.*\n",
    "\n",
    "- Describir brevemente ambos problemas planteados (clasificaci√≥n y regresi√≥n).\n",
    "- Describir brevemente los datos de entrada que les provee el problema. \n",
    "- Describir qu√© m√©tricas se ocupan para la evaluaci√≥n del problema y por qu√© se utilizan.\n",
    "- [Escribir al final] Describir brevemente el modelo que usaron para resolver el problema (incluyendo las transformaciones intermedias de datos).\n",
    "- [Escribir al final] Indicar si lograron resolver el problema a trav√©s de su modelo. Indiquen adem√°s si creen que los resultados de su mejor modelo son aceptables y como les fue con respecto al resto de los equipos.\n",
    "\n",
    "### 2. Preparaci√≥n del Dataset y An√°lisis Exploratorio de Datos [2 Puntos]\n",
    "\n",
    "\n",
    "*La idea de esta secci√≥n es que preparen y exploren el dataset y obtengan una idea de como son los datos de su problema para que en la siguientes secciones, puedan modelarlos.*\n",
    "\n",
    "#### Carga y Preparaci√≥n de los Datos\n",
    "\n",
    "Primero, se les solicita que ejecuten los siguientes pasos de carga de datos:\n",
    "\n",
    "- Cargar los datos con Pandas y fusionar por `id`.\n",
    "- Eliminar columnas `'poster_path'`, `'backdrop_path'`, `'recommendations'`.\n",
    "- Filtrar ejemplos con `revenue` igual a 0.\n",
    "- Filtrar ejemplos con `release_date` y `runtime` nulos.\n",
    "- Convertir fechas de release_date a `pd.DateTime`.\n",
    "- Conservar solo los ejemplos con `status` `\"Released\"`.\n",
    "- Rellenar valores nulos categ√≥ricos y de texto con `''`.\n",
    "- Discretizar `vote_average` a los siguientes bins y guardar los resultados en la columna `label`: \n",
    "  - (0, 5]: `'Negative'`\n",
    "  - (5, 6]: `'Mixed'`\n",
    "  - (6, 7]: `'Mostly Positive'`\n",
    "  - (7, 8]: `'Positive'`\n",
    "  - (8, 10]: `'Very Positive'`\n",
    "- Eliminar la columna `vote_average` e `id`\n",
    "- Renombrar la columna `revenue` por `target`.\n",
    "\n",
    "Todos los pasos anteriormente nombrados deben ser realizados con Pandas y sus m√©todos, y sin la ayuda de ning√∫n ciclo `for`. Si no conoce alguna operaci√≥n, se recomienda fuertemente buscar en la web antes de intentar hacerlo por fuerza bruta.\n",
    "\n",
    "#### EDA\n",
    "\n",
    "Luego, se les solicita que realizen un EDA en donde hagan tareas generales como:\n",
    "\n",
    "- Analizar los tipos de datos y distribuciones de las variables a trav√©s de histogramas.\n",
    "- Generar visualizaciones de las interacciones (como por ejemplo, una scatter matrix) en las distintas variables.\n",
    "- Ver las correlaciones entre las distintas variables y los valores faltantes de cada una de estas. \n",
    "- Proyectar los datos con UMAP para ver si existen relaciones entre las distintas variables de forma bi o tri dimensional.\n",
    "- Reportar los patrones y relaciones interesantes.\n",
    "\n",
    "Como tambi√©n tareas particulares como:\n",
    "\n",
    "- Genenrar un scatterplot de `budget` vs `target` (ex-revenue).\n",
    "- Buscar las peliculas de Marvel y hacer un scatter plot que en el eje x contenga `budget` y en el eje y `target` (ex-revenue). Utilice las columnas `` como `` y como `hover_name` .\n",
    "- Graficar a trav√©s de un gr√°fico de barras las 50 productoras m√°s frecuentes. Incluya la cantidad de `nans`.\n",
    "- Graficar a trav√©s de un gr√°fico de barras los 50 artistas m√°s frecuentes. \n",
    "\n",
    "Las tareas mencionadas anteriormente son lo m√≠nimo. Sin embargo, ojal√° no se restringan a lo mencionado anteriormente y exploren a√∫n m√°s en profundidad. Pueden usar tanto **`pandas`** como **plotly** y ``pandas profiling`` para esto. En el caso de generar un profile, adjuntar los resultados en el notebook.\n",
    "\n",
    "### 3. Preparaci√≥n de Datos [0.5 Punto]\n",
    "\n",
    "#### ColumnTransformer y Holdout\n",
    "\n",
    "*Esta secci√≥n consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo.*\n",
    "\n",
    "Generar un ColumnTransformer que:\n",
    "\n",
    "- Preprocese datos categ√≥ricos y ordinales.\n",
    "- Escale/estandarice datos num√©ricos.\n",
    "- Codifique texto.\n",
    "\n",
    "Luego, pruebe las transformaciones utilizando `fit_transform` y `get_feature_names out`.\n",
    "\n",
    "Posteriormente, ejecute un Holdout que le permita m√°s adelante evaluar los modelos. **Recuerde eliminar los target y las labels del dataset antes de dividirlo**.\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "Adicionalmente puede generar una nueva transformaci√≥n que genere nuevas features y que se aplique antes del ColumnTransformer dentro del pipeline de los modelos. Investigar [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) para ver como implementar una transformaci√≥n a partir de una funci√≥n que tome un dataframe y entregue uno distinto en la salida.\n",
    "\n",
    "- Encodear ciclicamente los meses/d√≠as de las fechas de lanzamiento.\n",
    "- Contar cuantas veces aparecen en las peliculas ciertos personajes c√©lebres.\n",
    "- Indicar si la pelicula es de una productora famosa o no.\n",
    "- Agrupar distintas keywords en categor√≠as m√°s generales.\n",
    "- Generar ratios con las variables numericas del dataset (como duraci√≥n de la pel√≠cula/presupuesto).\n",
    "- Contar los diferentes generos similares que posee una pelicula.\n",
    "- Extraer vectores desde los overviews de las peliculas.\n",
    "- Contar el n√∫mero de actores/productoras/g√©neros.\n",
    "- Etc... Usen su creatividad!\n",
    "\n",
    "Nuevamente, recuerde no utilizar ni los targets ni las labels para generar nuevas features.\n",
    "\n",
    "Nota: Este √∫ltimo paso no es requisito pero puede catapultarlos a la cima del tablero de las competencias.\n",
    "\n",
    "### 4. Clasificaci√≥n [1.5 puntos]\n",
    "\n",
    "#### Creaci√≥n del modelo **Dummy** y del *Baseline* [0.5 Puntos]\n",
    "\n",
    "*En esta secci√≥n crear√°n el modelo m√°s b√°sico posible que resuelva el problema. La idea de este modelo usarlo como comparaci√≥n para que en el siguiente paso lo puedan mejorar.*\n",
    "\n",
    "- Generar un modelo Dummy con estrategia estratificada que les permita comparar m√°s adelante si su baseline de clasificaci√≥n es mejor que el azar.\n",
    "- Generar un pipeline para la clasificaci√≥n con un clasificador relativamente sencillo a la salida (a su elecci√≥n, recomendado: arbol de decisi√≥n).\n",
    "- Evaluar ambos modelos seg√∫n las m√©tricas de evaluaci√≥n y reportar.\n",
    "\n",
    "#### Optimizaci√≥n del Modelo [1 Puntos]\n",
    "\n",
    "*Aqu√≠ deber√°n mejorar del modelo de clasificaci√≥n al variar los algoritmos/hiperpar√°metros que est√°n ocupando.*\n",
    "\n",
    "- Generar una nueva `Pipeline` enfocada en buscar el mejor modelo usando GridSearch.\n",
    "- Usar **`GridSearchCV`** o **`HalvingGridSearchCV`** para tunear hipermar√°metros. La primera demorar√° m√°s que la segunda pero les traer√° potencialmente mejores resultados. Pueden probar tambi√©n [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html) de la librer√≠a [`Optuna`](https://optuna.org/) , la cu√°l es bastante popular para buscar modelos de redes neuronales.\n",
    "- Agregar t√©cnicas de seleccion de atributos, como tambi√©n usar mejores clasificadores y explorar sus hiperpar√°metros. \n",
    "- Probar distintos par√°metros para las transformaciones de datos, seleccion de atributos, clasificadores, etc...\n",
    "- Probar modelos basados en gradient boosting/bagging. **Recomendaci√≥n fuerte:** Probar [`LightGMB`](https://lightgbm.readthedocs.io/en/latest/) o [`xgboost`](https://xgboost.readthedocs.io/en/stable/).\n",
    "- Probar activando/descativando los procesadores de texto, de categor√≠as, etc...\n",
    "- Recuerden setear la b√∫squeda para optimizar la m√©trica que se evalua en la competencia.\n",
    "\n",
    "Algunas notas interesantes sobre este proceso: \n",
    "\n",
    "- No se les pide rendimientos cercanos al 100% de la m√©trica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n",
    "- Hagan grillas computables: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces ach√≠quenla a algo que sepan que va a terminar. \n",
    "- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la b√∫squeda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n",
    "\n",
    "**Al final de este proceso, seleccione el mejor modelo de clasificaci√≥n encontrado, prediga las labels del test set de la competencia y env√≠elos a Codalab.**\n",
    "\n",
    "### 4. Regresi√≥n [1.5 Puntos]\n",
    "\n",
    "#### Creaci√≥n del modelo **Dummy** y del *Baseline* [0.5 Punto]\n",
    "\n",
    "*En esta secci√≥n crear√°n el modelo m√°s b√°sico posible que resuelva el segundo problema, el de las ganancias. La idea de este modelo usarlo como comparaci√≥n para que en el siguiente paso lo puedan mejorar.*\n",
    "\n",
    "- Generar un modelo Dummy que les permita comparar m√°s adelante si su baseline de regresi√≥n es mejor que el azar.\n",
    "- Generar un pipeline para la regresi√≥n con un regresor relativamente sencillo de utilizar (a su elecci√≥n, recomendado: arbol de decisi√≥n).\n",
    "- Evaluar ambos modelos seg√∫n las m√©tricas de evaluaci√≥n y reportar.\n",
    "\n",
    "#### Optimizaci√≥n del Modelo [1 Puntos]\n",
    "\n",
    "*Aqu√≠ deber√°n mejorar del modelo de regresi√≥n al variar los algoritmos/hiperpar√°metros que est√°n ocupando.*\n",
    "\n",
    "- Generar una nueva `Pipeline` enfocada en buscar el mejor modelo usando GridSearch.\n",
    "- Usar **`GridSearchCV`** o **`HalvingGridSearchCV`** para tunear hipermar√°metros. La primera demorar√° m√°s que la segunda pero les traer√° potencialmente mejores resultados. Pueden probar tambi√©n [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html) de la librer√≠a [`Optuna`](https://optuna.org/) , la cu√°l es bastante popular para buscar modelos de redes neuronales.\n",
    "- Agregar t√©cnicas de seleccion de atributos, como tambi√©n usar mejores clasificadores y explorar sus hiperpar√°metros. \n",
    "- Probar distintos par√°metros para las transformaciones de datos, seleccion de atributos, clasificadores, etc...\n",
    "- Probar modelos basados en gradient boosting/bagging. **Recomendaci√≥n fuerte:** Probar las librer√≠as [`LightGMB`](https://lightgbm.readthedocs.io/en/latest/) o [`xgboost`](https://xgboost.readthedocs.io/en/stable/).\n",
    "- Probar activando/descativando los procesadores de texto, de categor√≠as, etc...\n",
    "- Recuerden setear la b√∫squeda para optimizar la m√©trica que se evalua en la competencia.\n",
    "\n",
    "\n",
    "\n",
    "Algunas notas interesantes sobre este proceso (las mismas que antes...): \n",
    "\n",
    "- No se les pide rendimientos cercanos al 100% de la m√©trica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n",
    "- Hagan grillas computables: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces ach√≠quenla a algo que sepan que va a terminar. \n",
    "- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la b√∫squeda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n",
    "\n",
    "**Al final de este proceso, seleccione el mejor modelo de regresi√≥n encontrado, prediga los target del test set de la competencia y env√≠elos a Codalab.**\n",
    "\n",
    "\n",
    "### 6. Concluir [0.5 Punto] \n",
    "\n",
    "\n",
    "*Aqu√≠ deben escribir una breve conclusi√≥n del trabajo que hicieron en donde incluyan (pero no se limiten) a responder las siguientes preguntas:*\n",
    "\n",
    "- ¬øPudieron resolver exitosamente el problema?\n",
    "- ¬øSon aceptables los resultados obtenidos?\n",
    "- ¬øEn que medida el EDA ayud√≥ a comprender los datos en miras de generar un modelo predictivo?\n",
    "\n",
    "Respecto a la clasificaci√≥n:\n",
    "\n",
    "- ¬øComo fue el rendimiento del baseline para la clasificaci√≥n?\n",
    "- ¬øPudieron optimizar el baseline para la clasificaci√≥n?\n",
    "- ¬øQue tanto mejoro el baseline de la clasificaci√≥n con respecto a sus optimizaciones?\n",
    "\n",
    "Respecto a la regresi√≥n:\n",
    "\n",
    "- ¬øComo fue el rendimiento del baseline para la clasificaci√≥n?\n",
    "- ¬øPudieron optimizar el baseline para la clasificaci√≥n?\n",
    "- ¬øQue tanto mejoro el baseline de la clasificaci√≥n con respecto a sus optimizaciones?\n",
    "\n",
    "Finalmente: \n",
    "\n",
    "- ¬øEstuvieron conformes con sus resultados?\n",
    "- ¬øCreen que hayan mejores formas de modelar el problema?\n",
    "- ¬øEn general, qu√© aprendieron del proyecto? ¬øQu√© no aprendieron y les gustar√≠a haber aprendido?\n",
    "\n",
    "**OJO** si usted decide responder parte de estas preguntas, debe redactarlas en un formato de informe y no responderlas directamente.\n",
    "\n",
    "### 7. Punto por superar el baseline de la competencia. [1 Punto]\n",
    "\n",
    "*Este punto es solo informativo, no deben escribir nada aqu√≠*\n",
    "\n",
    "\n",
    "### Otras Instrucciones\n",
    "\n",
    "Pueden separar las tareas en 3 notebooks distintos: EDA, clasificaci√≥n y regresi√≥n. Esto les permitir√° tener el c√≥digo de estas tres secciones separadas y mejor ordenado a la vez que reduce el uso de memoria RAM.\n",
    "Noten que una estructura as√≠ requerir√° que la etapa de carga y preparaci√≥n de los datos est√© presente en los 3 notebooks (para que trabaje con el mismo dataset en cada instancia). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624c2bb",
   "metadata": {
    "cell_id": "00005-aeba1c85-fe35-4f74-9923-d2edc00d7e60",
    "deepnote_cell_height": 249.23333740234375,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## Esquema de la Tarea\n",
    "\n",
    "\n",
    "Pueden usar el siguiente esquema para organizar la tarea (y borrar todo lo anterior).\n",
    "Obviamente **no deben limitarse a lo que est√° escrito en esta**: puede incrementar en caso de m√°s t√©cnicas y obviar algunas partes en caso que alguna y otro punto no aplique a su problema.\n",
    "\n",
    "Pueden borrar las instrucciones anteriores y quedarse solo con lo que viene a continuaci√≥n.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df401720",
   "metadata": {
    "cell_id": "00006-84a35c5d-0758-4cbb-b2ca-b182898b80d0",
    "deepnote_cell_height": 295.8833312988281,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Proyecto\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- \\<Primer integrante\\> Crist√≥bal Igor\n",
    "- \\<Segundo integrante\\> Nicol√°s Gatica\n",
    "\n",
    "- \\<Nombre de usuarios en Codalab\\> cigor\n",
    "\n",
    "- \\<Nombre del Equipo en Codalab\\> -\n",
    "\n",
    "### Link de repositorio de GitHub: `\\<https://github.com/ngatica/LaboratoriosMDS\\>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93b698",
   "metadata": {
    "cell_id": "00007-447f1977-318e-432f-ba83-12d7e667ae68",
    "deepnote_cell_height": 253.13333129882812,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1. Introducci√≥n\n",
    "\n",
    "El objetivo de este proyecto consiste en medir el √©xito que tendr√° una pel√≠cula. Esto se ve reflejado en dos m√©tricas; los votos promedios, que seran categorizados en 'Negative', 'Mixed', 'Mostly Positive', 'Positive', 'Very Positive'; y la cantidad de dinero recaudado luego de ser estrenadas.\n",
    "\n",
    "Los datos que se proveen es un dataset con 6451 ejemplos que describen una observaci√≥n de pel√≠culas y series ya estrenadas. Esto a trav√©s de su t√≠tulo, presupuesto, duraci√≥n, lema, creditos, generos, lenguaje, resumen, productoras, fecha de lanzamiento y palabras claves.\n",
    "\n",
    "En total son 13 atributos, 11 correspondientes a las variables mencionadas y 2 correspondientes a la variables objetivos. Esta √∫ltimas son de tipo; categ√≥rico, asociado a la caracter√≠sticas de los votos de de las pel√≠culas (negativos, mixto, mayormente positivo, positivo, y muy positivos); y n√∫merico asociado a las recaudaciones logradas por la pel√≠cula.\n",
    "\n",
    "La primera tarea, correspondiente a la clasificaci√≥n de los votos promedios, se evalua en base a la m√©trica F1-macro ya que esta combina la precisi√≥n y la sensibilidad que tiene un test, calculando la media arm√≥nica entre ambas. Al tener un problema multiclase, la extensi√≥n macro obtiene el promedio de los F1 resultantes para cada una de las etiquetas.\n",
    "\n",
    "La segunda tarea, correspondiente a la predicci√≥n de la recaudaci√≥n, se evalua en base a la m√©trica R2 ya que esta es una medida estad√≠stica que muestra qu√© tan cerca se encuentran los puntos de datos sobre la linea de regresi√≥n.\n",
    "\n",
    "Nuestra propuesta para resolver ambos problemas consisti√≥, de forma resumida, en primero crear caracteristicas a partir de las ya existentes, y tambi√©n vectorizar las variables en formato de texto.\n",
    "Luego, para clasificar y predecir se utilizaron principalmente modelos ensamblados, esto dado que demostraron tener un mejor desempe√±o que los modelos m√°s b√°sicos.\n",
    "Por √∫ltimo, partir de estos modelos se procedi√≥ a mejorar los par√°metros asociados tanto a los modelos, como a los m√©todos de vectorizaci√≥n de los atributos de texto.\n",
    "\n",
    "En base a este procedimiento nuestros modelos cumplieron con las expectativas, lograron tener una buena capacidad de generalizaci√≥n y robustez, superando en ambos casos al baseline propuesto en la competencia, y destacando en la predicci√≥n de la recaudaci√≥n de las pel√≠culas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73159ed",
   "metadata": {
    "cell_id": "00008-6607fdcd-a35f-4e75-9b46-da3b181c1551",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "## 2. Prepraci√≥n y An√°lisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28d09b2-b19b-47d5-9126-3247ab205433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Librerias\n",
    "!pip install -q pyarrow\n",
    "!pip install -q pandas\n",
    "!pip install -q sklearn\n",
    "!pip install -q xgboost\n",
    "!pip install -q lightgbm\n",
    "!pip install -q nltk\n",
    "!pip install -q seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-procesamiento\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# Texto\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "!pip install -q lxml\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import os, warnings\n",
    "    warnings.simplefilter(\"ignore\") # Change the filter in this process\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4134b2",
   "metadata": {
    "cell_id": "00009-e413acae-3cf7-44db-847c-b846e3673adc",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166425,
    "source_hash": "55969c6e"
   },
   "outputs": [],
   "source": [
    "##  C√≥digo Preparaci√≥n de Datos.\n",
    "\n",
    "## Leyendo parquets\n",
    "data_numerica=pd.read_parquet('train_numerical_features.parquet')\n",
    "data_text=pd.read_parquet('train_text_features.parquet')\n",
    "\n",
    "## Merge data\n",
    "data=data_numerica.merge(data_text, how='inner', on=['id', 'title', 'credits', 'tagline'])\n",
    "\n",
    "## Eliminando columnas\n",
    "data.drop(columns=['poster_path', 'backdrop_path', 'recommendations'], inplace=True)\n",
    "\n",
    "## Filtrado revenue\n",
    "data=data[~(data.revenue==0)]\n",
    "\n",
    "## Filtrado release_date o runtime nulos\n",
    "data=data[(~(data.release_date.isna()) | (data.runtime.isna()))]\n",
    "\n",
    "## Fechas a datetime\n",
    "data['release_date']=pd.to_datetime(data.release_date)\n",
    "\n",
    "## Filtro status\n",
    "data=data[data.status=='Released']\n",
    "data.drop(columns=['status'], inplace=True)\n",
    "\n",
    "## Valores nulos de categ√≥ricas a string vacio\n",
    "data[data.select_dtypes(include='object').columns]=data[data.select_dtypes(include='object').columns].fillna('')\n",
    "\n",
    "## Discretizando los votos\n",
    "labels=['Negative', 'Mixed', 'Mostly Positive', 'Positive', 'Very Positive']\n",
    "data['label']=pd.cut(data.vote_average, bins=[0, 5, 6, 7, 8, 10], labels=labels)\n",
    "\n",
    "## Eliminando columnas vote average e id\n",
    "data.drop(columns=['vote_average', 'id'], inplace=True)\n",
    "\n",
    "## Renonmbrado columnas:\n",
    "data.rename(columns={'revenue': 'target'}, inplace=True)\n",
    "\n",
    "## Reseteando index\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e678393-c1d8-40b7-b180-bfda8d4c82bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>target</th>\n",
       "      <th>runtime</th>\n",
       "      <th>tagline</th>\n",
       "      <th>credits</th>\n",
       "      <th>genres</th>\n",
       "      <th>original_language</th>\n",
       "      <th>overview</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>release_date</th>\n",
       "      <th>keywords</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic Beasts: The Secrets of Dumbledore</td>\n",
       "      <td>200000000.0</td>\n",
       "      <td>400000000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>Return to the magic.</td>\n",
       "      <td>Jude Law-Eddie Redmayne-Mads Mikkelsen-Ezra Mi...</td>\n",
       "      <td>Fantasy-Adventure-Action</td>\n",
       "      <td>en</td>\n",
       "      <td>Professor Albus Dumbledore knows the powerful ...</td>\n",
       "      <td>Warner Bros. Pictures-Heyday Films</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>magic-curse-fantasy world-wizard-magical creat...</td>\n",
       "      <td>Mostly Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sonic the Hedgehog 2</td>\n",
       "      <td>110000000.0</td>\n",
       "      <td>393000000.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>Welcome to the next level.</td>\n",
       "      <td>James Marsden-Ben Schwartz-Tika Sumpter-Natash...</td>\n",
       "      <td>Action-Adventure-Family-Comedy</td>\n",
       "      <td>en</td>\n",
       "      <td>After settling in Green Hills Sonic is eager t...</td>\n",
       "      <td>Original Film-Blur Studio-Marza Animation Plan...</td>\n",
       "      <td>2022-03-30</td>\n",
       "      <td>sequel-based on video game-hedgehog-live actio...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Lost City</td>\n",
       "      <td>74000000.0</td>\n",
       "      <td>164289828.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>The adventure is real. The heroes are not.</td>\n",
       "      <td>Sandra Bullock-Channing Tatum-Daniel Radcliffe...</td>\n",
       "      <td>Action-Adventure-Comedy</td>\n",
       "      <td>en</td>\n",
       "      <td>A reclusive romance novelist was sure nothing ...</td>\n",
       "      <td>Paramount-Fortis Films-3dot Productions-Exhibi...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>duringcreditsstinger</td>\n",
       "      <td>Mostly Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title       budget       target  \\\n",
       "0  Fantastic Beasts: The Secrets of Dumbledore  200000000.0  400000000.0   \n",
       "1                         Sonic the Hedgehog 2  110000000.0  393000000.0   \n",
       "2                                The Lost City   74000000.0  164289828.0   \n",
       "\n",
       "   runtime                                     tagline  \\\n",
       "0    142.0                        Return to the magic.   \n",
       "1    122.0                  Welcome to the next level.   \n",
       "2    112.0  The adventure is real. The heroes are not.   \n",
       "\n",
       "                                             credits  \\\n",
       "0  Jude Law-Eddie Redmayne-Mads Mikkelsen-Ezra Mi...   \n",
       "1  James Marsden-Ben Schwartz-Tika Sumpter-Natash...   \n",
       "2  Sandra Bullock-Channing Tatum-Daniel Radcliffe...   \n",
       "\n",
       "                           genres original_language  \\\n",
       "0        Fantasy-Adventure-Action                en   \n",
       "1  Action-Adventure-Family-Comedy                en   \n",
       "2         Action-Adventure-Comedy                en   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Professor Albus Dumbledore knows the powerful ...   \n",
       "1  After settling in Green Hills Sonic is eager t...   \n",
       "2  A reclusive romance novelist was sure nothing ...   \n",
       "\n",
       "                                production_companies release_date  \\\n",
       "0                 Warner Bros. Pictures-Heyday Films   2022-04-06   \n",
       "1  Original Film-Blur Studio-Marza Animation Plan...   2022-03-30   \n",
       "2  Paramount-Fortis Films-3dot Productions-Exhibi...   2022-03-24   \n",
       "\n",
       "                                            keywords            label  \n",
       "0  magic-curse-fantasy world-wizard-magical creat...  Mostly Positive  \n",
       "1  sequel-based on video game-hedgehog-live actio...         Positive  \n",
       "2                               duringcreditsstinger  Mostly Positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71825663",
   "metadata": {
    "cell_id": "051fb133d1264c4dab1de21415aa18b6",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## C√≥digo EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093606a9",
   "metadata": {
    "cell_id": "6dfb65f58341449fb33eb025db788790",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "```\n",
    "An√°lisis del EDA.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d9f15",
   "metadata": {
    "cell_id": "00011-95957584-71f1-4669-a6e5-9b8ac7b8d4e0",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocesamiento, Holdout y Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbec91d-f209-4c90-9f75-142bf5317412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    # C√≥digo Feature Engineering (Opcional)\n",
    "    # Trabajando con la fecha de lanzamiento\n",
    "    ## Separando la fecha\n",
    "    df['release_year']=df.release_date.dt.year\n",
    "    df['release_day']=df.release_date.dt.day\n",
    "    df['release_month']=df.release_date.dt.month\n",
    "\n",
    "    ## Dia de la semana a la que pertenece\n",
    "    df['release_date_day'] = df['release_date'].transform(lambda x: x.weekday())\n",
    "\n",
    "    ## Si es del fin de semana\n",
    "    df['weekend'] = df['release_date_day'].transform(lambda x: 1 if x<4 else 0)\n",
    "\n",
    "    ## Dia del a√±o\n",
    "    df['dayofyear'] = pd.to_datetime(df['release_date']).dt.isocalendar().day\n",
    "\n",
    "    ## Semana del a√±o\n",
    "    df['weekofyear'] = pd.to_datetime(df['release_date']).dt.isocalendar().week\n",
    "\n",
    "\n",
    "    #Conteo de atributos\n",
    "    ## Generos\n",
    "    df['genres_count'] = df['genres'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Actores\n",
    "    df['actores_count'] = df['credits'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Productoras\n",
    "    df['production_companies_count'] = df['production_companies'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Keywords\n",
    "    df['keywords_count'] = df['keywords'].str.split(r\"-\").transform(lambda x: len(x))\n",
    "\n",
    "    ## Conteos del titulo\n",
    "    df['letters_title_count'] = df['title'].str.len() \n",
    "    df['words_title_count'] = df['title'].str.split().str.len() \n",
    "\n",
    "    ## Conteos del tagline\n",
    "    df['letters_tagline_count'] = df['tagline'].str.len() \n",
    "    df['words_tagline_count'] = df['tagline'].str.split().str.len() \n",
    "\n",
    "\n",
    "\n",
    "    ## Codificaci√≥n generos:\n",
    "    gen = data['genres'].str.split(r\"-\", expand=True)\n",
    "    generos = pd.DataFrame()\n",
    "    for i in range(len(gen.columns)):\n",
    "        generos = pd.concat([generos, gen[i]], axis = 0)\n",
    "    generos.dropna(axis=0, inplace=True)\n",
    "    generos=generos[0].unique()\n",
    "    for i in generos:\n",
    "        column = str('genre_' + i)\n",
    "        df[column] = df['genres'].str.split(r\"-\").transform(lambda x: 1 if i in x else 0)\n",
    "\n",
    "\n",
    "    # Generaci√≥n de ratios\n",
    "    ## Ratio actores budget\n",
    "    df['budget_actores'] = (df.budget)/(df.actores_count).replace([np.inf,-np.inf,np.nan],0)\n",
    "\n",
    "    ## Presupuesto dividido en a√±o\n",
    "    df['budget_anio']=(df.budget)/(df.release_year)\n",
    "\n",
    "    ## Presupuesto dividido en runtime\n",
    "    df['budget_runtime']=(df.budget)/(df.runtime).replace([np.inf,-np.inf,np.nan], 0)\n",
    "\n",
    "    ## Presupuesto en logaritmo\n",
    "    df['log_budget']=np.log1p(df.budget)\n",
    "\n",
    "    def intersection(lst1, lst2):\n",
    "        return len(list(set(lst1) & set(lst2)))\n",
    "\n",
    "    # Productoras mas famosas\n",
    "    disney_compaines = ['ABC Pictures', 'DisneyToon Studios', 'Disneynature','Touchstone Pictures', 'Marvel Studios']\n",
    "    df['disney'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, disney_compaines))\n",
    "\n",
    "    twenty_century_fox_compaines = ['Fox 2000 Pictures','Fox Atomic','Fox Searchlight Pictures']\n",
    "    df['fox'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, twenty_century_fox_compaines))\n",
    "\n",
    "    warner_bros_compaines = ['Warner Bros. Family Entertainment', 'Warner Bros. Pictures', 'New Line Cinema', 'Alloy Entertainment', 'Castle Rock Entertainment', 'Bad Robot']\n",
    "    df['warner'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, warner_bros_compaines))\n",
    "\n",
    "    nbcuniversal_compaines = ['Universal Pictures', 'Universal Pictures International (UPI)', 'DreamWorks Animation', 'DreamWorks Pictures', 'Focus Features']\n",
    "    df['nbc'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, nbcuniversal_compaines))\n",
    "\n",
    "    sony_pictures_compaines = ['Columbia Pictures', 'Sony Pictures', 'Lakeshore Entertainment', 'Marvel Studios', 'Will Packer Productions']\n",
    "    df['sony'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, sony_pictures_compaines))\n",
    "\n",
    "    paramount_pictures_compaines = ['Paramount']\n",
    "    df['paramount'] = df.production_companies.str.split(r',').apply(lambda x: intersection(x, paramount_pictures_compaines))\n",
    "\n",
    "    # Actores mas famosos\n",
    "    top_twenty_actors = ['Jennifer Lopez', 'Leonardo DiCaprio', 'Robert Downey Jr.', 'Brad Pitt',\n",
    "                         'Will Smith', 'Jessica Alba', 'Keanu Reeves', 'Jackie Chan', 'Clint Eastwood', \n",
    "                         'Arnold Schwarzenegger', 'Sylvester Stallone', 'Bill Cosby', 'Jack Nicholson',  \n",
    "                         'Adam Sandler', 'Mel Gibson', 'George Clooney', 'Tom Cruise', 'Tyler Perry',  \n",
    "                         'Jerry Seinfeld','Oprah Winfrey']\n",
    "    df['count_top_actors'] = df.credits.str.split(r'-').apply(lambda x: intersection(x, top_twenty_actors))\n",
    "\n",
    "    # Directores mas valorados\n",
    "    top_thirty_directors=['Richard Linklater', 'Mike Leigh', 'Jafar Panahi', 'Jean-Pierre Dardenne', 'The Coen Brothers', 'Hirokazu Koreeda', \n",
    "                          'Steven Spielberg', 'Martin Scorsese', 'Pedro Almodavar', 'Werner Herzog', 'Paul Thomas Anderson', 'Michael Haneke',\n",
    "                          'Clint Eastwood', 'Steven Soderbergh', 'Olivier Assayas', 'Peter Jackson', 'David O. Russell', 'Spike Lee',\n",
    "                          'Quentin Tarantino', 'Michael Winterbottom', 'Alex Gibney', 'Stephen Frears', 'Yimou Zhang', 'Jia Zhangke',\n",
    "                          'Noah Baumbach', 'Ridley Scott', 'Christopher Nolan', 'Jonathan Demme', 'Wes Anderson', 'Guillermo del Toro']\n",
    "    df['count_top_directors'] = df.credits.str.split(r'-').apply(lambda x: intersection(x, top_thirty_directors))   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec355cf7-a9fc-42d8-9094-64914e036072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>original_language</th>\n",
       "      <th>overview</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>release_date</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>credits</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15927</td>\n",
       "      <td>A Passage to India</td>\n",
       "      <td>Drama-Adventure-History</td>\n",
       "      <td>en</td>\n",
       "      <td>Set during the period of growing influence of ...</td>\n",
       "      <td>EMI Films-Thorn EMI Screen Entertainment-HBO</td>\n",
       "      <td>1984-12-14</td>\n",
       "      <td>8000000.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>David Lean, the Director of \"Doctor Zhivago\", ...</td>\n",
       "      <td>Judy Davis-Victor Banerjee-Peggy Ashcroft-Jame...</td>\n",
       "      <td>based on novel or book-cave-hindu-doctor-india...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9064</td>\n",
       "      <td>Hellbound: Hellraiser II</td>\n",
       "      <td>Horror</td>\n",
       "      <td>en</td>\n",
       "      <td>Doctor Channard is sent a new patient a girl w...</td>\n",
       "      <td>Film Futures-New World Pictures-Cinemarque Ent...</td>\n",
       "      <td>1988-12-23</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>Time to play.</td>\n",
       "      <td>Ashley Laurence-Clare Higgins-Kenneth Cranham-...</td>\n",
       "      <td>seduction-pain-stepmother-hell-pinhead-sequel-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                     title                   genres original_language  \\\n",
       "0  15927        A Passage to India  Drama-Adventure-History                en   \n",
       "1   9064  Hellbound: Hellraiser II                   Horror                en   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Set during the period of growing influence of ...   \n",
       "1  Doctor Channard is sent a new patient a girl w...   \n",
       "\n",
       "                                production_companies release_date     budget  \\\n",
       "0       EMI Films-Thorn EMI Screen Entertainment-HBO   1984-12-14  8000000.0   \n",
       "1  Film Futures-New World Pictures-Cinemarque Ent...   1988-12-23  3000000.0   \n",
       "\n",
       "   runtime    status                                            tagline  \\\n",
       "0    163.0  Released  David Lean, the Director of \"Doctor Zhivago\", ...   \n",
       "1     97.0  Released                                      Time to play.   \n",
       "\n",
       "                                             credits  \\\n",
       "0  Judy Davis-Victor Banerjee-Peggy Ashcroft-Jame...   \n",
       "1  Ashley Laurence-Clare Higgins-Kenneth Cranham-...   \n",
       "\n",
       "                                            keywords  \n",
       "0  based on novel or book-cave-hindu-doctor-india...  \n",
       "1  seduction-pain-stepmother-hell-pinhead-sequel-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_pickle('test.pickle')  \n",
    "data_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b396c747-4018-458b-93b1-997b3d387134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_parquet('data_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc38ff7-7c92-4c15-8be8-5137edac1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feature_engineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0592c4f4-70da-4df4-a933-e401475f4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = feature_engineering(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5923087-35be-475d-b081-3e8d1201f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Jul 17 23:08:56 2022\n",
      "Fold 1 started at Sun Jul 17 23:08:56 2022\n",
      "Fold 2 started at Sun Jul 17 23:08:56 2022\n",
      "Fold 3 started at Sun Jul 17 23:08:57 2022\n",
      "Fold 4 started at Sun Jul 17 23:08:57 2022\n",
      "Fold 5 started at Sun Jul 17 23:08:57 2022\n",
      "Fold 6 started at Sun Jul 17 23:08:58 2022\n",
      "Fold 7 started at Sun Jul 17 23:08:58 2022\n",
      "Fold 8 started at Sun Jul 17 23:08:58 2022\n",
      "Fold 9 started at Sun Jul 17 23:08:58 2022\n",
      "CV mean score: 2.2249, std: 0.1098.\n",
      "Fold 0 started at Sun Jul 17 23:08:59 2022\n",
      "Fold 1 started at Sun Jul 17 23:08:59 2022\n",
      "Fold 2 started at Sun Jul 17 23:09:00 2022\n",
      "Fold 3 started at Sun Jul 17 23:09:00 2022\n",
      "Fold 4 started at Sun Jul 17 23:09:00 2022\n",
      "Fold 5 started at Sun Jul 17 23:09:01 2022\n",
      "Fold 6 started at Sun Jul 17 23:09:01 2022\n",
      "Fold 7 started at Sun Jul 17 23:09:02 2022\n",
      "Fold 8 started at Sun Jul 17 23:09:02 2022\n",
      "Fold 9 started at Sun Jul 17 23:09:02 2022\n",
      "CV mean score: 2.2146, std: 0.1085.\n",
      "Fold 0 started at Sun Jul 17 23:09:04 2022\n",
      "Fold 1 started at Sun Jul 17 23:09:05 2022\n",
      "Fold 2 started at Sun Jul 17 23:09:06 2022\n",
      "Fold 3 started at Sun Jul 17 23:09:07 2022\n",
      "Fold 4 started at Sun Jul 17 23:09:09 2022\n",
      "Fold 5 started at Sun Jul 17 23:09:10 2022\n",
      "Fold 6 started at Sun Jul 17 23:09:12 2022\n",
      "Fold 7 started at Sun Jul 17 23:09:13 2022\n",
      "Fold 8 started at Sun Jul 17 23:09:15 2022\n",
      "Fold 9 started at Sun Jul 17 23:09:16 2022\n",
      "CV mean score: 2.1622, std: 0.1161.\n",
      "Fold 0 started at Sun Jul 17 23:09:18 2022\n",
      "Fold 1 started at Sun Jul 17 23:09:18 2022\n",
      "Fold 2 started at Sun Jul 17 23:09:19 2022\n",
      "Fold 3 started at Sun Jul 17 23:09:20 2022\n",
      "Fold 4 started at Sun Jul 17 23:09:20 2022\n",
      "Fold 5 started at Sun Jul 17 23:09:21 2022\n",
      "Fold 6 started at Sun Jul 17 23:09:22 2022\n",
      "Fold 7 started at Sun Jul 17 23:09:22 2022\n",
      "Fold 8 started at Sun Jul 17 23:09:23 2022\n",
      "Fold 9 started at Sun Jul 17 23:09:24 2022\n",
      "CV mean score: 2.0312, std: 0.1115.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q catboost\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import linear_model\n",
    "\n",
    "text_columns=['title', 'tagline', 'overview', 'keywords']\n",
    "train_texts = data[text_columns]\n",
    "test_texts = data_test[text_columns]\n",
    "\n",
    "X = data.drop(['target'], axis=1)\n",
    "y = np.log1p(data['target'])\n",
    "X_test = data_test.drop(['id'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    prediction = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if model_type == 'sklearn':\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "                    verbose=1000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_squared_error(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction\n",
    "\n",
    "\n",
    "\n",
    "for col in train_texts.columns:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=10\n",
    "    )\n",
    "    vectorizer.fit(list(train_texts[col].fillna('')) + list(test_texts[col].fillna('')))\n",
    "\n",
    "    \n",
    "    train_col_text = vectorizer.transform(train_texts[col].fillna(''))\n",
    "    test_col_text = vectorizer.transform(test_texts[col].fillna(''))\n",
    "    model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n",
    "    oof_text, prediction_text = train_model(train_col_text, test_col_text, y, params=None, model_type='sklearn', model=model)\n",
    "    \n",
    "    data[col + '_oof'] = oof_text\n",
    "    data_test[col + '_oof'] = prediction_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fac0b7d-f266-4f99-873c-377039688e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "for column in ['title', 'overview']:\n",
    "    data[column] = data[column].astype(str).apply(clean_text)\n",
    "    data_test[column] = data_test[column].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e065f0-53ef-4d2f-8755-7a9fecc901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_columns = list(data.drop(columns=['target', 'label']).select_dtypes(include=numerics).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a07dc8e8",
   "metadata": {
    "cell_id": "00012-f977f172-2409-44c1-9ff4-118d043985cc",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166470,
    "source_hash": "2dc8e0f4"
   },
   "outputs": [],
   "source": [
    "## C√≥digo Holdout\n",
    "data_labels=data[['target', 'label']]\n",
    "data_info=data.drop(columns=['target', 'label'])\n",
    "\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(data_info, data_labels.label,\n",
    "                                                                            random_state=42, test_size=0.2, stratify=data_labels.label)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(data_info, data_labels.target,\n",
    "                                                                            random_state=42, test_size=0.2)\n",
    "\n",
    "X_all_class=data_info\n",
    "y_all_class=data_labels.label\n",
    "\n",
    "X_all_reg=data_info\n",
    "y_all_reg=data_labels.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0787b2c-cc42-49c3-92d4-1a6d286f1b55",
   "metadata": {
    "cell_id": "00014-3a4f50bf-0f3a-496f-9360-ce23ceaba0cb",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166470,
    "source_hash": "2dc8e0f4"
   },
   "outputs": [],
   "source": [
    "#### C√≥digo aqu√≠ ####\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=10\n",
    "    )\n",
    "\n",
    "preprocessing_transformer_class = ColumnTransformer(\n",
    "    [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46be297d-5893-4ff9-8729-38421d0805fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidftitle__american</th>\n",
       "      <th>tfidftitle__away</th>\n",
       "      <th>tfidftitle__baby</th>\n",
       "      <th>tfidftitle__back</th>\n",
       "      <th>tfidftitle__bad</th>\n",
       "      <th>tfidftitle__ball</th>\n",
       "      <th>tfidftitle__batman</th>\n",
       "      <th>tfidftitle__battle</th>\n",
       "      <th>tfidftitle__best</th>\n",
       "      <th>tfidftitle__big</th>\n",
       "      <th>...</th>\n",
       "      <th>OneHotEncoder__original_language_no</th>\n",
       "      <th>OneHotEncoder__original_language_pl</th>\n",
       "      <th>OneHotEncoder__original_language_pt</th>\n",
       "      <th>OneHotEncoder__original_language_ro</th>\n",
       "      <th>OneHotEncoder__original_language_ru</th>\n",
       "      <th>OneHotEncoder__original_language_sv</th>\n",
       "      <th>OneHotEncoder__original_language_te</th>\n",
       "      <th>OneHotEncoder__original_language_th</th>\n",
       "      <th>OneHotEncoder__original_language_tr</th>\n",
       "      <th>OneHotEncoder__original_language_zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5160 rows √ó 3080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidftitle__american  tfidftitle__away  tfidftitle__baby  \\\n",
       "0                      0.0               0.0               0.0   \n",
       "1                      0.0               0.0               0.0   \n",
       "2                      0.0               0.0               0.0   \n",
       "3                      0.0               0.0               0.0   \n",
       "4                      0.0               0.0               0.0   \n",
       "...                    ...               ...               ...   \n",
       "5155                   0.0               0.0               0.0   \n",
       "5156                   0.0               0.0               0.0   \n",
       "5157                   0.0               0.0               0.0   \n",
       "5158                   0.0               0.0               0.0   \n",
       "5159                   0.0               0.0               0.0   \n",
       "\n",
       "      tfidftitle__back  tfidftitle__bad  tfidftitle__ball  tfidftitle__batman  \\\n",
       "0                  0.0              0.0               0.0                 0.0   \n",
       "1                  0.0              0.0               0.0                 0.0   \n",
       "2                  0.0              0.0               0.0                 0.0   \n",
       "3                  0.0              0.0               0.0                 0.0   \n",
       "4                  0.0              0.0               0.0                 0.0   \n",
       "...                ...              ...               ...                 ...   \n",
       "5155               0.0              0.0               0.0                 0.0   \n",
       "5156               0.0              0.0               0.0                 0.0   \n",
       "5157               0.0              0.0               0.0                 0.0   \n",
       "5158               0.0              0.0               0.0                 0.0   \n",
       "5159               0.0              0.0               0.0                 0.0   \n",
       "\n",
       "      tfidftitle__battle  tfidftitle__best  tfidftitle__big  ...  \\\n",
       "0                    0.0               0.0              0.0  ...   \n",
       "1                    0.0               0.0              0.0  ...   \n",
       "2                    0.0               0.0              0.0  ...   \n",
       "3                    0.0               0.0              0.0  ...   \n",
       "4                    0.0               0.0              0.0  ...   \n",
       "...                  ...               ...              ...  ...   \n",
       "5155                 0.0               0.0              0.0  ...   \n",
       "5156                 0.0               0.0              0.0  ...   \n",
       "5157                 0.0               0.0              0.0  ...   \n",
       "5158                 0.0               0.0              0.0  ...   \n",
       "5159                 0.0               0.0              0.0  ...   \n",
       "\n",
       "      OneHotEncoder__original_language_no  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_pl  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_pt  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_ro  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_ru  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_sv  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_te  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_th  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "...                                   ...   \n",
       "5155                                  0.0   \n",
       "5156                                  0.0   \n",
       "5157                                  0.0   \n",
       "5158                                  0.0   \n",
       "5159                                  0.0   \n",
       "\n",
       "      OneHotEncoder__original_language_tr  OneHotEncoder__original_language_zh  \n",
       "0                                     0.0                                  0.0  \n",
       "1                                     0.0                                  0.0  \n",
       "2                                     0.0                                  0.0  \n",
       "3                                     0.0                                  0.0  \n",
       "4                                     0.0                                  0.0  \n",
       "...                                   ...                                  ...  \n",
       "5155                                  0.0                                  0.0  \n",
       "5156                                  0.0                                  0.0  \n",
       "5157                                  0.0                                  0.0  \n",
       "5158                                  0.0                                  0.0  \n",
       "5159                                  0.0                                  0.0  \n",
       "\n",
       "[5160 rows x 3080 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=preprocessing_transformer_class.fit_transform(X_train_class).toarray(),\n",
    "            columns=preprocessing_transformer_class.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b2e93-6f1f-44f0-87b7-d2dbc88738b9",
   "metadata": {
    "cell_id": "ec9c0f8a2b044f858858ef3c3248c239",
    "deepnote_cell_height": 102,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "source": [
    "```\n",
    "Comentarios\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcbc0f",
   "metadata": {
    "cell_id": "00018-d6de5b4a-3ce2-4aaa-9421-121c4fcaf3b5",
    "deepnote_cell_height": 117.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Clasificaci√≥n\n",
    "\n",
    "### 4.1 Dummy y Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6aaeb1d",
   "metadata": {
    "cell_id": "00020-830185e8-7ee6-44a5-89f0-ccb5e5d4ef76",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166499,
    "source_hash": "3e943dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.1772358601612866\n"
     ]
    }
   ],
   "source": [
    "## C√≥digo Dummy\n",
    "dummy_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", DummyClassifier(strategy=\"stratified\", random_state=42))])\n",
    "\n",
    "dummy_pipe.fit(X_train_class, y_train_class)\n",
    "dummy_y_pred = dummy_pipe.predict(X_test_class)\n",
    "\n",
    "print('F1:', f1_score(y_test_class, dummy_y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abb3f143-99e7-4d21-927f-72c0ef9335e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.34361880421069213 Modelo: GradientBoostingClassifier()\n",
      "F1: 0.3279457492980857 Modelo: LGBMClassifier()\n",
      "F1: 0.24739573851798807 Modelo: RandomForestClassifier()\n",
      "F1: 0.23621503883190806 Modelo: SVC()\n",
      "F1: 0.3111668270540872 Modelo: AdaBoostClassifier()\n",
      "F1: 0.31756763093628315 Modelo: VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
      "                             ('knn', LGBMClassifier()),\n",
      "                             ('svc', AdaBoostClassifier())])\n"
     ]
    }
   ],
   "source": [
    "## C√≥digo Comparaci√≥n de m√©tricas\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf2 = LGBMClassifier()\n",
    "clf3 = AdaBoostClassifier()\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)])\n",
    "\n",
    "modelos=[GradientBoostingClassifier(), LGBMClassifier(), RandomForestClassifier(), SVC(), AdaBoostClassifier(), eclf]\n",
    "\n",
    "for model in modelos:\n",
    "    class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", model)])\n",
    "    class_pipe.fit(X_train_class, y_train_class)\n",
    "    y_pred = class_pipe.predict(X_test_class)\n",
    "    \n",
    "    print('F1:', f1_score(y_test_class, y_pred, average='macro'), f'Modelo: {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4614f0af-ca73-4729-af5f-7c7e0bf5edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.30375832061388364 Modelo: GradientBoostingClassifier()\n",
      "F1: 0.30136121006042366 Modelo: LGBMClassifier()\n",
      "F1: 0.23320853310663633 Modelo: RandomForestClassifier()\n",
      "F1: 0.2229800936089344 Modelo: SVC()\n",
      "F1: 0.33130807410909224 Modelo: AdaBoostClassifier()\n",
      "F1: 0.2984209846387036 Modelo: VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
      "                             ('knn', LGBMClassifier()),\n",
      "                             ('svc', AdaBoostClassifier())])\n"
     ]
    }
   ],
   "source": [
    "## C√≥digo Comparaci√≥n de m√©tricas\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf2 = LGBMClassifier()\n",
    "clf3 = AdaBoostClassifier()\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)])\n",
    "\n",
    "modelos=[GradientBoostingClassifier(), LGBMClassifier(), RandomForestClassifier(), SVC(), AdaBoostClassifier(), eclf]\n",
    "\n",
    "for model in modelos:\n",
    "    class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", model)])\n",
    "    print('F1:', cross_val_score(class_pipe, X_all_class, y_all_class, cv=5, scoring='f1_macro').mean() , f'Modelo: {model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76428c9-d158-49c6-af36-13452ad0b7df",
   "metadata": {
    "cell_id": "8b46d8fd6f9544b199b67020abad2562",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "id": "53c33b23",
    "tags": []
   },
   "source": [
    "```\n",
    "Justificaci√≥n\n",
    "```\n",
    "\n",
    "Como se ve, el modelo dummy entrega un F1-macro muy inferior con respecto a todos los modelos entrenados, esto dado que solo utiliza los labels y las probabilidades de estos para predecir.\n",
    "\n",
    "En cuanto a los modelos entrenados, los que presentaron los mejores resultados fueron los ensamblados, y aquellos que aprenden de sus errores en cada iteraci√≥n (GBC y LGBM).\n",
    "\n",
    "A continuaci√≥n se har√° el GridSearch para los modelos de GBC y LGBMC, dado que fueron los que mejor F1-Macro obtuvieron (tanto de en los conjuntos definidos, como al hacer cross validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96167ac",
   "metadata": {
    "cell_id": "00021-c294ef41-853d-4297-b051-d5d4e6577715",
    "deepnote_cell_height": 61.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### 4.2 B√∫squeda del mejor modelo de Clasificaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "120aebb2-3809-4d78-851c-d0b7425a7387",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('tfidftitle',\n",
       "                                    TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                    sublinear_tf=True),\n",
       "                                    'title'),\n",
       "                                   ('tfidfoverview',\n",
       "                                    TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                    sublinear_tf=True),\n",
       "                                    'overview'),\n",
       "                                   ('StandardScaler', MinMaxScaler(),\n",
       "                                    ['budget', 'runtime', 'release_year',\n",
       "                                     'release_day', 'release_month',\n",
       "                                     'release_date_day', 'weekend', 'genres_count...\n",
       "                                     'words_title_count', 'letters_tagline_count',\n",
       "                                     'words_tagline_count', 'genre_Fantasy',\n",
       "                                     'genre_Action', 'genre_Animation',\n",
       "                                     'genre_Crime', 'genre_Science Fiction',\n",
       "                                     'genre_Horror', 'genre_Adventure',\n",
       "                                     'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                     'genre_Thriller', 'genre_Romance',\n",
       "                                     'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                   ('OneHotEncoder',\n",
       "                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                    ['original_language'])])),\n",
       "  ('bf', SelectPercentile(percentile=100)),\n",
       "  ('clf',\n",
       "   VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
       "                                ('knn', LGBMClassifier()),\n",
       "                                ('svc', AdaBoostClassifier())]))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('tfidftitle',\n",
       "                                  TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                  sublinear_tf=True),\n",
       "                                  'title'),\n",
       "                                 ('tfidfoverview',\n",
       "                                  TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                                                  sublinear_tf=True),\n",
       "                                  'overview'),\n",
       "                                 ('StandardScaler', MinMaxScaler(),\n",
       "                                  ['budget', 'runtime', 'release_year',\n",
       "                                   'release_day', 'release_month',\n",
       "                                   'release_date_day', 'weekend', 'genres_count...\n",
       "                                   'words_title_count', 'letters_tagline_count',\n",
       "                                   'words_tagline_count', 'genre_Fantasy',\n",
       "                                   'genre_Action', 'genre_Animation',\n",
       "                                   'genre_Crime', 'genre_Science Fiction',\n",
       "                                   'genre_Horror', 'genre_Adventure',\n",
       "                                   'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                   'genre_Thriller', 'genre_Romance',\n",
       "                                   'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                 ('OneHotEncoder',\n",
       "                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                  ['original_language'])]),\n",
       " 'bf': SelectPercentile(percentile=100),\n",
       " 'clf': VotingClassifier(estimators=[('dt', GradientBoostingClassifier()),\n",
       "                              ('knn', LGBMClassifier()),\n",
       "                              ('svc', AdaBoostClassifier())]),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('tfidftitle',\n",
       "   TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       "   'title'),\n",
       "  ('tfidfoverview',\n",
       "   TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       "   'overview'),\n",
       "  ('StandardScaler',\n",
       "   MinMaxScaler(),\n",
       "   ['budget',\n",
       "    'runtime',\n",
       "    'release_year',\n",
       "    'release_day',\n",
       "    'release_month',\n",
       "    'release_date_day',\n",
       "    'weekend',\n",
       "    'genres_count',\n",
       "    'actores_count',\n",
       "    'production_companies_count',\n",
       "    'keywords_count',\n",
       "    'letters_title_count',\n",
       "    'words_title_count',\n",
       "    'letters_tagline_count',\n",
       "    'words_tagline_count',\n",
       "    'genre_Fantasy',\n",
       "    'genre_Action',\n",
       "    'genre_Animation',\n",
       "    'genre_Crime',\n",
       "    'genre_Science Fiction',\n",
       "    'genre_Horror',\n",
       "    'genre_Adventure',\n",
       "    'genre_Family',\n",
       "    'genre_Comedy',\n",
       "    'genre_Drama',\n",
       "    'genre_Thriller',\n",
       "    'genre_Romance',\n",
       "    'genre_War',\n",
       "    'genre_Mystery',\n",
       "    'genre_Music',\n",
       "    'genre_Western',\n",
       "    'genre_Documentary',\n",
       "    'genre_History',\n",
       "    'genre_TV Movie',\n",
       "    'budget_actores',\n",
       "    'budget_anio',\n",
       "    'budget_runtime',\n",
       "    'log_budget',\n",
       "    'disney',\n",
       "    'fox',\n",
       "    'warner',\n",
       "    'nbc',\n",
       "    'sony',\n",
       "    'paramount',\n",
       "    'count_top_actors',\n",
       "    'count_top_directors',\n",
       "    'title_oof',\n",
       "    'tagline_oof',\n",
       "    'overview_oof',\n",
       "    'keywords_oof']),\n",
       "  ('OneHotEncoder',\n",
       "   OneHotEncoder(handle_unknown='ignore'),\n",
       "   ['original_language'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__verbose_feature_names_out': True,\n",
       " 'preprocessing__tfidftitle': TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       " 'preprocessing__tfidfoverview': TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True),\n",
       " 'preprocessing__StandardScaler': MinMaxScaler(),\n",
       " 'preprocessing__OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__tfidftitle__analyzer': 'word',\n",
       " 'preprocessing__tfidftitle__binary': False,\n",
       " 'preprocessing__tfidftitle__decode_error': 'strict',\n",
       " 'preprocessing__tfidftitle__dtype': numpy.float64,\n",
       " 'preprocessing__tfidftitle__encoding': 'utf-8',\n",
       " 'preprocessing__tfidftitle__input': 'content',\n",
       " 'preprocessing__tfidftitle__lowercase': True,\n",
       " 'preprocessing__tfidftitle__max_df': 1.0,\n",
       " 'preprocessing__tfidftitle__max_features': None,\n",
       " 'preprocessing__tfidftitle__min_df': 10,\n",
       " 'preprocessing__tfidftitle__ngram_range': (1, 2),\n",
       " 'preprocessing__tfidftitle__norm': 'l2',\n",
       " 'preprocessing__tfidftitle__preprocessor': None,\n",
       " 'preprocessing__tfidftitle__smooth_idf': True,\n",
       " 'preprocessing__tfidftitle__stop_words': None,\n",
       " 'preprocessing__tfidftitle__strip_accents': None,\n",
       " 'preprocessing__tfidftitle__sublinear_tf': True,\n",
       " 'preprocessing__tfidftitle__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'preprocessing__tfidftitle__tokenizer': None,\n",
       " 'preprocessing__tfidftitle__use_idf': True,\n",
       " 'preprocessing__tfidftitle__vocabulary': None,\n",
       " 'preprocessing__tfidfoverview__analyzer': 'word',\n",
       " 'preprocessing__tfidfoverview__binary': False,\n",
       " 'preprocessing__tfidfoverview__decode_error': 'strict',\n",
       " 'preprocessing__tfidfoverview__dtype': numpy.float64,\n",
       " 'preprocessing__tfidfoverview__encoding': 'utf-8',\n",
       " 'preprocessing__tfidfoverview__input': 'content',\n",
       " 'preprocessing__tfidfoverview__lowercase': True,\n",
       " 'preprocessing__tfidfoverview__max_df': 1.0,\n",
       " 'preprocessing__tfidfoverview__max_features': None,\n",
       " 'preprocessing__tfidfoverview__min_df': 10,\n",
       " 'preprocessing__tfidfoverview__ngram_range': (1, 2),\n",
       " 'preprocessing__tfidfoverview__norm': 'l2',\n",
       " 'preprocessing__tfidfoverview__preprocessor': None,\n",
       " 'preprocessing__tfidfoverview__smooth_idf': True,\n",
       " 'preprocessing__tfidfoverview__stop_words': None,\n",
       " 'preprocessing__tfidfoverview__strip_accents': None,\n",
       " 'preprocessing__tfidfoverview__sublinear_tf': True,\n",
       " 'preprocessing__tfidfoverview__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'preprocessing__tfidfoverview__tokenizer': None,\n",
       " 'preprocessing__tfidfoverview__use_idf': True,\n",
       " 'preprocessing__tfidfoverview__vocabulary': None,\n",
       " 'preprocessing__StandardScaler__clip': False,\n",
       " 'preprocessing__StandardScaler__copy': True,\n",
       " 'preprocessing__StandardScaler__feature_range': (0, 1),\n",
       " 'preprocessing__OneHotEncoder__categories': 'auto',\n",
       " 'preprocessing__OneHotEncoder__drop': None,\n",
       " 'preprocessing__OneHotEncoder__dtype': numpy.float64,\n",
       " 'preprocessing__OneHotEncoder__handle_unknown': 'ignore',\n",
       " 'preprocessing__OneHotEncoder__max_categories': None,\n",
       " 'preprocessing__OneHotEncoder__min_frequency': None,\n",
       " 'preprocessing__OneHotEncoder__sparse': True,\n",
       " 'bf__percentile': 100,\n",
       " 'bf__score_func': <function sklearn.feature_selection._univariate_selection.f_classif(X, y)>,\n",
       " 'clf__estimators': [('dt', GradientBoostingClassifier()),\n",
       "  ('knn', LGBMClassifier()),\n",
       "  ('svc', AdaBoostClassifier())],\n",
       " 'clf__flatten_transform': True,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__verbose': False,\n",
       " 'clf__voting': 'hard',\n",
       " 'clf__weights': None,\n",
       " 'clf__dt': GradientBoostingClassifier(),\n",
       " 'clf__knn': LGBMClassifier(),\n",
       " 'clf__svc': AdaBoostClassifier(),\n",
       " 'clf__dt__ccp_alpha': 0.0,\n",
       " 'clf__dt__criterion': 'friedman_mse',\n",
       " 'clf__dt__init': None,\n",
       " 'clf__dt__learning_rate': 0.1,\n",
       " 'clf__dt__loss': 'log_loss',\n",
       " 'clf__dt__max_depth': 3,\n",
       " 'clf__dt__max_features': None,\n",
       " 'clf__dt__max_leaf_nodes': None,\n",
       " 'clf__dt__min_impurity_decrease': 0.0,\n",
       " 'clf__dt__min_samples_leaf': 1,\n",
       " 'clf__dt__min_samples_split': 2,\n",
       " 'clf__dt__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__dt__n_estimators': 100,\n",
       " 'clf__dt__n_iter_no_change': None,\n",
       " 'clf__dt__random_state': None,\n",
       " 'clf__dt__subsample': 1.0,\n",
       " 'clf__dt__tol': 0.0001,\n",
       " 'clf__dt__validation_fraction': 0.1,\n",
       " 'clf__dt__verbose': 0,\n",
       " 'clf__dt__warm_start': False,\n",
       " 'clf__knn__boosting_type': 'gbdt',\n",
       " 'clf__knn__class_weight': None,\n",
       " 'clf__knn__colsample_bytree': 1.0,\n",
       " 'clf__knn__importance_type': 'split',\n",
       " 'clf__knn__learning_rate': 0.1,\n",
       " 'clf__knn__max_depth': -1,\n",
       " 'clf__knn__min_child_samples': 20,\n",
       " 'clf__knn__min_child_weight': 0.001,\n",
       " 'clf__knn__min_split_gain': 0.0,\n",
       " 'clf__knn__n_estimators': 100,\n",
       " 'clf__knn__n_jobs': -1,\n",
       " 'clf__knn__num_leaves': 31,\n",
       " 'clf__knn__objective': None,\n",
       " 'clf__knn__random_state': None,\n",
       " 'clf__knn__reg_alpha': 0.0,\n",
       " 'clf__knn__reg_lambda': 0.0,\n",
       " 'clf__knn__silent': 'warn',\n",
       " 'clf__knn__subsample': 1.0,\n",
       " 'clf__knn__subsample_for_bin': 200000,\n",
       " 'clf__knn__subsample_freq': 0,\n",
       " 'clf__svc__algorithm': 'SAMME.R',\n",
       " 'clf__svc__base_estimator': None,\n",
       " 'clf__svc__learning_rate': 1.0,\n",
       " 'clf__svc__n_estimators': 50,\n",
       " 'clf__svc__random_state': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b0cf06b-a907-49e8-97d2-c3335275811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3564 candidates, totalling 17820 fits\n",
      "0.347376358401553 {'bf__percentile': 50, 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 1: vectorizer y percentile\n",
    "    {\n",
    "        \"bf__percentile\": range(50, 101, 5),\n",
    "        'preprocessing__tfidftitle__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [1, 3, 10],\n",
    "        'preprocessing__tfidfoverview__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [1, 3, 10],\n",
    "                            }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5b72277-da9c-49ef-be44-ed29d55af87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "0.34814444529711136 {'bf__percentile': 50, 'clf': AdaBoostClassifier(), 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 2: percentile y modelos\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [GradientBoostingClassifier(n)],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3]\n",
    "    },\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [AdaBoostClassifier()],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3]\n",
    "    }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs2 = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs2.best_score_, gs2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6ea580d-c1b3-4411-ac50-ae8748188520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "0.32266080235362954 {'bf__percentile': 50, 'clf': AdaBoostClassifier(base_estimator=GradientBoostingClassifier(),\n",
      "                   learning_rate=1.5, n_estimators=25), 'clf__algorithm': 'SAMME.R', 'clf__base_estimator': GradientBoostingClassifier(), 'clf__learning_rate': 1.5, 'clf__n_estimators': 25, 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 3: hiperparametros modelo\n",
    "    {\n",
    "        \"bf__percentile\": [50],\n",
    "        \"clf\": [AdaBoostClassifier()],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3],\n",
    "        'clf__base_estimator': [RandomForestClassifier(), GradientBoostingClassifier()],\n",
    "        'clf__n_estimators': [10, 25, 50],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.5, 1.0, 1.5],\n",
    "        'clf__algorithm': ['SAMME', 'SAMME.R']\n",
    "    }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs3 = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs3.best_score_, gs3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e920b9-fa78-4bca-a128-b52b315d06ae",
   "metadata": {},
   "source": [
    "Baj√≥ su desempe√±o, se volver√° a hacer el GridSearch sin modificar el base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5bfd7782-a09a-4477-b146-32309cb2d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n",
      "0.34780252055318284 {'bf__percentile': 50, 'clf': AdaBoostClassifier(), 'clf__algorithm': 'SAMME.R', 'clf__learning_rate': 1.0, 'clf__n_estimators': 50, 'preprocessing__tfidfoverview__min_df': 3, 'preprocessing__tfidfoverview__ngram_range': (2, 2), 'preprocessing__tfidfoverview__norm': 'l1', 'preprocessing__tfidftitle__min_df': 3, 'preprocessing__tfidftitle__ngram_range': (1, 2), 'preprocessing__tfidftitle__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 3: hiperparametros modelo\n",
    "    {\n",
    "        \"bf__percentile\": [45, 50, 55],\n",
    "        \"clf\": [AdaBoostClassifier()],\n",
    "        'preprocessing__tfidftitle__norm': ['l1'],\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [3],\n",
    "        'preprocessing__tfidfoverview__norm': ['l1'],\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [3],\n",
    "        # 'clf__base_estimator': [RandomForestClassifier(), GradientBoostingClassifier()],\n",
    "        'clf__n_estimators': [10, 25, 40, 50, 60],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.5, 0.8, 1.0, 1.2, 1.5],\n",
    "        'clf__algorithm': ['SAMME', 'SAMME.R']\n",
    "    }\n",
    "]\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=50)),\n",
    "                        (\"clf\", AdaBoostClassifier(random_state=3))])\n",
    "\n",
    "gs4 = GridSearchCV(class_pipe, grid, n_jobs=-1, scoring=\"f1_macro\", cv=5, verbose=1).fit(X_all_class, y_all_class)\n",
    "print(gs4.best_score_, gs4.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13993f22-63af-44fe-bb1c-32bd2ec3f915",
   "metadata": {},
   "source": [
    "Finalmente, guardando el mejor modelo luego de varios GridSearch para el AdaBoostClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c75d656-33b0-4804-beb6-1b9708e6fd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3675788440827209\n"
     ]
    }
   ],
   "source": [
    "vectorizer_1 = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                norm='l1'\n",
    "    )\n",
    "\n",
    "vectorizer_2 = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                ngram_range=(2, 2),\n",
    "                min_df=3,\n",
    "                norm='l1'\n",
    "    )\n",
    "\n",
    "\n",
    "preprocessing_transformer_class = ColumnTransformer(\n",
    "    [('tfidftitle', vectorizer_1, 'title'),\n",
    "     ('tfidfoverview', vectorizer_2, 'overview'),\n",
    "     ('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=50)),\n",
    "                        (\"clf\", AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1.0, n_estimators=50))])\n",
    "\n",
    "class_pipe.fit(X_all_class, y_all_class)\n",
    "y_pred = class_pipe.predict(X_all_class)\n",
    "\n",
    "print('F1:', f1_score(y_all_class, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bea09-5d03-4e96-8737-ee8a0d5bfddf",
   "metadata": {},
   "source": [
    "Probando ahoora el GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ac81218-907f-4389-8149-dbe3990b7449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "F1: 0.8247417666055202\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingC\n",
    "learning_rate = [0.02,0.03]\n",
    "n_estimators = [100, 500, 1000]\n",
    "subsample = [0.9, 0.5, 0.2]\n",
    "max_depth = [4,6,8]\n",
    "\n",
    "hyperparams = {'n_estimators': n_estimators, 'subsample': subsample}\n",
    "\n",
    "\n",
    "gd_gbc=GridSearchCV(estimator = GradientBoostingClassifier(n_estimators=500), param_grid = hyperparams, \n",
    "                verbose=3, cv=5, scoring = 'f1_macro', n_jobs=-1)\n",
    "\n",
    "class_pipe2 = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=40)),\n",
    "                        (\"clf\", gd_gbc)])\n",
    "\n",
    "class_pipe2.fit(X_train_class, y_train_class)\n",
    "y_pred = class_pipe2.predict(X_all_class)\n",
    "print('F1:', f1_score(y_all_class, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47e18b2e-da4c-473a-b142-4bdedd6a39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3509967149571193 {'n_estimators': 1000, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(gd_gbc.best_score_, gd_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcbe25-ed8b-4edb-92ef-de9212a685ca",
   "metadata": {},
   "source": [
    "Viendo las features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbf8860d-d638-46be-b678-5a6cb55082ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StandardScaler__runtime</td>\n",
       "      <td>0.074299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StandardScaler__release_year</td>\n",
       "      <td>0.059640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StandardScaler__keywords_count</td>\n",
       "      <td>0.033530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StandardScaler__genre_Drama</td>\n",
       "      <td>0.025186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>StandardScaler__overview_oof</td>\n",
       "      <td>0.018516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>StandardScaler__budget_actores</td>\n",
       "      <td>0.018264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>StandardScaler__budget_runtime</td>\n",
       "      <td>0.018123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>StandardScaler__keywords_oof</td>\n",
       "      <td>0.017435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>StandardScaler__genre_Animation</td>\n",
       "      <td>0.016915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OneHotEncoder__original_language_en</td>\n",
       "      <td>0.016793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>StandardScaler__title_oof</td>\n",
       "      <td>0.015657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>StandardScaler__actores_count</td>\n",
       "      <td>0.014517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>StandardScaler__tagline_oof</td>\n",
       "      <td>0.014443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>StandardScaler__letters_tagline_count</td>\n",
       "      <td>0.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>StandardScaler__genre_Documentary</td>\n",
       "      <td>0.009267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>StandardScaler__budget_anio</td>\n",
       "      <td>0.008501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StandardScaler__letters_title_count</td>\n",
       "      <td>0.007524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>StandardScaler__release_month</td>\n",
       "      <td>0.006806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>StandardScaler__release_day</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StandardScaler__genre_Horror</td>\n",
       "      <td>0.005578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Feature  importance\n",
       "0                 StandardScaler__runtime    0.074299\n",
       "1            StandardScaler__release_year    0.059640\n",
       "2          StandardScaler__keywords_count    0.033530\n",
       "3             StandardScaler__genre_Drama    0.025186\n",
       "4            StandardScaler__overview_oof    0.018516\n",
       "5          StandardScaler__budget_actores    0.018264\n",
       "6          StandardScaler__budget_runtime    0.018123\n",
       "7            StandardScaler__keywords_oof    0.017435\n",
       "8         StandardScaler__genre_Animation    0.016915\n",
       "9     OneHotEncoder__original_language_en    0.016793\n",
       "10              StandardScaler__title_oof    0.015657\n",
       "11          StandardScaler__actores_count    0.014517\n",
       "12            StandardScaler__tagline_oof    0.014443\n",
       "13  StandardScaler__letters_tagline_count    0.010498\n",
       "14      StandardScaler__genre_Documentary    0.009267\n",
       "15            StandardScaler__budget_anio    0.008501\n",
       "16    StandardScaler__letters_title_count    0.007524\n",
       "17          StandardScaler__release_month    0.006806\n",
       "18            StandardScaler__release_day    0.006599\n",
       "19           StandardScaler__genre_Horror    0.005578"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_importances=pd.DataFrame({'Feature': preprocessing_transformer_class.get_feature_names_out(), 'Select': class_pipe2.steps[1][1].get_support()})\n",
    "feat_importances=feat_importances[feat_importances.Select==True]\n",
    "feat_importances['importance']=class_pipe2.steps[2][1].best_estimator_.feature_importances_\n",
    "feat_importances.sort_values(by='importance', ascending=False)[['Feature', 'importance']][:20].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ad86e",
   "metadata": {
    "cell_id": "00023-73786884-e1b2-448a-9636-ab05cf3c1fc5",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "```\n",
    "Justificaci√≥n Aqu√≠\n",
    "```\n",
    "Como se puede ver, el GridSearch del GBC da mucho mejor resultado que todo el GridSearch realizado para el AdaBoost, esto principalmente por el gran n√∫mero de estimadores a utilizar (1000), sin embargo para la predicci√≥n final se utiliz√≥ la cantidad de 500 dado que con 1000 es muy posible que estuviera sobreajustando y no generalizando de buena manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6514b",
   "metadata": {
    "cell_id": "2d58aececbe34d6184477c8e3cfaa2e3",
    "deepnote_cell_height": 117.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Regresi√≥n\n",
    "\n",
    "### 5.1 Dummy y Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a57b61b",
   "metadata": {
    "cell_id": "86fd465c690a46d1bafed3115c0bceff",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤: -0.0004752178066032009\n"
     ]
    }
   ],
   "source": [
    "## C√≥digo Dummy\n",
    "dummy_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", DummyRegressor())])\n",
    "\n",
    "dummy_pipe.fit(X_train_reg, y_train_reg)\n",
    "dummy_y_pred = dummy_pipe.predict(X_test_reg)\n",
    "\n",
    "print('R¬≤:', r2_score(y_test_reg, dummy_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55467133-ab67-4cd2-bc8c-2b509cba1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤: 0.5700759442917105 Modelo: RandomForestRegressor()\n",
      "R¬≤: 0.5610659144946457 Modelo: SGDRegressor()\n",
      "R¬≤: 0.5716157260849514 Modelo: GradientBoostingRegressor()\n",
      "R¬≤: 0.5448146031357388 Modelo: BaggingRegressor()\n",
      "R¬≤: 0.5740716858110675 Modelo: VotingRegressor(estimators=[('gb',\n",
      "                             GradientBoostingRegressor(learning_rate=0.02,\n",
      "                                                       n_estimators=1500)),\n",
      "                            ('rf',\n",
      "                             RandomForestRegressor(bootstrap=False,\n",
      "                                                   max_features='sqrt')),\n",
      "                            ('lr', BaggingRegressor())])\n"
     ]
    }
   ],
   "source": [
    "## C√≥digo Comparaci√≥n de m√©tricas\n",
    "reg1 = GradientBoostingRegressor(learning_rate=0.02, n_estimators=1500)\n",
    "reg2 = RandomForestRegressor(bootstrap=False, max_features='sqrt')\n",
    "reg3 = BaggingRegressor()\n",
    "vr = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "modelos=[RandomForestRegressor(), SGDRegressor(), GradientBoostingRegressor(), BaggingRegressor(),\n",
    "         vr]\n",
    "\n",
    "for model in modelos:\n",
    "    reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", model)])\n",
    "    reg_pipe.fit(X_train_reg, y_train_reg)\n",
    "    y_pred = reg_pipe.predict(X_test_reg)\n",
    "    \n",
    "    print('R¬≤:', r2_score(y_test_reg, y_pred), f'Modelo: {model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122122e3-505b-4ad3-ab80-1ef91fc56fca",
   "metadata": {
    "cell_id": "23ba7e6c56c841d8b2c11563ca64bf20",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "id": "59354a18",
    "tags": []
   },
   "source": [
    "```\n",
    "Justificaci√≥n\n",
    "```\n",
    "En este caso de regresi√≥n, el modelo dummy tambi√©n entrega un R2 cercano a 0 (es decir no se ajusta para nada a los datos), y muy inferior con respecto a todos los modelos entrenados. Esto dado que solo utiliza el promedio de la variable a predecir, asignando este valor como predicci√≥n a todas las entradas.\n",
    "\n",
    "En cuanto a los modelos entrenados, los que presentaron los mejores resultados fueron los ensamblados, entre ellos se encuentran el RandomForest el GradientBoosting, adem√°s de la combinaci√≥n de estos en el VotingRegressor.\n",
    "\n",
    "A continuaci√≥n se har√° el GridSearch para lps modelos de GBC y RFR dado que fueron los que mejor R2 tuvieron sobre el conjunto de testeo (se podr√≠a ahondar en el VotingRegressor pero por temas de tiempo y complejidad de realizar el GridSearch se omitir√° este modelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db11df",
   "metadata": {
    "cell_id": "df491c3ed9704211be52235df236b889",
    "deepnote_cell_height": 61.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### 5.2 B√∫squeda del mejor modelo de Regresi√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e4b6a0-e9a8-4c51-b18e-4b57743fd172",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "c60f85b8-2fc9-4943-8e16-023763fd7c62",
    "outputId": "50dcfec5-655d-400a-fa2e-65224e2bfbe5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-583e8f1cf831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                         (\"clf\", RandomForestRegressor(random_state=3))])\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 1: vectorizer\n",
    "    {\n",
    "        'preprocessing__tfidftitle__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidftitle__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidftitle__min_df': [1, 3, 10],\n",
    "        'preprocessing__tfidfoverview__norm': ('l1', 'l2'),\n",
    "        'preprocessing__tfidfoverview__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'preprocessing__tfidfoverview__min_df': [1, 3, 10],\n",
    "                            }\n",
    "]\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", RandomForestRegressor(random_state=3))])\n",
    "\n",
    "gs = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038874b-df7c-4b2e-8077-4ed22c3bfaae",
   "metadata": {},
   "source": [
    "Dado que tard√≥ demasiado el GridSearch con la realizaci√≥n del vectorizer se opt√≥ por deshabilitar este paso del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45456b88-c8d8-49ba-9e37-bc84c728348b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('StandardScaler', MinMaxScaler(),\n",
       "                                    ['budget', 'runtime', 'release_year',\n",
       "                                     'release_day', 'release_month',\n",
       "                                     'release_date_day', 'weekend', 'genres_count',\n",
       "                                     'actores_count', 'production_companies_count',\n",
       "                                     'keywords_count', 'letters_title_count',\n",
       "                                     'words_title_count', 'letters_tagline_count',\n",
       "                                     'words_tagline_count', 'genre_Fantasy',\n",
       "                                     'genre_Action', 'genre_Animation',\n",
       "                                     'genre_Crime', 'genre_Science Fiction',\n",
       "                                     'genre_Horror', 'genre_Adventure',\n",
       "                                     'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                     'genre_Thriller', 'genre_Romance',\n",
       "                                     'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                   ('OneHotEncoder',\n",
       "                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                    ['original_language'])])),\n",
       "  ('bf', SelectPercentile(percentile=100)),\n",
       "  ('clf', RandomForestRegressor())],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('StandardScaler', MinMaxScaler(),\n",
       "                                  ['budget', 'runtime', 'release_year',\n",
       "                                   'release_day', 'release_month',\n",
       "                                   'release_date_day', 'weekend', 'genres_count',\n",
       "                                   'actores_count', 'production_companies_count',\n",
       "                                   'keywords_count', 'letters_title_count',\n",
       "                                   'words_title_count', 'letters_tagline_count',\n",
       "                                   'words_tagline_count', 'genre_Fantasy',\n",
       "                                   'genre_Action', 'genre_Animation',\n",
       "                                   'genre_Crime', 'genre_Science Fiction',\n",
       "                                   'genre_Horror', 'genre_Adventure',\n",
       "                                   'genre_Family', 'genre_Comedy', 'genre_Drama',\n",
       "                                   'genre_Thriller', 'genre_Romance',\n",
       "                                   'genre_War', 'genre_Mystery', 'genre_Music', ...]),\n",
       "                                 ('OneHotEncoder',\n",
       "                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                  ['original_language'])]),\n",
       " 'bf': SelectPercentile(percentile=100),\n",
       " 'clf': RandomForestRegressor(),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('StandardScaler',\n",
       "   MinMaxScaler(),\n",
       "   ['budget',\n",
       "    'runtime',\n",
       "    'release_year',\n",
       "    'release_day',\n",
       "    'release_month',\n",
       "    'release_date_day',\n",
       "    'weekend',\n",
       "    'genres_count',\n",
       "    'actores_count',\n",
       "    'production_companies_count',\n",
       "    'keywords_count',\n",
       "    'letters_title_count',\n",
       "    'words_title_count',\n",
       "    'letters_tagline_count',\n",
       "    'words_tagline_count',\n",
       "    'genre_Fantasy',\n",
       "    'genre_Action',\n",
       "    'genre_Animation',\n",
       "    'genre_Crime',\n",
       "    'genre_Science Fiction',\n",
       "    'genre_Horror',\n",
       "    'genre_Adventure',\n",
       "    'genre_Family',\n",
       "    'genre_Comedy',\n",
       "    'genre_Drama',\n",
       "    'genre_Thriller',\n",
       "    'genre_Romance',\n",
       "    'genre_War',\n",
       "    'genre_Mystery',\n",
       "    'genre_Music',\n",
       "    'genre_Western',\n",
       "    'genre_Documentary',\n",
       "    'genre_History',\n",
       "    'genre_TV Movie',\n",
       "    'budget_actores',\n",
       "    'budget_anio',\n",
       "    'budget_runtime',\n",
       "    'log_budget',\n",
       "    'disney',\n",
       "    'fox',\n",
       "    'warner',\n",
       "    'nbc',\n",
       "    'sony',\n",
       "    'paramount',\n",
       "    'count_top_actors',\n",
       "    'count_top_directors',\n",
       "    'title_oof',\n",
       "    'tagline_oof',\n",
       "    'overview_oof',\n",
       "    'keywords_oof']),\n",
       "  ('OneHotEncoder',\n",
       "   OneHotEncoder(handle_unknown='ignore'),\n",
       "   ['original_language'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__verbose_feature_names_out': True,\n",
       " 'preprocessing__StandardScaler': MinMaxScaler(),\n",
       " 'preprocessing__OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__StandardScaler__clip': False,\n",
       " 'preprocessing__StandardScaler__copy': True,\n",
       " 'preprocessing__StandardScaler__feature_range': (0, 1),\n",
       " 'preprocessing__OneHotEncoder__categories': 'auto',\n",
       " 'preprocessing__OneHotEncoder__drop': None,\n",
       " 'preprocessing__OneHotEncoder__dtype': numpy.float64,\n",
       " 'preprocessing__OneHotEncoder__handle_unknown': 'ignore',\n",
       " 'preprocessing__OneHotEncoder__max_categories': None,\n",
       " 'preprocessing__OneHotEncoder__min_frequency': None,\n",
       " 'preprocessing__OneHotEncoder__sparse': True,\n",
       " 'bf__percentile': 100,\n",
       " 'bf__score_func': <function sklearn.feature_selection._univariate_selection.f_classif(X, y)>,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__ccp_alpha': 0.0,\n",
       " 'clf__criterion': 'squared_error',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 1.0,\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__max_samples': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 100,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    # [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", RandomForestRegressor())])\n",
    "\n",
    "reg_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c60f85b8-2fc9-4943-8e16-023763fd7c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid = [\n",
    "#     # grilla 1: vectorizer\n",
    "#     {\n",
    "#         'preprocessing__tfidftitle__norm': ('l1', 'l2'),\n",
    "#         'preprocessing__tfidftitle__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "#         'preprocessing__tfidftitle__min_df': [1, 3, 10],\n",
    "#         'preprocessing__tfidfoverview__norm': ('l1', 'l2'),\n",
    "#         'preprocessing__tfidfoverview__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "#         'preprocessing__tfidfoverview__min_df': [1, 3, 10],\n",
    "#                             }\n",
    "# ]\n",
    "# reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "#                         (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "#                         (\"clf\", RandomForestRegressor(random_state=3))])\n",
    "\n",
    "# gs = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392c89f2-655d-4dc3-9c4b-7bf7fff4d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "0.05557985195183481 {'bf__percentile': 80, 'clf': GradientBoostingRegressor()}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 2: percentile y modelos\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "    },\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [RandomForestRegressor()],\n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "gs2 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs2.best_score_, gs2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace3f67e-a59b-4e18-9903-b8b8e5e9ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "0.06119467558900451 {'bf__percentile': 85, 'clf': GradientBoostingRegressor(criterion='squared_error'), 'clf__criterion': 'squared_error', 'clf__learning_rate': 0.1, 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 3: hiperparametros modelo v1\n",
    "    {\n",
    "        \"bf__percentile\": [80, 85, 90, 100],\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "        'clf__n_estimators': [25, 50, 100, 200],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.5, 1.0, 1.5],\n",
    "        'clf__criterion': ['friedman_mse', 'squared_error']\n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "\n",
    "gs3 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs3.best_score_, gs3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae43837-1037-4def-9d4f-f54017d43669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "0.07605075955315092 {'bf__percentile': 85, 'clf': GradientBoostingRegressor(criterion='squared_error', max_depth=5,\n",
      "                          max_features='sqrt', min_samples_split=9), 'clf__criterion': 'squared_error', 'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 9, 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 4: hiperparametros modelo v2\n",
    "    {\n",
    "        \"bf__percentile\": [80, 85, 90],\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "        'clf__n_estimators': [100],\n",
    "        'clf__learning_rate': [0.1],\n",
    "        'clf__criterion': ['squared_error'],\n",
    "        'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'clf__min_samples_split': [2, 5, 9],\n",
    "        'clf__max_depth': [3, 5, 9],\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "\n",
    "gs4 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs4.best_score_, gs4.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8c519ef-2520-48b1-9316-b58607287ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "0.08553275202699868 {'bf__percentile': 85, 'clf': GradientBoostingRegressor(criterion='squared_error', max_depth=5,\n",
      "                          max_features='sqrt', min_samples_leaf=8,\n",
      "                          min_samples_split=15, subsample=0.97), 'clf__criterion': 'squared_error', 'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 8, 'clf__min_samples_split': 15, 'clf__n_estimators': 100, 'clf__subsample': 0.97}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 5: hiperparametros modelo v3\n",
    "    {\n",
    "        \"bf__percentile\": [85],\n",
    "        \"clf\": [GradientBoostingRegressor()],\n",
    "        'clf__n_estimators': [100],\n",
    "        'clf__learning_rate': [0.1],\n",
    "        'clf__criterion': ['squared_error'],\n",
    "        'clf__max_features': ['sqrt'],\n",
    "        'clf__min_samples_split': [15, 20, 35],\n",
    "        'clf__max_depth': [5],\n",
    "        'clf__subsample': [0.95, 0.97, 0.985, 1.0],\n",
    "        'clf__min_samples_leaf': [1, 3, 5, 8, 10, 12],\n",
    "        \n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "\n",
    "gs5 = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gs5.best_score_, gs5.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91775cbe",
   "metadata": {
    "cell_id": "b00b9baef48947d9abcfb6972bdaa3aa",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤: 0.8087562984111456\n"
     ]
    }
   ],
   "source": [
    "### C√≥digo Predicci√≥n de datos de la competencia aqu√≠\n",
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    # [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=85)),\n",
    "                        (\"clf\", GradientBoostingRegressor(criterion='squared_error', max_depth=5,\n",
    "                          max_features='sqrt', min_samples_leaf=8,\n",
    "                          min_samples_split=15, subsample=0.97))])\n",
    "\n",
    "reg_pipe.fit(X_all_reg, y_all_reg)\n",
    "y_pred = reg_pipe.predict(X_all_reg)\n",
    "\n",
    "print('R¬≤:', r2_score(y_all_reg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3add7e-5cdd-4e13-88f2-bef1c2c345e0",
   "metadata": {},
   "source": [
    "Probando el RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ef58c3f-7d85-4ba8-86c5-90646ddc405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "0.015364982796172866 {'bf__percentile': 80, 'clf': RandomForestRegressor(criterion='poisson'), 'clf__criterion': 'poisson'}\n"
     ]
    }
   ],
   "source": [
    "grid = [\n",
    "    # grilla 2: percentile y modelos\n",
    "    {\n",
    "        \"bf__percentile\": range(10, 101, 10),\n",
    "        \"clf\": [RandomForestRegressor()],\n",
    "        'clf__criterion': ['squared_error', 'absolute_error', 'poisson']\n",
    "    }\n",
    "]\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", GradientBoostingRegressor(random_state=3))])\n",
    "gsrf = GridSearchCV(reg_pipe, grid, n_jobs=-1, scoring=\"r2\", cv=5, verbose=1).fit(X_all_reg, y_all_reg)\n",
    "print(gsrf.best_score_, gsrf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a68ae766-3f21-4009-8034-891187152dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤: 0.9436675084604951\n"
     ]
    }
   ],
   "source": [
    "preprocessing_transformer_reg = ColumnTransformer(\n",
    "    # [(f'tfidf{txt}', vectorizer, txt) for txt in ['title', 'overview']] +\n",
    "    [('StandardScaler', MinMaxScaler(), num_columns),\n",
    "     ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'),  ['original_language'])]\n",
    ")\n",
    "\n",
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=80)),\n",
    "                        (\"clf\", RandomForestRegressor(criterion='poisson'))])\n",
    "\n",
    "reg_pipe.fit(X_all_reg, y_all_reg)\n",
    "y_pred = reg_pipe.predict(X_all_reg)\n",
    "\n",
    "print('R¬≤:', r2_score(y_all_reg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15c2f9-6c45-4512-84a4-96bf55b74091",
   "metadata": {
    "cell_id": "23ba7e6c56c841d8b2c11563ca64bf20",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "id": "59354a18",
    "tags": []
   },
   "source": [
    "```\n",
    "Justificaci√≥n\n",
    "```\n",
    "En este caso de regresi√≥n, el RandomForestRegressor, al ajustarle el par√°metro de criterio de error a poisson logr√≥ mejorar considerablemente lo obtenido por el modelo de GradientBoosting, es pore sto que tambi√©n fue utilizado como modelo final al momento de competir. (No se ahondo en el GridSearch de este modelo dada la demora que se tiene.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dcbcc",
   "metadata": {
    "cell_id": "00025-2acf9c12-da85-4c1f-add5-f2b0f600177f",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0ea2a",
   "metadata": {
    "cell_id": "00026-15c07b20-0e16-48fa-bf3c-33aeb2c4c1db",
    "deepnote_cell_height": 51.53334045410156,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Los resultados de la competencia son los que se muestran a continuaci√≥n:\n",
    "    - Problema de clasificaci√≥n: 0.84 F1 - Score\n",
    "    - Problema de regresi√≥n: 0.91 R2\n",
    "\n",
    "En base a estos resultados se cree que la resoluci√≥n del problema se logr√≥ de buena manera dado que la m√©trica de F1 Score en la clasificaci√≥n alcanz√≥ valores de 0.84 en los datos a clasificar, lo cual indica que de alguna manera que la presici√≥n y el recall del modelo est√°n obteniendo buenos resultados, sin embargo debemos tener cuidado con la clasificaci√≥n de las minoritarias (en general bajaban bastante los score). Por su parte, el modelo de regresi√≥n se ajusta de muy buena manera a las predicciones de √©xito monetario que tendr√≠a una pel√≠cula, esto se debe principalmente la ingenier√≠a de variables realizada como lo eran el conteo de algunas variables de texto, la vectorizaci√≥n de texto de las variables como overview y keyword, los ratios de budget vs runtime, el conteo de actores y productores relevantes, y tambi√©n al ajuste de hiperpar√°metros para el modelo de regresi√≥n.\n",
    "\n",
    "Para llegar a esto, el EDA result√≥ √∫til de tal manera que se lograr√≠a entender principalmente c√≥mo malear o manejar las features con las que ya se contaba, sin embargo, creemos como equipo que la ingenier√≠a de variables no se bas√≥ principalmente en el eda, sino en la creatividad sobre c√≥mo expresar de mejor manera los datos con tal de obtener mejores caracterizaciones. Para esto tambi√©n creemos que ayud√≥ bastante entender el contexto de estar trabajando en un problema orientado al mundo de las pel√≠culas, por lo cual deb√≠amos tambi√©n agregar features realacionadas a eso, como los mejores actores, mejores productoras, etc.\n",
    "\n",
    "En cuanto a los modelos, se tiene que tanto el modelo clasificador como el modelo de regresi√≥n en versi√≥n dummy entregaban m√©tricas que no eran aceptables para una soluci√≥n, en este sentido el baseline fue f√°cil de superar sobretodo en la regresi√≥n, ya que este es un problema m√°s espec√≠fico. En cuanto a otros modelos, se utilizaron algunos simples como KNN y DesicionTreeClassifier en la clasificaci√≥n y LinearRegresor para la regresi√≥n que no entregar√≠an resultados muy elevados, y que los clasificadores boosteados sobrepasar√≠an muy f√°cilmente. Por esto se utiliz√≥ el GradientBoostClassifier entregando las mejores m√©tricas con los hiperpar√°metros por default al igual que AdaBoosting. Luego estos ser√≠an ingresados a una grilla y el modelo de GradientBoost terminar√≠a ofreciendo mejores m√©tricas. Lo mismo se hizo para el regresor, en donde modelos como Lasso, GradientBoost y RandomForestRegressor entregar√≠an los mejores resultados, los cuales se juntaron en un clasificador de votaci√≥n sin mayor √©xito y se ingresaron en una grilla entregando el mejor modelo como RandomForestRegressor.\n",
    "\n",
    "Cabe destacar que luego de la grilla se ajustaron algunos par√°metros a mano dado, esto dado que la grilla a veces no ofrec√≠a los mejores resultados que si lograbamos con un ajuste a mano de algunos hiperpar√°metros.\n",
    "\n",
    "Por √∫ltimo, como equipo nos encontramos sumamente conformes con el desempe√±o del proyecto, obteniendo buenos lugares en la tabla de resultados, sin embargo dado que en la misma tabla existen mejores resultados en la clasificaci√≥n, creemos que claramente el clasificador cuenta con un gran margen de mejoras, esto se podr√≠a hacer obteniendo vectores de sem√°ntica de texto y algunas redes neuronales de texto para as√≠ entrenar un modelo m√°s robusto.\n",
    "\n",
    "En cuanto a lo aprendido dentro de este proceso, creemos que fue la metodolog√≠a a seguir para llegar a cabo un protyecto de competencia que mantiene todas las caracter√≠sticas de un problemas de ciencia de datos que se puede presentar en la vida real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790b480",
   "metadata": {
    "cell_id": "00027-1e362e1d-a776-4423-93d5-0f568476e4c1",
    "deepnote_cell_height": 84.10000610351562,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Anexo: Generaci√≥n de Archivo Submit de la Competencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb8c57",
   "metadata": {
    "cell_id": "00028-0a64e7e8-1077-4868-8c96-db3d51323157",
    "deepnote_cell_height": 671.9000244140625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Para subir los resultados obtenidos a la pagina de CodaLab utilice la funci√≥n `generateFiles` entregada mas abajo. Esto es debido a que usted deber√° generar archivos que respeten extrictamente el formato de CodaLab, de lo contario los resultados no se veran reflejados en la pagina de la competencia.\n",
    "\n",
    "Para los resultados obtenidos en su modelo de clasificaci√≥n y regresi√≥n, estos ser√°n guardados en un archivo zip que contenga los archivos `predicctions_clf.txt` para la clasificaci√≥n y `predicctions_rgr.clf` para la regresi√≥n. Los resultados, como se comento antes, deberan ser obtenidos en base al dataset `test.pickle` y en cada una de las lineas deberan presentar las predicciones realizadas.\n",
    "\n",
    "Ejemplos de archivos:\n",
    "\n",
    "- [ ] `predicctions_clf.txt`\n",
    "\n",
    "        Mostly Positive\n",
    "        Mostly Positive\n",
    "        Negative\n",
    "        Positive\n",
    "        Negative\n",
    "        Positive\n",
    "        ...\n",
    "\n",
    "- [ ] `predicctions_rgr.txt`\n",
    "\n",
    "        16103.58\n",
    "        16103.58\n",
    "        16041.89\n",
    "        9328.62\n",
    "        107976.03\n",
    "        194374.08\n",
    "        ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a3c4a6",
   "metadata": {
    "cell_id": "00029-55f95a4c-2d1f-4354-a690-049fea34bdac",
    "deepnote_cell_height": 620.13330078125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166501,
    "source_hash": "b1cdf32f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "def generateFiles(predict_data, clf_pipe, rgr_pipe):\n",
    "    \"\"\"Genera los archivos a subir en CodaLab\n",
    "\n",
    "    Input\n",
    "    predict_data: Dataframe con los datos de entrada a predecir\n",
    "    clf_pipe: pipeline del clf\n",
    "    rgr_pipe: pipeline del rgr\n",
    "\n",
    "    Ouput\n",
    "    archivo de txt\n",
    "    \"\"\"\n",
    "    y_pred_clf = clf_pipe.predict(predict_data)\n",
    "    y_pred_rgr = rgr_pipe.predict(predict_data)\n",
    "    \n",
    "    with open('./predictions_clf.txt', 'w') as f:\n",
    "        for item in y_pred_clf:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with open('./predictions_rgr.txt', 'w') as f:\n",
    "        for item in y_pred_rgr:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with ZipFile('predictions.zip', 'w') as zipObj2:\n",
    "        zipObj2.write('predictions_rgr.txt')\n",
    "        zipObj2.write('predictions_clf.txt')\n",
    "\n",
    "    os.remove(\"predictions_rgr.txt\")\n",
    "    os.remove(\"predictions_clf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e1b3bb-0beb-495b-9508-6df20a49bbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤: 0.9478521316670252\n",
      "F1: 0.8586326112171789\n"
     ]
    }
   ],
   "source": [
    "reg_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_reg),\n",
    "                        (\"bf\", SelectPercentile(f_classif, percentile=100)),\n",
    "                        (\"clf\", RandomForestRegressor(criterion='poisson'))])\n",
    "\n",
    "reg_pipe.fit(X_all_reg, y_all_reg)\n",
    "print('R¬≤:', r2_score(y_all_reg, reg_pipe.predict(X_all_reg)))\n",
    "\n",
    "class_pipe = Pipeline(steps=[(\"preprocessing\", preprocessing_transformer_class),\n",
    "                    (\"bf\", SelectPercentile(f_classif, percentile=50)),\n",
    "                    (\"clf\", GradientBoostingClassifier(n_estimators=500))])\n",
    "class_pipe.fit(X_all_class, y_all_class)\n",
    "print('F1:', f1_score(y_all_class, class_pipe.predict(X_all_class), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65649abb-3aed-42f1-98ac-2c855fa371b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16432509-fae3-4f72-9b97-2beaa394005a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>original_language</th>\n",
       "      <th>overview</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>release_date</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>status</th>\n",
       "      <th>...</th>\n",
       "      <th>warner</th>\n",
       "      <th>nbc</th>\n",
       "      <th>sony</th>\n",
       "      <th>paramount</th>\n",
       "      <th>count_top_actors</th>\n",
       "      <th>count_top_directors</th>\n",
       "      <th>title_oof</th>\n",
       "      <th>tagline_oof</th>\n",
       "      <th>overview_oof</th>\n",
       "      <th>keywords_oof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15927</td>\n",
       "      <td>passage india</td>\n",
       "      <td>Drama-Adventure-History</td>\n",
       "      <td>en</td>\n",
       "      <td>set period growing influence indian independen...</td>\n",
       "      <td>EMI Films-Thorn EMI Screen Entertainment-HBO</td>\n",
       "      <td>1984-12-14</td>\n",
       "      <td>8000000.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.899538</td>\n",
       "      <td>16.792093</td>\n",
       "      <td>16.731907</td>\n",
       "      <td>17.311062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9064</td>\n",
       "      <td>hellbound hellraiser ii</td>\n",
       "      <td>Horror</td>\n",
       "      <td>en</td>\n",
       "      <td>doctor channard sent new patient girl warning ...</td>\n",
       "      <td>Film Futures-New World Pictures-Cinemarque Ent...</td>\n",
       "      <td>1988-12-23</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.347646</td>\n",
       "      <td>16.528879</td>\n",
       "      <td>17.133209</td>\n",
       "      <td>16.788112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41602</td>\n",
       "      <td>charlie countryman</td>\n",
       "      <td>Action-Comedy-Drama-Romance-Thriller</td>\n",
       "      <td>en</td>\n",
       "      <td>traveling abroad guy falls romanian beauty who...</td>\n",
       "      <td>Bona Fide Productions-Voltage Pictures</td>\n",
       "      <td>2013-02-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.785088</td>\n",
       "      <td>16.540153</td>\n",
       "      <td>15.872622</td>\n",
       "      <td>16.321421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>293646</td>\n",
       "      <td>33</td>\n",
       "      <td>Drama-History</td>\n",
       "      <td>en</td>\n",
       "      <td>based true story collapse mine san jose chilet...</td>\n",
       "      <td>Alcon Entertainment-Phoenix Pictures-Half Circ...</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>25000000.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.907404</td>\n",
       "      <td>16.509489</td>\n",
       "      <td>16.757019</td>\n",
       "      <td>15.795558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157353</td>\n",
       "      <td>transcendence</td>\n",
       "      <td>Thriller-Science Fiction-Drama-Mystery</td>\n",
       "      <td>en</td>\n",
       "      <td>two leading computer scientists work toward go...</td>\n",
       "      <td>DMG Entertainment-Warner Bros. Pictures-Alcon ...</td>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>100000000.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.785088</td>\n",
       "      <td>17.001481</td>\n",
       "      <td>17.358071</td>\n",
       "      <td>16.925404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>6439</td>\n",
       "      <td>racing stripes</td>\n",
       "      <td>Comedy-Family</td>\n",
       "      <td>en</td>\n",
       "      <td>shattered illusions hard repair especially goo...</td>\n",
       "      <td>Alcon Entertainment</td>\n",
       "      <td>2005-01-06</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.785088</td>\n",
       "      <td>16.320282</td>\n",
       "      <td>17.618127</td>\n",
       "      <td>17.547067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>9396</td>\n",
       "      <td>crocodile dundee ii</td>\n",
       "      <td>Adventure-Comedy</td>\n",
       "      <td>en</td>\n",
       "      <td>australian outback expert protects new york lo...</td>\n",
       "      <td>Paramount</td>\n",
       "      <td>1988-05-19</td>\n",
       "      <td>14000000.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.347646</td>\n",
       "      <td>17.681122</td>\n",
       "      <td>17.204335</td>\n",
       "      <td>17.362434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>9357</td>\n",
       "      <td>one hour photo</td>\n",
       "      <td>Drama-Thriller</td>\n",
       "      <td>en</td>\n",
       "      <td>sy photo guy parrish lovingly developed photos...</td>\n",
       "      <td>Fox Searchlight Pictures-Catch 23 Entertainmen...</td>\n",
       "      <td>2002-08-21</td>\n",
       "      <td>12000000.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.665157</td>\n",
       "      <td>17.074108</td>\n",
       "      <td>16.932835</td>\n",
       "      <td>16.438952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>9494</td>\n",
       "      <td>look whos talking</td>\n",
       "      <td>Comedy-Romance</td>\n",
       "      <td>en</td>\n",
       "      <td>mollie single working mother whos find perfect...</td>\n",
       "      <td>Management Company Entertainment Group (MCEG)-...</td>\n",
       "      <td>1989-10-12</td>\n",
       "      <td>7500000.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.083372</td>\n",
       "      <td>17.259105</td>\n",
       "      <td>17.445195</td>\n",
       "      <td>16.654946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>9772</td>\n",
       "      <td>air force one</td>\n",
       "      <td>Action-Thriller</td>\n",
       "      <td>en</td>\n",
       "      <td>russian terrorists conspire hijack aircraft pr...</td>\n",
       "      <td>Radiant Productions-Beacon Communications-Beac...</td>\n",
       "      <td>1997-07-25</td>\n",
       "      <td>85000000.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.755094</td>\n",
       "      <td>17.231367</td>\n",
       "      <td>17.362235</td>\n",
       "      <td>18.309514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>656 rows √ó 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                    title                                  genres  \\\n",
       "0     15927            passage india                 Drama-Adventure-History   \n",
       "1      9064  hellbound hellraiser ii                                  Horror   \n",
       "2     41602       charlie countryman    Action-Comedy-Drama-Romance-Thriller   \n",
       "3    293646                       33                           Drama-History   \n",
       "4    157353            transcendence  Thriller-Science Fiction-Drama-Mystery   \n",
       "..      ...                      ...                                     ...   \n",
       "651    6439           racing stripes                           Comedy-Family   \n",
       "652    9396      crocodile dundee ii                        Adventure-Comedy   \n",
       "653    9357           one hour photo                          Drama-Thriller   \n",
       "654    9494        look whos talking                          Comedy-Romance   \n",
       "655    9772            air force one                         Action-Thriller   \n",
       "\n",
       "    original_language                                           overview  \\\n",
       "0                  en  set period growing influence indian independen...   \n",
       "1                  en  doctor channard sent new patient girl warning ...   \n",
       "2                  en  traveling abroad guy falls romanian beauty who...   \n",
       "3                  en  based true story collapse mine san jose chilet...   \n",
       "4                  en  two leading computer scientists work toward go...   \n",
       "..                ...                                                ...   \n",
       "651                en  shattered illusions hard repair especially goo...   \n",
       "652                en  australian outback expert protects new york lo...   \n",
       "653                en  sy photo guy parrish lovingly developed photos...   \n",
       "654                en  mollie single working mother whos find perfect...   \n",
       "655                en  russian terrorists conspire hijack aircraft pr...   \n",
       "\n",
       "                                  production_companies release_date  \\\n",
       "0         EMI Films-Thorn EMI Screen Entertainment-HBO   1984-12-14   \n",
       "1    Film Futures-New World Pictures-Cinemarque Ent...   1988-12-23   \n",
       "2               Bona Fide Productions-Voltage Pictures   2013-02-09   \n",
       "3    Alcon Entertainment-Phoenix Pictures-Half Circ...   2015-08-06   \n",
       "4    DMG Entertainment-Warner Bros. Pictures-Alcon ...   2014-04-16   \n",
       "..                                                 ...          ...   \n",
       "651                                Alcon Entertainment   2005-01-06   \n",
       "652                                          Paramount   1988-05-19   \n",
       "653  Fox Searchlight Pictures-Catch 23 Entertainmen...   2002-08-21   \n",
       "654  Management Company Entertainment Group (MCEG)-...   1989-10-12   \n",
       "655  Radiant Productions-Beacon Communications-Beac...   1997-07-25   \n",
       "\n",
       "          budget  runtime    status  ... warner nbc sony  paramount  \\\n",
       "0      8000000.0    163.0  Released  ...      0   0    0          0   \n",
       "1      3000000.0     97.0  Released  ...      0   0    0          0   \n",
       "2            0.0    108.0  Released  ...      0   0    0          0   \n",
       "3     25000000.0    120.0  Released  ...      0   0    0          0   \n",
       "4    100000000.0    119.0  Released  ...      0   0    0          0   \n",
       "..           ...      ...       ...  ...    ...  ..  ...        ...   \n",
       "651   30000000.0    102.0  Released  ...      0   0    0          0   \n",
       "652   14000000.0    110.0  Released  ...      0   0    0          1   \n",
       "653   12000000.0     96.0  Released  ...      0   0    0          0   \n",
       "654    7500000.0     93.0  Released  ...      0   0    0          0   \n",
       "655   85000000.0    124.0  Released  ...      0   0    0          0   \n",
       "\n",
       "     count_top_actors  count_top_directors  title_oof  tagline_oof  \\\n",
       "0                   0                    0  16.899538    16.792093   \n",
       "1                   0                    0  17.347646    16.528879   \n",
       "2                   0                    0  16.785088    16.540153   \n",
       "3                   0                    0  16.907404    16.509489   \n",
       "4                   0                    0  16.785088    17.001481   \n",
       "..                ...                  ...        ...          ...   \n",
       "651                 0                    0  16.785088    16.320282   \n",
       "652                 0                    0  17.347646    17.681122   \n",
       "653                 0                    0  16.665157    17.074108   \n",
       "654                 0                    0  17.083372    17.259105   \n",
       "655                 0                    0  16.755094    17.231367   \n",
       "\n",
       "     overview_oof  keywords_oof  \n",
       "0       16.731907     17.311062  \n",
       "1       17.133209     16.788112  \n",
       "2       15.872622     16.321421  \n",
       "3       16.757019     15.795558  \n",
       "4       17.358071     16.925404  \n",
       "..            ...           ...  \n",
       "651     17.618127     17.547067  \n",
       "652     17.204335     17.362434  \n",
       "653     16.932835     16.438952  \n",
       "654     17.445195     16.654946  \n",
       "655     17.362235     18.309514  \n",
       "\n",
       "[656 rows x 63 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41cc6b2f",
   "metadata": {
    "cell_id": "b589f659ce2246919e3707e79420aff2",
    "deepnote_cell_height": 138,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ejecutar funci√≥n para generar el archivo de predicciones.\n",
    "# perdict_data debe tener cargada los datos del text.pickle\n",
    "# mientras que clf_pipe y rgr_pipe, son los pipeline de \n",
    "# clasificaci√≥n y regresi√≥n respectivamente.\n",
    "generateFiles(data_test, class_pipe, reg_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda9345",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "dbddc0f3-10b8-4160-bc27-ac993196164c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
